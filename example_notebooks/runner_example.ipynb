{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runner example\n",
    "\n",
    "This notebook will go through the options of the runner class. We will show how to fit and evaluate a model in parallel, and how to do cross-validation. \n",
    "\n",
    "We will also demonstrate how to transfer and extend a HBR model using the runner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pcntoolkit.dataio.norm_data import NormData\n",
    "from pcntoolkit.normative_model.norm_conf import NormConf\n",
    "from pcntoolkit.normative_model.norm_blr import NormBLR\n",
    "from pcntoolkit.regression_model.blr.blr_conf import BLRConf\n",
    "from pcntoolkit.normative_model.norm_hbr import NormHBR\n",
    "from pcntoolkit.regression_model.hbr.hbr_conf import HBRConf\n",
    "\n",
    "from pcntoolkit.util.runner import Runner\n",
    "from pcntoolkit.util.plotter import plot_centiles, plot_qq\n",
    "\n",
    "os.makedirs(\"resources/data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we download a small example dataset from github. Saving this dataset on your local device (under 'resources/data/fcon1000.csv' for example) saves time and bandwidth if you re-run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook for the first time, you need to download the dataset from github.\n",
    "# If you have already downloaded the dataset, you can comment out the following line\n",
    "\n",
    "pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/predictive-clinical-neuroscience/PCNtoolkit-demo/refs/heads/main/data/fcon1000.csv\"\n",
    ").to_csv(\"resources/data/fcon1000.csv\", index=False)\n",
    "data = pd.read_csv(\"resources/data/fcon1000.csv\")\n",
    "covariates = [\"age\"]\n",
    "batch_effects = [\"sex\", \"site\"]\n",
    "response_vars = \"lh_G_cingul-Post-dorsal_thickness,lh_G_cingul-Post-ventral_thickness,lh_G_cuneus_thickness,lh_G_front_inf-Opercular_thickness,lh_G_front_inf-Orbital_thickness,lh_G_front_inf-Triangul_thickness,lh_G_front_middle_thickness,lh_G_front_sup_thickness,lh_G_Ins_lg&S_cent_ins_thickness,lh_G_insular_short_thickness,lh_G_occipital_middle_thickness,lh_G_occipital_sup_thickness,lh_G_oc-temp_lat-fusifor_thickness,lh_G_oc-temp_med-Lingual_thickness,lh_G_oc-temp_med-Parahip_thickness,lh_G_orbital_thickness,lh_G_pariet_inf-Angular_thickness,lh_G_pariet_inf-Supramar_thickness,lh_G_parietal_sup_thickness,lh_G_postcentral_thickness,lh_G_precentral_thickness,lh_G_precuneus_thickness,lh_G_rectus_thickness,lh_G_subcallosal_thickness,lh_G_temp_sup-G_T_transv_thickness,lh_G_temp_sup-Lateral_thickness,lh_G_temp_sup-Plan_polar_thickness,lh_G_temp_sup-Plan_tempo_thickness,lh_G_temporal_inf_thickness,lh_G_temporal_middle_thickness,lh_Lat_Fis-ant-Horizont_thickness,lh_Lat_Fis-ant-Vertical_thickness,lh_Lat_Fis-post_thickness,lh_Pole_occipital_thickness,lh_Pole_temporal_thickness,lh_S_calcarine_thickness\".split(\",\")\n",
    "norm_data = NormData.from_dataframe(\n",
    "    name=\"full\",\n",
    "    dataframe=data,\n",
    "    covariates=[\"age\"],\n",
    "    batch_effects=[\"sex\", \"site\"],\n",
    "    response_vars=response_vars,\n",
    ")\n",
    "\n",
    "# Leave two sites out for doing transfer and extend later\n",
    "transfer_sites = [\"Milwaukee_b\", \"Oulu\"]\n",
    "transfer_data, fit_data = norm_data.split_batch_effects({\"site\": transfer_sites}, names=(\"transfer\", \"fit\"))\n",
    "\n",
    "# Split into train and test sets\n",
    "train, test = fit_data.train_test_split()\n",
    "transfer_train, transfer_test = transfer_data.train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the normative model\n",
    "\n",
    "The normative model will be configured using a `NormConf` object, containing save and log paths and the preprocessing configurations, and a `RegConf` object, specific to the regression model type. \n",
    "\n",
    "Our `NormConf` object configures:\n",
    "- a save path paths and whether to save the model and results\n",
    "- a standardization step for both the covariates (inscaler) and the response vars (outscaler)\n",
    "- a Bspline basis expansion of order 3 with 5 knots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process: 2010482 - Configuration of normative model is valid.\n"
     ]
    }
   ],
   "source": [
    "# Create a NormConf object\n",
    "norm_conf = NormConf(\n",
    "    savemodel=True,\n",
    "    saveresults=True,\n",
    "    save_dir=\"resources/blr/save_dir\",\n",
    "    inscaler=\"standardize\",\n",
    "    outscaler=\"standardize\",\n",
    "    basis_function=\"bspline\",\n",
    "    basis_function_kwargs={\"order\": 3, \"nknots\": 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process: 2010482 - Configuration of regression model is valid.\n"
     ]
    }
   ],
   "source": [
    "blr_conf = BLRConf(\n",
    "    optimizer=\"l-bfgs-b\",\n",
    "    n_iter=200,\n",
    "    heteroskedastic=True,\n",
    "    intercept=True,\n",
    "    fixed_effect=False,\n",
    "    fixed_effect_var=False,\n",
    "    warp=\"WarpSinhArcsinh\",\n",
    "    warp_reparam=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine normative and blr conf in normative model\n",
    "We can either use the NormBLR constructor, or the factory method to create a normative BLR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Using the constructor\n",
    "new_model = NormBLR(norm_conf=norm_conf, reg_conf=blr_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "Normally we would just call 'fit_predict' on the model directly, but because we want to use the runner to do cross-validation in parallel, we need to first create a runner object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(\n",
    "    cross_validate=False,\n",
    "    cv_folds=3,\n",
    "    parallelize=True,\n",
    "    environment=\"/project/3022000.05/projects/stijdboe/envs/dev_refactor\",\n",
    "    job_type=\"slurm\",  # or \"slurm\" if you are on a slurm cluster\n",
    "    n_jobs=10,\n",
    "    log_dir=\"resources/runner_output/log_dir\",\n",
    "    temp_dir=\"resources/runner_output/temp_dir\",\n",
    ")\n",
    "# Local parallelization might not work on login nodes due to resource restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runner object will now fit the model in parallel, and save the results in save directories that it will create for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------------\n",
      "            PCNtoolkit Job Status Monitor Â®\n",
      "-----------------------------------------------------\n",
      "Job ID      Name      State      Time      Nodes\n",
      "-----------------------------------------------------\n",
      "\n",
      "46821561    job_0     PENDING    0:00                    \n",
      "46821562    job_1     PENDING    0:00                    \n",
      "46821563    job_2     PENDING    0:00                    \n",
      "46821564    job_3     PENDING    0:00                    \n",
      "46821565    job_4     PENDING    0:00                    \n",
      "46821566    job_5     PENDING    0:00                    \n",
      "46821567    job_6     PENDING    0:00                    \n",
      "46821568    job_7     PENDING    0:00                    \n",
      "46821569    job_8     PENDING    0:00                    \n",
      "46821570    job_9     PENDING    0:00                    \n",
      "\n",
      "-----------------------------------------------------\n",
      "Total active jobs: 10\n",
      "Total completed jobs: 0\n",
      "Total failed jobs: 0\n",
      "-----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runner.fit_predict(new_model, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a fold model\n",
    "We can load a model for a specific fold by calling `load_model` on the runner object. This will return a `NormBLR` object, which we can inspect and use to predict on new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process: 1982211 - Configuration of normative model is valid.\n",
      "Process: 1982211 - Configuration of normative model is valid.\n",
      "Process: 1982211 - Configuration of regression model is valid.\n",
      "Process: 1982211 - Configuration of regression model is valid.\n",
      "Process: 1982211 - Configuration of regression model is valid.\n",
      "Process: 1982211 - Configuration of regression model is valid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pcntoolkit.normative_model.norm_blr.NormBLR at 0x7fd6339e1b20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fitted_model = runner.load_model()\n",
    "display(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the model \n",
    "\n",
    "The norm_blr model contains a collection of regression models, one for each response variable. We can inspect those models individually by calling `norm_blr.regression_models.get(\"{responsevar}\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_name': 'rh_MeanThickness_thickness',\n",
       " '_reg_conf': BLRConf(n_iter=200, tol=1e-05, ard=False, optimizer='l-bfgs-b', l_bfgs_b_l=0.1, l_bfgs_b_epsilon=0.1, l_bfgs_b_norm='l2', intercept=True, fixed_effect=False, heteroskedastic=True, intercept_var=False, fixed_effect_var=False, warp='WarpSinhArcsinh', warp_reparam=True),\n",
       " 'is_fitted': True,\n",
       " '_is_from_dict': True,\n",
       " 'hyp': array([-0.00271425,  0.40459816,  0.57171555,  0.14023606, -0.02021072,\n",
       "        -0.02146683, -0.00490682, -0.28481558, -0.09296211, -0.06571992,\n",
       "        -0.00115522,  0.00524956,  0.00339416,  0.00250672, -0.03653256,\n",
       "        -0.00530122,  0.00506945]),\n",
       " 'nlZ': 979.881210875374,\n",
       " 'N': 744,\n",
       " 'D': 8,\n",
       " 'lambda_n_vec': None,\n",
       " 'Sigma_a': array([[1.06792757, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.00115589, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.9947642 , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.9966116 , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.99749642,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         1.03720808, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 1.00531529, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.900262  ]]),\n",
       " 'Lambda_a': array([[0.93639309, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.99884545, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 1.00526336, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 1.00339992, 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 1.00250986,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.9641267 , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.99471281, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 1.11078775]]),\n",
       " 'hyp0': None,\n",
       " 'n_hyp': 0,\n",
       " 'var_D': 0,\n",
       " 'alpha': None,\n",
       " 'beta': None,\n",
       " 'gamma': None,\n",
       " 'm': array([ 2.57260216,  0.71730288,  0.0989639 , -0.47015866, -0.37883896,\n",
       "        -2.18467429, -0.80680823, -0.48205957]),\n",
       " 'A': array([[3.34896020e+00, 7.46054199e+00, 4.49835547e+00, 5.86164164e-01,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.49576287e+01],\n",
       "        [7.46054199e+00, 1.38249222e+02, 1.64001240e+02, 3.37801094e+01,\n",
       "         5.75762114e-02, 0.00000000e+00, 0.00000000e+00, 3.42549844e+02],\n",
       "        [4.49835547e+00, 1.64001240e+02, 2.59898472e+02, 8.11514910e+01,\n",
       "         2.39766574e+00, 1.59895556e-02, 0.00000000e+00, 5.10957950e+02],\n",
       "        [5.86164164e-01, 3.37801094e+01, 8.11514910e+01, 6.31224539e+01,\n",
       "         1.42865685e+01, 1.48943124e+00, 2.29132128e-02, 1.93435731e+02],\n",
       "        [0.00000000e+00, 5.75762114e-02, 2.39766574e+00, 1.42865685e+01,\n",
       "         1.66301101e+01, 5.42301097e+00, 2.57726706e-01, 3.80501484e+01],\n",
       "        [0.00000000e+00, 0.00000000e+00, 1.59895556e-02, 1.48943124e+00,\n",
       "         5.42301097e+00, 5.74865879e+00, 6.96697694e-01, 1.24096615e+01],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.29132128e-02,\n",
       "         2.57726706e-01, 6.96697694e-01, 1.38614998e+00, 1.36877478e+00],\n",
       "        [1.49576287e+01, 3.42549844e+02, 5.10957950e+02, 1.93435731e+02,\n",
       "         3.80501484e+01, 1.24096615e+01, 1.36877478e+00, 1.11484053e+03]]),\n",
       " 'dnlZ': None,\n",
       " 'ys': None,\n",
       " 's2': None,\n",
       " 'warp': <pcntoolkit.regression_model.blr.warp.WarpSinhArcsinh at 0x7fd633906870>,\n",
       " 'warp_reparam': True,\n",
       " 'warp_params': 2,\n",
       " 'n_gamma': 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_hbr_model = fitted_model.regression_models.get(\"rh_MeanThickness_thickness\")  # type: ignore\n",
    "single_hbr_model.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Calling `predict` will extend the predict_data object with a number of useful arrays.\n",
    "1. `measures`: DataArray, which contains a number of evaluation statistics. \n",
    "1. `zscores`: the predicted z-scores for each datapoint.  \n",
    "1. `centiles`: the predicted centiles of variation evaluated at each covariate in the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------\n",
      "             PCNtoolkit Job Status Monitor Â®\n",
      "--------------------------------------------------------\n",
      "Job ID      Name              State      Time      Nodes\n",
      "--------------------------------------------------------\n",
      "\n",
      "46821015    job_1             FAILED                        \n",
      "46821014    job_0             FAILED                        \n",
      "\n",
      "--------------------------------------------------------    \n",
      "All jobs completed!\n",
      "--------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runner.predict(new_model, test)\n",
    "# display(test.measures.to_pandas().T.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets with a zscores DataArray will have the `.plot_qq()` function available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test.zscores.to_pandas())  # the zscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test.centiles.to_dataframe().unstack(level=[\"response_vars\", \"cdf\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qq(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `plot_centiles()` can be called as a function of the model. A synthetic dataset is created internally, so we need to pass the original dataset (`train` in this case) as a template. We also need to pass which covariate is to be plotted on the x-axis, and the batch-effects for which the centiles are to be plotted. \n",
    "\n",
    "The lines correspond to the CDF values of: [0.05, 0.25, 0.5, 0.75, 0.95]. It is also possible to pass a list of CDF values to plot.\n",
    "\n",
    "It may seem strange that the centiles do not match the plotted data, but that is because the centiles are calculated for a single batch effect, and it is superimposed on the full dataset. The blue markers correspond to the data for which the centiles are calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_centiles(\n",
    "    fitted_model,\n",
    "    train,\n",
    "    covariate=\"age\",\n",
    "    show_data=True,\n",
    "    hue_data=\"sex\",\n",
    "    markers_data=\"site\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of 0.1587 and 0.8413 correspond to a standard deviation of -1 and 1. We plot the centiles again for these values, and we also highlight a specific site. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_centiles(\n",
    "    fitted_model,\n",
    "    train,\n",
    "    covariate=\"age\",\n",
    "    centiles=[0.1587, 0.8413],\n",
    "    show_data=True,\n",
    "    batch_effects={\"site\": [\"Beijing_Zang\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfering with the runner\n",
    "\n",
    "The runner can also be used to transfer or extenda model to a new dataset. This is done by calling `transfer` on the runner object. This will transfer the model to the new dataset, and save the transfered model in the save directory.\n",
    "\n",
    "runner.transfer(transfer_train, transfer_test)\n",
    "\n",
    "Let's first fit an HBR model, and then transfer it to the transfer dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcntoolkit.regression_model.hbr.prior import make_prior\n",
    "\n",
    "\n",
    "mu = make_prior(\n",
    "    name=\"mu\",\n",
    "    linear=True,\n",
    "    slope=make_prior(dist_name=\"Normal\", dist_params=(0.0, 5.0)),\n",
    "    intercept=make_prior(\n",
    "        random=True,\n",
    "        sigma=make_prior(dist_name=\"HalfNormal\", dist_params=(1.0,)),\n",
    "        mu=make_prior(dist_name=\"Normal\", dist_params=(0.0, 0.5)),\n",
    "    ),\n",
    ")\n",
    "sigma = make_prior(\n",
    "    name=\"sigma\",\n",
    "    linear=True,\n",
    "    slope=make_prior(dist_name=\"Normal\", dist_params=(0.0, 3.0)),\n",
    "    intercept=make_prior(\n",
    "        dist_name=\"Normal\",\n",
    "        dist_params=(\n",
    "            1.0,\n",
    "            1.0,\n",
    "        ),\n",
    "    ),\n",
    "    mapping=\"softplus\",\n",
    "    mapping_params=(0.0, 3.0),\n",
    ")\n",
    "\n",
    "# Configure the HBRConf object\n",
    "hbr_conf = HBRConf(\n",
    "    draws=2048,\n",
    "    tune=512,\n",
    "    chains=4,\n",
    "    pymc_cores=16,\n",
    "    likelihood=\"Normal\",\n",
    "    mu=mu,\n",
    "    sigma=sigma,\n",
    "    nuts_sampler=\"nutpie\",\n",
    ")\n",
    "save_dir = \"resources/hbr_transfer_runner/save_dir\"\n",
    "\n",
    "norm_conf = NormConf(\n",
    "    savemodel=True,\n",
    "    saveresults=True,\n",
    "    save_dir=\"resources/hbr_transfer_runner/save_dir\",\n",
    "    inscaler=\"standardize\",\n",
    "    outscaler=\"standardize\",\n",
    "    basis_function=\"bspline\",\n",
    "    basis_function_kwargs={\"order\": 3, \"nknots\": 5},\n",
    ")\n",
    "\n",
    "new_hbr_model = NormHBR(norm_conf=norm_conf, reg_conf=hbr_conf)\n",
    "runner = Runner(\n",
    "    cross_validate=False,\n",
    "    cv_folds=3,\n",
    "    parallelize=True,\n",
    "    time_limit=\"00:15:00\",\n",
    "    job_type=\"slurm\",  # or \"slurm\" if you are on a slurm cluster\n",
    "    n_jobs=2,\n",
    "    log_dir=\"resources/hbr_transfer_runner/log_dir\",\n",
    "    temp_dir=\"resources/hbr_transfer_runner/temp_dir\",\n",
    ")\n",
    "runner.fit_predict(new_hbr_model, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_hbr = NormHBR.load(\"resources/hbr_transfer_runner/save_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_hbr.predict(test)\n",
    "try:\n",
    "    norm_hbr.predict(transfer_test)\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â \n",
    "The original model cannot be used to predict on the transfer dataset, because it was not trained on it\n",
    "\n",
    "See the useful error message below:\n",
    "\n",
    "{e}\n",
    "â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â \n",
    "\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_transfer = NormHBR.load(\"resources/hbr_transfer_runner/save_dir\")\n",
    "runner = Runner(\n",
    "    cross_validate=False,\n",
    "    parallelize=True,\n",
    "    job_type=\"local\",  # or \"slurm\" if you are on a slurm cluster\n",
    "    n_jobs=2,\n",
    "    log_dir=\"resources/hbr_transfer_runner/log_dir\",\n",
    "    temp_dir=\"resources/hbr_transfer_runner/temp_dir\",\n",
    ")\n",
    "runner.transfer_predict(to_transfer, transfer_train, transfer_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfered_model = NormHBR.load(\"resources/hbr_transfer_runner/save_dir_transfer\")\n",
    "transfered_model = runner.load_fold_model(0)\n",
    "transfered_model.predict(transfer_test)\n",
    "try:\n",
    "    transfered_model.predict(test)\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â \n",
    "The transfered model cannot be used to predict on the original dataset, because it was transfered to a different dataset\n",
    "\n",
    "See the useful error message below:\n",
    "\n",
    "{e}\n",
    "â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â â \n",
    "\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_extend = NormHBR.load(\"resources/hbr_transfer_runner/save_dir\")\n",
    "runner.extend(to_extend, transfer_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_model = runner.load_fold_model(0)\n",
    "extended_model.predict(transfer_test)\n",
    "extended_model.predict(test)\n",
    "print(\"The extended model can now be used to predict on both the original and transfer dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, now you have seen how to:\n",
    "- Use the runner to do cross-validation in parallel\n",
    "- Inspect the model of a specific fold\n",
    "- Evaluate the model on a test set\n",
    "- Create useful plots\n",
    "\n",
    "We hope this tutorial was useful. If you have any questions or remarks, please let us know on GitHub. Thanks!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_refactor_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
