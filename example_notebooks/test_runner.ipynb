{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Runner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pcntoolkit.dataio.norm_data import NormData\n",
    "from pcntoolkit.normative_model.norm_conf import NormConf\n",
    "from pcntoolkit.normative_model.norm_blr import NormBLR\n",
    "from pcntoolkit.regression_model.blr.blr_conf import BLRConf\n",
    "from pcntoolkit.normative_model.norm_factory import load_normative_model\n",
    "from pcntoolkit.normative_model.norm_factory import create_normative_model\n",
    "from pcntoolkit.regression_model.hbr.hbr_conf import HBRConf\n",
    "from pcntoolkit.runner import Runner\n",
    "from pcntoolkit.regression_model.hbr.param import Param\n",
    "\n",
    "import seaborn as sns\n",
    "import arviz as az\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we download a small example dataset from github. Saving this dataset on your local device (under 'resources/data/fcon1000.csv' for example) saves time and bandwidth if you re-run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we download a small example dataset from github. Saving this dataset on your local device (under 'resources/data/fcon1000.csv' for example) saves time and bandwidth if you re-run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# First download the dataset from github\n",
    "# fcon=pd.read_csv(\"https://raw.githubusercontent.com/pcn-toolkit/pcn-toolkit/master/resources/data/fcon1000.csv\")\n",
    "data = pd.read_csv(\"resources/data/fcon1000.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of sex and site in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "data[\"sex \"] = np.where(data[\"sex\"] == 1, [\"male\"], [\"female\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our HBR models will use random effects to model differences between sites. Because the random effects are best captured when there are enough samples of each effect in the data, we will have to remove some sites that are too small. We will filter out sites for which any of the sexes is represented by less than 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Group the data by site and sex\n",
    "site_counts = data.groupby([\"site\", \"sex\"]).size().reset_index(name=\"counts\")  # type: ignore\n",
    "\n",
    "# Get the sites with only one sex present\n",
    "sex_count_per_site = site_counts[\"site\"].value_counts()\n",
    "sites_with_one_sex = sex_count_per_site[sex_count_per_site == 1]\n",
    "sites_with_one_sex.index\n",
    "\n",
    "# remove the sites with less than 10 samples\n",
    "data = data[~data[\"site\"].isin(sites_with_one_sex.index)]\n",
    "\n",
    "\n",
    "# find the sites that have less than 10 samples\n",
    "site_counts = site_counts[site_counts[\"counts\"] < 10]\n",
    "\n",
    "# remove the sites with less than 10 samples\n",
    "data = data[~data[\"site\"].isin(site_counts[\"site\"])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the seven largest sites, which we will use for train and transfer. Two of those are randomly selected for transfering later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "site_counts = data.groupby([\"site\"]).size().reset_index(name=\"counts\")  # type: ignore\n",
    "site_counts = site_counts.sort_values(\"counts\", ascending=False)\n",
    "site_counts = site_counts.head(7)\n",
    "\n",
    "np.random.seed(45)\n",
    "# randomly select 2 sites from the top 7 sites for transfering\n",
    "transfer_sites = site_counts.sample(2)[\"site\"]\n",
    "transfer_data = data[data[\"site\"].isin(transfer_sites)]\n",
    "\n",
    "# The remaining sites are used for training the model\n",
    "fit_sites = site_counts[~site_counts.isin(transfer_sites)][\"site\"]\n",
    "fit_sites.dropna(inplace=True)\n",
    "fit_data = data[data[\"site\"].isin(fit_sites)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the data into `NormData` objects. All functions in the PCNtoolkit expect the data to be provided as instances of the `NormData` class. The class manages all preprocessing, basis expansions, and dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covariates = [\"age\"]\n",
    "batch_effects = [\"sex\", \"site\"]\n",
    "response_vars = [\"rh_MeanThickness_thickness\", \"WM-hypointensities\"]\n",
    "\n",
    "# Create a normdata object from the downloaded data\n",
    "normdata = NormData.from_dataframe(\n",
    "    name=\"fit\",  # name of the dataset\n",
    "    dataframe=fit_data,  # pandas dataframe\n",
    "    covariates=covariates,\n",
    "    batch_effects=batch_effects,\n",
    "    response_vars=response_vars,\n",
    ")\n",
    "\n",
    "# Create a transfer data object from the downloaded data\n",
    "transfer_data = NormData.from_dataframe(\n",
    "    name=\"transfer\",\n",
    "    dataframe=transfer_data,\n",
    "    covariates=covariates,\n",
    "    batch_effects=batch_effects,\n",
    "    response_vars=response_vars,\n",
    ")\n",
    "\n",
    "fit_data, predict_data = normdata.train_test_split(splits=(0.8, 0.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the normative model\n",
    "\n",
    "The normative model will be configured using a `NormConf` object, containing save and log paths and the preprocessing configurations, and a `RegConf` object, specific to the regression model type. Our `NormConf` configuration contains canonical paths, a standardization step for both the input as as the output data, and a Bspline basis expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration of normative model is valid.\n"
     ]
    }
   ],
   "source": [
    "# Create a NormConf object\n",
    "norm_conf = NormConf(\n",
    "    savemodel=True,\n",
    "    save_dir=\"/project/3022000.05/projects/stijdboe/wdir/save_dir\",\n",
    "    inscaler=\"none\",\n",
    "    outscaler=\"none\",\n",
    "    basis_function=\"bspline\",\n",
    "    order=3,\n",
    "    nknots=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the regression model\n",
    "\n",
    "HBR models need to specificy (possibly recursive) parameter configurations. Here, we configure a HBR model with a SHASHb likelihood, a bspline regression in `mu` and `sigma`, and a random effect in the intercept of `mu`. Note that because sigma has to be strictly positive, we specify a `softplus` mapping, so that the output of the linear regression is mapped to the positive domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration of regression model is valid.\n"
     ]
    }
   ],
   "source": [
    "blr_conf = BLRConf(\n",
    "    intercept=True,\n",
    "    random_intercept=False,\n",
    "    heteroskedastic=False,\n",
    "    intercept_var=False,\n",
    "    n_iter=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine normative and hbr conf in normative model\n",
    "We can either use the NormHBR constructor, or the factory method to create a normative HBR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Using the constructor\n",
    "norm_blr = NormBLR(norm_conf=norm_conf, reg_conf=blr_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No python path specified. Using interpreter path of current process: /project/3022000.05/projects/stijdboe/envs/pcntk_dev/bin/python\n"
     ]
    }
   ],
   "source": [
    "runner = Runner(\n",
    "    norm_blr,\n",
    "    cross_validate=True,\n",
    "    cv_folds=10,\n",
    "    parallelize=True,\n",
    "    job_type=\"slurm\",\n",
    "    n_jobs=2,\n",
    "    log_dir=\"/project/3022000.05/projects/stijdboe/wdir/log_dir\",\n",
    "    temp_dir=\"/project/3022000.05/projects/stijdboe/wdir/temp_dir\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status Monitor:\n",
      "------------------------------------------------------------\n",
      "Job ID     Name     State     Time     Nodes\n",
      "------------------------------------------------------------\n",
      "46669232   1        PENDING   0:00             \n",
      "46669231   0        PENDING   0:00             \n",
      "46669232   1        PENDING   0:00             \n",
      "46669231   0        PENDING   0:00             \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfit_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/3022000.05/projects/stijdboe/envs/pcntk_dev/lib/python3.12/site-packages/pcntoolkit/runner.py:96\u001b[0m, in \u001b[0;36mRunner.fit_predict\u001b[0;34m(self, fit_data, predict_data)\u001b[0m\n\u001b[1;32m     94\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_fit_predict_chunk_fn()\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit_fit_predict_jobs(fn, fit_data, predict_data)\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/3022000.05/projects/stijdboe/envs/pcntk_dev/lib/python3.12/site-packages/pcntoolkit/runner.py:338\u001b[0m, in \u001b[0;36mRunner.wait_for_jobs\u001b[0;34m(self, check_interval)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_job_ids[job_name]\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_job_ids:\n\u001b[0;32m--> 338\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# Move past the status lines before printing completion message\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_job_ids) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll jobs completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "runner.fit_predict(fit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration of normative model is valid.\n",
      "Configuration of normative model is valid.\n",
      "Configuration of regression model is valid.\n",
      "Configuration of regression model is valid.\n",
      "Configuration of regression model is valid.\n",
      "Configuration of regression model is valid.\n"
     ]
    }
   ],
   "source": [
    "new_nm = load_normative_model(os.path.join(norm_conf.save_dir, \"folds\", \"fold_0\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['normative_model_dict.json', 'reg_model_rh_MeanThickness_thickness.json', 'reg_model_WM-hypointensities.json']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(os.path.join(norm_conf.save_dir, \"folds\", \"fold_0\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/3022000.05/projects/stijdboe/wdir/save_dir\n"
     ]
    }
   ],
   "source": [
    "print(norm_conf.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_refactor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
