{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Runner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pcntoolkit.dataio.norm_data import NormData\n",
    "from pcntoolkit.normative_model.norm_conf import NormConf\n",
    "from pcntoolkit.normative_model.norm_blr import NormBLR\n",
    "from pcntoolkit.regression_model.blr.blr_conf import BLRConf\n",
    "from pcntoolkit.normative_model.norm_factory import load_normative_model\n",
    "from pcntoolkit.normative_model.norm_factory import create_normative_model\n",
    "from pcntoolkit.regression_model.hbr.hbr_conf import HBRConf\n",
    "from pcntoolkit.runner import Runner\n",
    "from pcntoolkit.regression_model.hbr.param import Param\n",
    "\n",
    "import seaborn as sns\n",
    "import arviz as az\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we download a small example dataset from github. Saving this dataset on your local device (under 'resources/data/fcon1000.csv' for example) saves time and bandwidth if you re-run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we download a small example dataset from github. Saving this dataset on your local device (under 'resources/data/fcon1000.csv' for example) saves time and bandwidth if you re-run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# First download the dataset from github\n",
    "# fcon=pd.read_csv(\"https://raw.githubusercontent.com/pcn-toolkit/pcn-toolkit/master/resources/data/fcon1000.csv\")\n",
    "data = pd.read_csv(\"resources/data/fcon1000.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of sex and site in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "data[\"sex \"] = np.where(data[\"sex\"] == 1, [\"male\"], [\"female\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our HBR models will use random effects to model differences between sites. Because the random effects are best captured when there are enough samples of each effect in the data, we will have to remove some sites that are too small. We will filter out sites for which any of the sexes is represented by less than 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Group the data by site and sex\n",
    "site_counts = data.groupby([\"site\", \"sex\"]).size().reset_index(name=\"counts\")  # type: ignore\n",
    "\n",
    "# Get the sites with only one sex present\n",
    "sex_count_per_site = site_counts[\"site\"].value_counts()\n",
    "sites_with_one_sex = sex_count_per_site[sex_count_per_site == 1]\n",
    "sites_with_one_sex.index\n",
    "\n",
    "# remove the sites with less than 10 samples\n",
    "data = data[~data[\"site\"].isin(sites_with_one_sex.index)]\n",
    "\n",
    "\n",
    "# find the sites that have less than 10 samples\n",
    "site_counts = site_counts[site_counts[\"counts\"] < 10]\n",
    "\n",
    "# remove the sites with less than 10 samples\n",
    "data = data[~data[\"site\"].isin(site_counts[\"site\"])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the seven largest sites, which we will use for train and transfer. Two of those are randomly selected for transfering later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "site_counts = data.groupby([\"site\"]).size().reset_index(name=\"counts\")  # type: ignore\n",
    "site_counts = site_counts.sort_values(\"counts\", ascending=False)\n",
    "site_counts = site_counts.head(7)\n",
    "\n",
    "np.random.seed(45)\n",
    "# randomly select 2 sites from the top 7 sites for transfering\n",
    "transfer_sites = site_counts.sample(2)[\"site\"]\n",
    "transfer_data = data[data[\"site\"].isin(transfer_sites)]\n",
    "\n",
    "# The remaining sites are used for training the model\n",
    "fit_sites = site_counts[~site_counts.isin(transfer_sites)][\"site\"]\n",
    "fit_sites.dropna(inplace=True)\n",
    "fit_data = data[data[\"site\"].isin(fit_sites)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the data into `NormData` objects. All functions in the PCNtoolkit expect the data to be provided as instances of the `NormData` class. The class manages all preprocessing, basis expansions, and dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covariates = [\"age\"]\n",
    "batch_effects = [\"sex\", \"site\"]\n",
    "response_vars = [\"rh_MeanThickness_thickness\", \"WM-hypointensities\"]\n",
    "\n",
    "# Create a normdata object from the downloaded data\n",
    "normdata = NormData.from_dataframe(\n",
    "    name=\"fit\",  # name of the dataset\n",
    "    dataframe=fit_data,  # pandas dataframe\n",
    "    covariates=covariates,\n",
    "    batch_effects=batch_effects,\n",
    "    response_vars=response_vars,\n",
    ")\n",
    "\n",
    "# Create a transfer data object from the downloaded data\n",
    "transfer_data = NormData.from_dataframe(\n",
    "    name=\"transfer\",\n",
    "    dataframe=transfer_data,\n",
    "    covariates=covariates,\n",
    "    batch_effects=batch_effects,\n",
    "    response_vars=response_vars,\n",
    ")\n",
    "\n",
    "fit_data, predict_data = normdata.train_test_split(splits=(0.8, 0.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the normative model\n",
    "\n",
    "The normative model will be configured using a `NormConf` object, containing save and log paths and the preprocessing configurations, and a `RegConf` object, specific to the regression model type. Our `NormConf` configuration contains canonical paths, a standardization step for both the input as as the output data, and a Bspline basis expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration of normative model is valid.\n"
     ]
    }
   ],
   "source": [
    "# Create a NormConf object\n",
    "norm_conf = NormConf(\n",
    "    savemodel=True,\n",
    "    saveresults=True,\n",
    "    # save_dir=\"/project/3022000.05/projects/stijdboe/wdir/save_dir\",\n",
    "    save_dir=\"/Users/stijndeboer/Projects/PCN/PCNtoolkit/example_notebooks/save_dir\",\n",
    "    inscaler=\"none\",\n",
    "    outscaler=\"none\",\n",
    "    basis_function=\"bspline\",\n",
    "    order=3,\n",
    "    nknots=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the regression model\n",
    "\n",
    "HBR models need to specificy (possibly recursive) parameter configurations. Here, we configure a HBR model with a SHASHb likelihood, a bspline regression in `mu` and `sigma`, and a random effect in the intercept of `mu`. Note that because sigma has to be strictly positive, we specify a `softplus` mapping, so that the output of the linear regression is mapped to the positive domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration of regression model is valid.\n"
     ]
    }
   ],
   "source": [
    "blr_conf = BLRConf(\n",
    "    intercept=True,\n",
    "    random_intercept=False,\n",
    "    heteroskedastic=False,\n",
    "    intercept_var=False,\n",
    "    n_iter=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine normative and hbr conf in normative model\n",
    "We can either use the NormHBR constructor, or the factory method to create a normative HBR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Using the constructor\n",
    "norm_blr = NormBLR(norm_conf=norm_conf, reg_conf=blr_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No python path specified. Using interpreter path of current process: /opt/anaconda3/envs/ptk_dev/bin/python\n"
     ]
    }
   ],
   "source": [
    "runner = Runner(\n",
    "    norm_blr,\n",
    "    cross_validate=False,\n",
    "    cv_folds=10,\n",
    "    parallelize=False,\n",
    "    job_type=\"local\",\n",
    "    n_jobs=2,\n",
    "    # log_dir=\"/project/3022000.05/projects/stijdboe/wdir/log_dir\",\n",
    "    # temp_dir=\"/project/3022000.05/projects/stijdboe/wdir/temp_dir\",\n",
    "    log_dir=\"/Users/stijndeboer/Projects/PCN/PCNtoolkit/example_notebooks/log_dir\",\n",
    "    temp_dir=\"/Users/stijndeboer/Projects/PCN/PCNtoolkit/example_notebooks/temp_dir\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to fit and predict 2 models\n",
      "Fitting and predicting model for rh_MeanThickness_thickness\n",
      "Fitting and predicting model for WM-hypointensities\n",
      "93103 Saving model to /Users/stijndeboer/Projects/PCN/PCNtoolkit/example_notebooks/save_dir\n",
      "Computing zscores for rh_MeanThickness_thickness\n",
      "Computing zscores for WM-hypointensities\n",
      "Computing centiles for rh_MeanThickness_thickness\n",
      "Computing centiles for WM-hypointensities\n",
      "Computing centiles for rh_MeanThickness_thickness\n",
      "Computing centiles for WM-hypointensities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ptk_dev/lib/python3.12/site-packages/pcntoolkit/util/evaluator.py:341: RuntimeWarning: invalid value encountered in log\n",
      "  nll = -np.mean(y * np.log(yhat) + (1 - y) * np.log(1 - yhat))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All jobs completed!\n"
     ]
    }
   ],
   "source": [
    "runner.fit_predict(fit_data, predict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Path /Users/stijndeboer/Projects/PCN/PCNtoolkit/example_notebooks/save_dir/folds/fold_0 does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/ptk_dev/lib/python3.12/site-packages/pcntoolkit/normative_model/norm_factory.py:94\u001b[0m, in \u001b[0;36mload_normative_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnormative_model.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     95\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/stijndeboer/Projects/PCN/PCNtoolkit/example_notebooks/save_dir/folds/fold_0/model/normative_model.json'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_nm \u001b[38;5;241m=\u001b[39m \u001b[43mload_normative_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfolds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfold_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ptk_dev/lib/python3.12/site-packages/pcntoolkit/normative_model/norm_factory.py:97\u001b[0m, in \u001b[0;36mload_normative_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     95\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     99\u001b[0m norm_conf \u001b[38;5;241m=\u001b[39m NormConf\u001b[38;5;241m.\u001b[39mfrom_dict(metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm_conf\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    100\u001b[0m model_name \u001b[38;5;241m=\u001b[39m norm_conf\u001b[38;5;241m.\u001b[39mnormative_model_name\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Path /Users/stijndeboer/Projects/PCN/PCNtoolkit/example_notebooks/save_dir/folds/fold_0 does not exist."
     ]
    }
   ],
   "source": [
    "new_nm = load_normative_model(os.path.join(norm_conf.save_dir, \"folds\", \"fold_0\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/stijndeboer/Projects/PCN/PCNtoolkit/example_notebooks/save_dir/folds/fold_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfolds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfold_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/stijndeboer/Projects/PCN/PCNtoolkit/example_notebooks/save_dir/folds/fold_0'"
     ]
    }
   ],
   "source": [
    "print(os.listdir(os.path.join(norm_conf.save_dir, \"folds\", \"fold_0\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/3022000.05/projects/stijdboe/wdir/save_dir\n"
     ]
    }
   ],
   "source": [
    "print(norm_conf.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_refactor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
