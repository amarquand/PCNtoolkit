{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HBR transfer example\n",
    "\n",
    "Welcome to this example/tutorial notebook that will go through the fitting, evaluation, transfering, and extending of HBR models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pcntoolkit.dataio.norm_data import NormData\n",
    "from pcntoolkit.normative_model.norm_conf import NormConf\n",
    "from pcntoolkit.normative_model.norm_hbr import NormHBR\n",
    "from pcntoolkit.normative_model.norm_factory import create_normative_model\n",
    "from pcntoolkit.regression_model.hbr.hbr_conf import HBRConf\n",
    "from pcntoolkit.regression_model.hbr.param import make_param\n",
    "from pcntoolkit.regression_model.hbr.hbr import HBR\n",
    "from pcntoolkit.util.plotter import plot_centiles, plot_qq\n",
    "import arviz as az\n",
    "\n",
    "import pymc.math as math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we download a small example dataset from github. Saving this dataset on your local device (under 'resources/data/fcon1000.csv' for example) saves time and bandwidth if you re-run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook for the first time, you need to download the dataset from github.\n",
    "# If you have already downloaded the dataset, you can comment out the following line\n",
    "pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/predictive-clinical-neuroscience/PCNtoolkit-demo/refs/heads/main/data/fcon1000.csv\"\n",
    ").to_csv(\"resources/data/fcon1000.csv\", index=False)\n",
    "data = pd.read_csv(\"resources/data/fcon1000.csv\")\n",
    "covariates = [\"age\"]\n",
    "batch_effects = [\"sex\", \"site\"]\n",
    "response_vars = [\"rh_MeanThickness_thickness\", \"WM-hypointensities\"]\n",
    "norm_data = NormData.from_dataframe(\n",
    "    name=\"full\",\n",
    "    dataframe=data,\n",
    "    covariates=[\"age\"],\n",
    "    batch_effects=[\"sex\", \"site\"],\n",
    "    response_vars=[\"rh_MeanThickness_thickness\", \"WM-hypointensities\"],\n",
    ")\n",
    "\n",
    "# Leave two sites out for doing transfer and extend later\n",
    "transfer_sites = [\"Milwaukee_b\", \"Oulu\"]\n",
    "transfer_data, fit_data = norm_data.split_batch_effects(\n",
    "    {\"site\": transfer_sites}, names=(\"transfer\", \"fit\")\n",
    ")\n",
    "\n",
    "# Split into train and test sets\n",
    "train, test = fit_data.train_test_split()\n",
    "transfer_train, transfer_test = transfer_data.train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the normative model\n",
    "\n",
    "The normative model will be configured using a `NormConf` object, containing save and log paths and the preprocessing configurations, and a `RegConf` object, specific to the regression model type. \n",
    "\n",
    "Our `NormConf` object configures:\n",
    "- a save path paths and whether to save the model and results\n",
    "- a standardization step for both the covariates (inscaler) and the response vars (outscaler)\n",
    "- a Bspline basis expansion of order 3 with 5 knots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration of normative model is valid.\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"resources/hbr/save_dir\"\n",
    "# Create a NormConf object\n",
    "norm_conf = NormConf(\n",
    "    savemodel=True,\n",
    "    saveresults=True,\n",
    "    save_dir=save_dir,\n",
    "    inscaler=\"standardize\",\n",
    "    outscaler=\"standardize\",\n",
    "    basis_function=\"linear\",\n",
    "    basis_function_kwargs={\"order\": 3, \"nknots\": 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the regression model\n",
    "\n",
    "HBR models need to specificy (possibly recursive) parameter configurations. Here, we configure a HBR model with a Normal likelihood, a bspline regression in `mu` and `sigma`, and a random effect in the intercept of `mu`. Note that because sigma has to be strictly positive, we specify a `softplus` mapping, so that the output of the linear regression is mapped to the positive domain. We also use the mapping_params to scale the mapping by a factor of 3, to avoid spikes in the resulting density. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "mu = make_param(\n",
    "    name=\"mu\",\n",
    "    linear=True,\n",
    "    # slope=make_param(dist_name=\"Normal\", dist_params=(0.0, 5.0)),\n",
    "    slope=make_param(random=False),\n",
    "    intercept=make_param(\n",
    "        random=True,\n",
    "        sigma=make_param(dist_name=\"HalfNormal\", dist_params=(1.0,)),\n",
    "        mu=make_param(dist_name=\"Normal\", dist_params=(0.0, 0.5)),\n",
    "    ),\n",
    ")\n",
    "sigma = make_param(\n",
    "    name=\"sigma\",\n",
    "    linear=False,\n",
    "    slope=make_param(dist_name=\"Normal\", dist_params=(0.0, 3.0)),\n",
    "    intercept=make_param(\n",
    "        dist_name=\"Normal\",\n",
    "        dist_params=(\n",
    "            1.0,\n",
    "            1.0,\n",
    "        ),\n",
    "    ),\n",
    "    mapping=\"softplus\",\n",
    "    mapping_params=(0.0, 3.0),\n",
    ")\n",
    "\n",
    "epsilon = make_param(\n",
    "    name=\"epsilon\",\n",
    "    dist_name=\"Normal\",\n",
    "    dist_params=(0.0, 1.0),\n",
    ")\n",
    "\n",
    "delta = make_param(\n",
    "    name=\"delta\",\n",
    "    dist_name=\"Normal\",\n",
    "    dist_params=(1.0, 1.0),\n",
    "    mapping=\"softplus\",\n",
    "    mapping_params=(0.0, 2.0, 0.3),\n",
    ")\n",
    "\n",
    "\n",
    "# Configure the HBRConf object\n",
    "hbr_conf = HBRConf(\n",
    "    draws=2048,\n",
    "    tune=512,\n",
    "    chains=4,\n",
    "    pymc_cores=16,\n",
    "    likelihood=\"Normal\",\n",
    "    mu=mu,\n",
    "    sigma=sigma,\n",
    "    epsilon=epsilon,\n",
    "    delta=delta,\n",
    "    nuts_sampler=\"nutpie\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine normative and hbr conf in normative model\n",
    "We can either use the NormHBR constructor, or the factory method to create a normative HBR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pcntoolkit.normative_model.norm_hbr.NormHBR at 0x3321ffcb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using the constructor\n",
    "norm_hbr = NormHBR(norm_conf=norm_conf, reg_conf=hbr_conf)\n",
    "display(norm_hbr)\n",
    "del norm_hbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pcntoolkit.normative_model.norm_hbr.NormHBR at 0x3321ffcb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the factory method\n",
    "norm_hbr = create_normative_model(norm_conf, hbr_conf)\n",
    "norm_hbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to fit and predict 2 models\n",
      "Fitting and predicting model for rh_MeanThickness_thickness\n"
     ]
    },
    {
     "ename": "TypingError",
     "evalue": "Failed in nopython mode pipeline (step: nopython frontend)\nFailed in nopython mode pipeline (step: nopython frontend)\nInvalid use of type(CPUDispatcher(<function careduce_add at 0x176cb2840>)) with parameters (readonly array(float64, 2d, C))\nKnown signatures:\n * (Array(float64, 2, 'A', False, aligned=True),) -> array(float64, 1d, A)\nDuring: resolving callee type: type(CPUDispatcher(<function careduce_add at 0x176cb2840>))\nDuring: typing of call at /var/folders/m8/vtbcb7c96ms3mbjny3b70h3w0000gp/T/tmpivd0vkzq (23)\n\n\nFile \"../../../../../../var/folders/m8/vtbcb7c96ms3mbjny3b70h3w0000gp/T/tmpivd0vkzq\", line 23:\ndef numba_funcified_fgraph(_unconstrained_point, y, X, sex_data, site_data):\n    <source elided>\n    # Sum{axis=1}(X)\n    tensor_variable_10 = careduce_add(X)\n    ^\n\nDuring: resolving callee type: type(CPUDispatcher(<function numba_funcified_fgraph at 0x177bdb600>))\nDuring: typing of call at /opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/nutpie/compile_pymc.py (637)\n\nDuring: resolving callee type: type(CPUDispatcher(<function numba_funcified_fgraph at 0x177bdb600>))\nDuring: typing of call at /opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/nutpie/compile_pymc.py (637)\n\nDuring: resolving callee type: type(CPUDispatcher(<function numba_funcified_fgraph at 0x177bdb600>))\nDuring: typing of call at /opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/nutpie/compile_pymc.py (637)\n\nDuring: resolving callee type: type(CPUDispatcher(<function numba_funcified_fgraph at 0x177bdb600>))\nDuring: typing of call at /opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/nutpie/compile_pymc.py (637)\n\nDuring: resolving callee type: type(CPUDispatcher(<function numba_funcified_fgraph at 0x177bdb600>))\nDuring: typing of call at /opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/nutpie/compile_pymc.py (637)\n\nDuring: resolving callee type: type(CPUDispatcher(<function numba_funcified_fgraph at 0x177bdb600>))\nDuring: typing of call at /opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/nutpie/compile_pymc.py (637)\n\n\nFile \"../../../../../../opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/nutpie/compile_pymc.py\", line 637:\n    def extract_shared(x, user_data_):\n        <source elided>\n\n        return inner(x, *_shared_tuple)\n        ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# predictions = norm_hbr.fit_predict(train, test)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnorm_hbr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/pcntoolkit/normative_model/norm_base.py:270\u001b[0m, in \u001b[0;36mNormBase.fit_predict\u001b[0;34m(self, fit_data, predict_data)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfocus(responsevar)\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting and predicting model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponsevar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp_fit_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresp_predict_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fitted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/pcntoolkit/normative_model/norm_hbr.py:145\u001b[0m, in \u001b[0;36mNormHBR._fit_predict\u001b[0;34m(self, fit_data, predict_data)\u001b[0m\n\u001b[1;32m    143\u001b[0m fit_hbrdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormdata_to_hbrdata(fit_data)\n\u001b[1;32m    144\u001b[0m predict_hbrdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormdata_to_hbrdata(predict_data)\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfocused_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfit_hbrdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_hbrdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/pcntoolkit/regression_model/hbr/hbr.py:221\u001b[0m, in \u001b[0;36mHBR.fit_predict\u001b[0;34m(self, fit_hbrdata, predict_hbrdata)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile_model(fit_hbrdata)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpymc_model:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midata \u001b[38;5;241m=\u001b[39m \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpymc_cores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnuts_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnuts_sampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fitted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    230\u001b[0m predict_hbrdata\u001b[38;5;241m.\u001b[39mset_data_in_existing_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpymc_model)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/pymc/sampling/mcmc.py:781\u001b[0m, in \u001b[0;36msample\u001b[0;34m(draws, tune, chains, cores, random_seed, progressbar, progressbar_theme, step, var_names, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, blas_cores, model, compile_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    777\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel can not be sampled with NUTS alone. It either has discrete variables or a non-differentiable log-probability.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    778\u001b[0m         )\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m joined_blas_limiter():\n\u001b[0;32m--> 781\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_external_nuts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m            \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnuts_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdraws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdraws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_accept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnuts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget_accept\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43minitvals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitvals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvar_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43midata_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midata_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnuts_sampler_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnuts_sampler_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exclusive_nuts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m provided_steps:\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;66;03m# Special path for NUTS initialization\u001b[39;00m\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnuts\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/pymc/sampling/mcmc.py:337\u001b[0m, in \u001b[0;36m_sample_external_nuts\u001b[0;34m(sampler, draws, tune, chains, target_accept, random_seed, initvals, model, var_names, progressbar, idata_kwargs, compute_convergence_checks, nuts_sampler_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwarg \u001b[38;5;129;01min\u001b[39;00m nuts_sampler_kwargs:\n\u001b[1;32m    336\u001b[0m         compile_kwargs[kwarg] \u001b[38;5;241m=\u001b[39m nuts_sampler_kwargs\u001b[38;5;241m.\u001b[39mpop(kwarg)\n\u001b[0;32m--> 337\u001b[0m compiled_model \u001b[38;5;241m=\u001b[39m \u001b[43mnutpie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_pymc_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompile_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m t_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    342\u001b[0m idata \u001b[38;5;241m=\u001b[39m nutpie\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    343\u001b[0m     compiled_model,\n\u001b[1;32m    344\u001b[0m     draws\u001b[38;5;241m=\u001b[39mdraws,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnuts_sampler_kwargs,\n\u001b[1;32m    351\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/nutpie/compile_pymc.py:391\u001b[0m, in \u001b[0;36mcompile_pymc_model\u001b[0;34m(model, backend, gradient_backend, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumba\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumba\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_pymc_model_numba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile_pymc_model_jax(\n\u001b[1;32m    394\u001b[0m         model, gradient_backend\u001b[38;5;241m=\u001b[39mgradient_backend, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    395\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/nutpie/compile_pymc.py:207\u001b[0m, in \u001b[0;36m_compile_pymc_model_numba\u001b[0;34m(model, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m    201\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    203\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot cache compiled function .* as it uses dynamic globals\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    204\u001b[0m         category\u001b[38;5;241m=\u001b[39mnumba\u001b[38;5;241m.\u001b[39mNumbaWarning,\n\u001b[1;32m    205\u001b[0m     )\n\u001b[0;32m--> 207\u001b[0m     logp_numba \u001b[38;5;241m=\u001b[39m \u001b[43mnumba\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_sig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogp_numba_raw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m expand_shared_names \u001b[38;5;241m=\u001b[39m [var\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m expand_fn_pt\u001b[38;5;241m.\u001b[39mget_shared()]\n\u001b[1;32m    210\u001b[0m expand_numba_raw, c_sig_expand \u001b[38;5;241m=\u001b[39m _make_c_expand_func(\n\u001b[1;32m    211\u001b[0m     n_dim, n_expanded, expand_fn, user_data, expand_shared_names, shared_data\n\u001b[1;32m    212\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/decorators.py:275\u001b[0m, in \u001b[0;36mcfunc.<locals>.wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache:\n\u001b[1;32m    274\u001b[0m     res\u001b[38;5;241m.\u001b[39menable_caching()\n\u001b[0;32m--> 275\u001b[0m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire_compile_lock\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/ccallback.py:68\u001b[0m, in \u001b[0;36mCFunc.compile\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m cres \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mload_overload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sig,\n\u001b[1;32m     66\u001b[0m                                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_targetdescr\u001b[38;5;241m.\u001b[39mtarget_context)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cres \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     cres \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39msave_overload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sig, cres)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/ccallback.py:82\u001b[0m, in \u001b[0;36mCFunc._compile_uncached\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m sig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sig\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Compile native function as well as cfunc wrapper\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/dispatcher.py:84\u001b[0m, in \u001b[0;36m_FunctionCompiler.compile\u001b[0;34m(self, args, return_type)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retval\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/dispatcher.py:94\u001b[0m, in \u001b[0;36m_FunctionCompiler._compile_cached\u001b[0;34m(self, args, return_type)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mTypingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_failed_cache[key] \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/dispatcher.py:107\u001b[0m, in \u001b[0;36m_FunctionCompiler._compile_core\u001b[0;34m(self, args, return_type)\u001b[0m\n\u001b[1;32m    104\u001b[0m flags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_customize_flags(flags)\n\u001b[1;32m    106\u001b[0m impl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_implementation(args, {})\n\u001b[0;32m--> 107\u001b[0m cres \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_extra\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargetdescr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtyping_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargetdescr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                              \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mpipeline_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Check typing error if object mode is used\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cres\u001b[38;5;241m.\u001b[39mtyping_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m flags\u001b[38;5;241m.\u001b[39menable_pyobject:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/compiler.py:744\u001b[0m, in \u001b[0;36mcompile_extra\u001b[0;34m(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compiler entry point\u001b[39;00m\n\u001b[1;32m    721\u001b[0m \n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03mParameter\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;124;03m    compiler pipeline\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    742\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m pipeline_class(typingctx, targetctx, library,\n\u001b[1;32m    743\u001b[0m                           args, return_type, flags, \u001b[38;5;28mlocals\u001b[39m)\n\u001b[0;32m--> 744\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_extra\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/compiler.py:438\u001b[0m, in \u001b[0;36mCompilerBase.compile_extra\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlifted \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlifted_from \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_bytecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/compiler.py:506\u001b[0m, in \u001b[0;36mCompilerBase._compile_bytecode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03mPopulate and run pipeline for bytecode input\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfunc_ir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/compiler.py:485\u001b[0m, in \u001b[0;36mCompilerBase._compile_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mfail_reason \u001b[38;5;241m=\u001b[39m e\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_final_pipeline:\n\u001b[0;32m--> 485\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CompilerError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll available pipelines exhausted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/compiler.py:472\u001b[0m, in \u001b[0;36mCompilerBase._compile_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    470\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 472\u001b[0m     \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mcr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/compiler_machinery.py:368\u001b[0m, in \u001b[0;36mPassManager.run\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    365\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m mode pipeline (step: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \\\n\u001b[1;32m    366\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_name, pass_desc)\n\u001b[1;32m    367\u001b[0m patched_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_patch_error(msg, e)\n\u001b[0;32m--> 368\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m patched_exception\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/compiler_machinery.py:356\u001b[0m, in \u001b[0;36mPassManager.run\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    354\u001b[0m pass_inst \u001b[38;5;241m=\u001b[39m _pass_registry\u001b[38;5;241m.\u001b[39mget(pss)\u001b[38;5;241m.\u001b[39mpass_inst\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pass_inst, CompilerPass):\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runPass\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass_inst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLegacy pass in use\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire_compile_lock\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/compiler_machinery.py:311\u001b[0m, in \u001b[0;36mPassManager._runPass\u001b[0;34m(self, index, pss, internal_state)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m check(pss\u001b[38;5;241m.\u001b[39mrun_initialization, internal_state)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SimpleTimer() \u001b[38;5;28;01mas\u001b[39;00m pass_time:\n\u001b[0;32m--> 311\u001b[0m     mutated \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minternal_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SimpleTimer() \u001b[38;5;28;01mas\u001b[39;00m finalize_time:\n\u001b[1;32m    313\u001b[0m     mutated \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m check(pss\u001b[38;5;241m.\u001b[39mrun_finalizer, internal_state)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/compiler_machinery.py:273\u001b[0m, in \u001b[0;36mPassManager._runPass.<locals>.check\u001b[0;34m(func, compiler_state)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(func, compiler_state):\n\u001b[0;32m--> 273\u001b[0m     mangled \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiler_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mangled \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    275\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompilerPass implementations should return True/False. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompilerPass with name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m did not.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/typed_passes.py:112\u001b[0m, in \u001b[0;36mBaseTypeInference.run_pass\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03mType inference and legalization\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fallback_context(state, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m failed type inference\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    110\u001b[0m                       \u001b[38;5;241m%\u001b[39m (state\u001b[38;5;241m.\u001b[39mfunc_id\u001b[38;5;241m.\u001b[39mfunc_name,)):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# Type inference\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     typemap, return_type, calltypes, errs \u001b[38;5;241m=\u001b[39m \u001b[43mtype_inference_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtypingctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargetctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_ir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     state\u001b[38;5;241m.\u001b[39mtypemap \u001b[38;5;241m=\u001b[39m typemap\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# save errors in case of partial typing\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/typed_passes.py:93\u001b[0m, in \u001b[0;36mtype_inference_stage\u001b[0;34m(typingctx, targetctx, interp, args, return_type, locals, raise_errors)\u001b[0m\n\u001b[1;32m     91\u001b[0m     infer\u001b[38;5;241m.\u001b[39mbuild_constraint()\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# return errors in case of partial typing\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     errs \u001b[38;5;241m=\u001b[39m \u001b[43minfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     typemap, restype, calltypes \u001b[38;5;241m=\u001b[39m infer\u001b[38;5;241m.\u001b[39munify(raise_errors\u001b[38;5;241m=\u001b[39mraise_errors)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _TypingResults(typemap, restype, calltypes, errs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/numba/core/typeinfer.py:1091\u001b[0m, in \u001b[0;36mTypeInferer.propagate\u001b[0;34m(self, raise_errors)\u001b[0m\n\u001b[1;32m   1088\u001b[0m force_lit_args \u001b[38;5;241m=\u001b[39m [e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m errors\n\u001b[1;32m   1089\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ForceLiteralArg)]\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_lit_args:\n\u001b[0;32m-> 1091\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m reduce(operator\u001b[38;5;241m.\u001b[39mor_, force_lit_args)\n",
      "\u001b[0;31mTypingError\u001b[0m: Failed in nopython mode pipeline (step: nopython frontend)\nFailed in nopython mode pipeline (step: nopython frontend)\nInvalid use of type(CPUDispatcher(<function careduce_add at 0x176cb2840>)) with parameters (readonly array(float64, 2d, C))\nKnown signatures:\n * (Array(float64, 2, 'A', False, aligned=True),) -> array(float64, 1d, A)\nDuring: resolving callee type: type(CPUDispatcher(<function careduce_add at 0x176cb2840>))\nDuring: typing of call at /var/folders/m8/vtbcb7c96ms3mbjny3b70h3w0000gp/T/tmpivd0vkzq (23)\n\n\nFile \"../../../../../../var/folders/m8/vtbcb7c96ms3mbjny3b70h3w0000gp/T/tmpivd0vkzq\", line 23:\ndef numba_funcified_fgraph(_unconstrained_point, y, X, sex_data, site_data):\n    <source elided>\n    # Sum{axis=1}(X)\n    tensor_variable_10 = careduce_add(X)\n    ^\n\nDuring: resolving callee type: type(CPUDispatcher(<function numba_funcified_fgraph at 0x177bdb600>))\nDuring: typing of call at /opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/nutpie/compile_pymc.py (637)\n\nDuring: resolving callee type: type(CPUDispatcher(<function numba_funcified_fgraph at 0x177bdb600>))\nDuring: typing of call at /opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/nutpie/compile_pymc.py (637)\n\nDuring: resolving callee type: type(CPUDispatcher(<function numba_funcified_fgraph at 0x177bdb600>))\nDuring: typing of call at /opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/nutpie/compile_pymc.py (637)\n\nDuring: resolving callee type: type(CPUDispatcher(<function numba_funcified_fgraph at 0x177bdb600>))\nDuring: typing of call at /opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/nutpie/compile_pymc.py (637)\n\nDuring: resolving callee type: type(CPUDispatcher(<function numba_funcified_fgraph at 0x177bdb600>))\nDuring: typing of call at /opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/nutpie/compile_pymc.py (637)\n\nDuring: resolving callee type: type(CPUDispatcher(<function numba_funcified_fgraph at 0x177bdb600>))\nDuring: typing of call at /opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/nutpie/compile_pymc.py (637)\n\n\nFile \"../../../../../../opt/anaconda3/envs/dev_refactor_2/lib/python3.12/site-packages/nutpie/compile_pymc.py\", line 637:\n    def extract_shared(x, user_data_):\n        <source elided>\n\n        return inner(x, *_shared_tuple)\n        ^\n"
     ]
    }
   ],
   "source": [
    "# predictions = norm_hbr.fit_predict(train, test)\n",
    "norm_hbr.fit_predict(train, test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the model \n",
    "\n",
    "The norm_hbr model contains a collection of regression models, one for each response variable. We can inspect those models individually by calling `norm_hbr.regression_models.get(\"{responsevar}\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'norm_hbr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model: HBR \u001b[38;5;241m=\u001b[39m \u001b[43mnorm_hbr\u001b[49m\u001b[38;5;241m.\u001b[39mregression_models\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrh_MeanThickness_thickness\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mpymc_model\u001b[38;5;241m.\u001b[39mto_graphviz()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'norm_hbr' is not defined"
     ]
    }
   ],
   "source": [
    "model: HBR = norm_hbr.regression_models.get(\"rh_MeanThickness_thickness\")  # type: ignore\n",
    "model.pymc_model.to_graphviz()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the arviz library to inspect the posterior samples (trace) of the model. Here we only use the 'plot_trace' function to inspect the trace of the model, but there are many other useful functions available. If you are not familiar with arviz, we recommend checking out the [arviz documentation](https://arviz-devs.github.io/arviz/index.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = model.idata  # type: ignore\n",
    "\n",
    "plt.tight_layout()\n",
    "az.plot_trace(\n",
    "    idata.posterior,\n",
    "    var_names=[\n",
    "        \"~mu_samples\",\n",
    "        \"~sigma_samples\",\n",
    "        \"~epsilon_samples\",\n",
    "        \"~delta_samples\",\n",
    "        \"~intercept_mu\",\n",
    "        \"~scaled_site_offset_intercept_mu\",\n",
    "        \"~scaled_sex_offset_intercept_mu\",\n",
    "    ],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Calling `predict` will extend the predict_data object with a number of useful arrays.\n",
    "1. `measures`: DataArray, which contains a number of evaluation statistics. \n",
    "1. `zscores`: the predicted z-scores for each datapoint.  \n",
    "1. `centiles`: the predicted centiles of variation evaluated at each covariate in the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test.measures.to_pandas().T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets with a zscores DataArray will have the `.plot_qq()` function available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test.zscores.to_pandas())  # the zscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test.centiles.to_dataframe().unstack(level=[\"response_vars\", \"cdf\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qq(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `plot_centiles()` can be called as a function of the model. A synthetic dataset is created internally, so we need to pass the original dataset (`train` in this case) as a template. We also need to pass which covariate is to be plotted on the x-axis, and the batch-effects for which the centiles are to be plotted. \n",
    "\n",
    "The lines correspond to the CDF values of: [0.05, 0.25, 0.5, 0.75, 0.95]. It is also possible to pass a list of CDF values to plot.\n",
    "\n",
    "It may seem strange that the centiles do not match the plotted data, but that is because the centiles are calculated for a single batch effect, and it is superimposed on the full dataset. The blue markers correspond to the data for which the centiles are calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_centiles(\n",
    "    norm_hbr,\n",
    "    train,\n",
    "    covariate=\"age\",\n",
    "    show_data=True,\n",
    "    hue_data=\"sex\",\n",
    "    markers_data=\"site\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of 0.1587 and 0.8413 correspond to a standard deviation of -1 and 1. We plot the centiles again for these values, and we also highlight a specific site. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_centiles(\n",
    "    norm_hbr,\n",
    "    train,\n",
    "    covariate=\"age\",\n",
    "    cummul_densities=[0.1587, 0.8413],\n",
    "    show_data=True,\n",
    "    batch_effects={\"site\": [\"Beijing_Zang\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer\n",
    "\n",
    "If we transfer to new dataset, we use the samples from the posterior to derive priors for a new model. That new model is then fitted on the new dataset. Our new model will not work on the old data. To give some extra control over the fit, we let the `freedom` parameter control the variance of the derived factorized posterior. Here we set it to something small (1e-4) to create a spike prior on the model parameters, to ensure that we do not 'forget' about what we learned from the original data. If we have a very large new dataset, we can set the freedom parameter to something bigger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcntoolkit.normative_model.norm_factory import load_normative_model\n",
    "\n",
    "save_dir = \"resources/hbr/save_dir\"\n",
    "model = load_normative_model(save_dir)\n",
    "\n",
    "transfered_model = model.transfer_predict(\n",
    "    transfer_train, transfer_test, freedom=1e-4, nuts_sampler=\"nutpie\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfered_model.predict(transfer_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save, load, predict, and plot just as before. \n",
    "\n",
    "If the orinal save directory path was `{path}/{save_dir}`, then the transfered model's save directory path will be `{path}/{save_dir}_tansfer`, and the same holds for the log dir path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend\n",
    "\n",
    "Extending a model on new data amounts to generating synthetic data according to the learned distribution, merging that with the new data, and fitting a new model on that merged dataset. This enables true federated learning, because the original data does not need to be shipped with the model to extend it to a new dataset. \n",
    "\n",
    "Because this is an extended model, we can make predictions on all sites, even those that were only present in the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"resources/hbr/save_dir\"\n",
    "from pcntoolkit.normative_model.norm_factory import load_normative_model\n",
    "\n",
    "model = load_normative_model(save_dir)\n",
    "\n",
    "extended_model = model.extend_predict(transfer_train, transfer_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_model.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, now you have seen how to:\n",
    "- Use the NormData class to load in your data\n",
    "- Create and fit a normative model\n",
    "- Get the evaluation statistics, and create some useful plots\n",
    "- Transfer the model to another dataset\n",
    "- Extend the model to another dataset\n",
    "\n",
    "We hope this tutorial was useful. If you have any questions or remarks, please let us know on GitHub. Thanks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_refactor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
