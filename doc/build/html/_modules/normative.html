

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>normative &mdash; Predictive Clinical Neuroscience Toolkit 0.20 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pages/css/pcntoolkit_tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pages/css/pcntoolkit.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pages/css/pcntoolkit_nomaxwidth.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/tabs.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/pcn-logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pages/installation.html">Installation</a></li>
</ul>
<p class="caption"><span class="caption-text">Background</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pages/pcntoolkit_background.html">PCNtoolkit Background</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/pcntoolkit_background.html#intro-to-normative-modelling">Intro to normative modelling</a></li>
</ul>
<p class="caption"><span class="caption-text">Function &amp; Class Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modindex.html">Module Index</a></li>
</ul>
<p class="caption"><span class="caption-text">Current Events</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pages/updates.html">Updates</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pages/tutorial_CPC2020.html">Gaussian Process Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/tutorial_ROIcorticalthickness.html">Bayesian Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/tutorial_HBR.html">Hierarchical Bayesian Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/tutorial_braincharts_fit_nm.html">Estimating lifespan normative models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/tutorial_braincharts_apply_nm.html">Using lifespan models to make predictions on new data</a></li>
</ul>
<p class="caption"><span class="caption-text">Other Useful Stuff</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pages/FAQs.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/citing.html">How to cite PCNtoolkit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/references.html">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/acknowledgements.html">Acknowledgements</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Predictive Clinical Neuroscience Toolkit</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">Module code</a> &raquo;</li>
        
      <li>normative</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for normative</h1><div class="highlight"><pre>
<span></span>#!/Users/andre/sfw/anaconda3/bin/python

# ------------------------------------------------------------------------------
#  Usage:
#  python normative.py -m [maskfile] -k [number of CV folds] -c &lt;covariates&gt;
#                      -t [test covariates] -r [test responses] &lt;infile&gt;
#
#  Either the -k switch or -t switch should be specified, but not both.
#  If -t is selected, a set of responses should be provided with the -r switch
#
#  Written by A. Marquand
# ------------------------------------------------------------------------------

from __future__ import print_function
from __future__ import division

import os
import sys
import numpy as np
import argparse
import pickle
import glob

from sklearn.model_selection import KFold
try:  # run as a package if installed
    from pcntoolkit import configs
    from pcntoolkit.dataio import fileio
    from pcntoolkit.normative_model.norm_utils import norm_init
    from pcntoolkit.util.utils import compute_pearsonr, CustomCV, explained_var
    from pcntoolkit.util.utils import compute_MSLL, scaler
except ImportError:
    pass

    path = os.path.abspath(os.path.dirname(__file__))
    if path not in sys.path:
        sys.path.append(path)
        #sys.path.append(os.path.join(path,&#39;normative_model&#39;))
    del path
    
    import configs
    from dataio import fileio

    from util.utils import compute_pearsonr, CustomCV, explained_var, compute_MSLL
    from util.utils import scaler
    from normative_model.norm_utils import norm_init

PICKLE_PROTOCOL = configs.PICKLE_PROTOCOL

<div class="viewcode-block" id="load_response_vars"><a class="viewcode-back" href="../pages/modindex.html#normative.load_response_vars">[docs]</a>def load_response_vars(datafile, maskfile=None, vol=True):
    &quot;&quot;&quot; load response variables (of any data type)&quot;&quot;&quot;

    if fileio.file_type(datafile) == &#39;nifti&#39;:
        dat = fileio.load_nifti(datafile, vol=vol)
        volmask = fileio.create_mask(dat, mask=maskfile)
        Y = fileio.vol2vec(dat, volmask).T
    else:
        Y = fileio.load(datafile)
        volmask = None
        if fileio.file_type(datafile) == &#39;cifti&#39;:
            Y = Y.T

    return Y, volmask</div>


<div class="viewcode-block" id="get_args"><a class="viewcode-back" href="../pages/modindex.html#normative.get_args">[docs]</a>def get_args(*args):
    &quot;&quot;&quot; Parse command line arguments&quot;&quot;&quot;

    # parse arguments
    parser = argparse.ArgumentParser(description=&quot;Normative Modeling&quot;)
    parser.add_argument(&quot;responses&quot;)
    parser.add_argument(&quot;-f&quot;, help=&quot;Function to call&quot;, dest=&quot;func&quot;, 
                        default=&quot;estimate&quot;)
    parser.add_argument(&quot;-m&quot;, help=&quot;mask file&quot;, dest=&quot;maskfile&quot;, default=None)
    parser.add_argument(&quot;-c&quot;, help=&quot;covariates file&quot;, dest=&quot;covfile&quot;,
                        default=None)
    parser.add_argument(&quot;-k&quot;, help=&quot;cross-validation folds&quot;, dest=&quot;cvfolds&quot;,
                        default=None)
    parser.add_argument(&quot;-t&quot;, help=&quot;covariates (test data)&quot;, dest=&quot;testcov&quot;,
                        default=None)
    parser.add_argument(&quot;-r&quot;, help=&quot;responses (test data)&quot;, dest=&quot;testresp&quot;,
                        default=None)
    parser.add_argument(&quot;-a&quot;, help=&quot;algorithm&quot;, dest=&quot;alg&quot;, default=&quot;gpr&quot;)
    parser.add_argument(&quot;-x&quot;, help=&quot;algorithm specific config options&quot;, 
                        dest=&quot;configparam&quot;, default=None)
    # parser.add_argument(&#39;-s&#39;, action=&#39;store_false&#39;, 
    #                 help=&quot;Flag to skip standardization.&quot;, dest=&quot;standardize&quot;)
    parser.add_argument(&quot;keyword_args&quot;, nargs=argparse.REMAINDER)
    
    args = parser.parse_args()
    
    # Process required  arguemnts 
    wdir = os.path.realpath(os.path.curdir)
    respfile = os.path.join(wdir, args.responses)
    if args.covfile is None:
        raise(ValueError, &quot;No covariates specified&quot;)
    else:
        covfile = args.covfile
    
    # Process optional arguments
    if args.maskfile is None:
        maskfile = None
    else:
        maskfile = os.path.join(wdir, args.maskfile)
    if args.testcov is None and args.cvfolds is not None:
        testcov = None
        testresp = None
        cvfolds = int(args.cvfolds)
        print(&quot;Running under &quot; + str(cvfolds) + &quot; fold cross-validation.&quot;)
    else:
        print(&quot;Test covariates specified&quot;)
        testcov = args.testcov
        cvfolds = None
        if args.testresp is None:
            testresp = None
            print(&quot;No test response variables specified&quot;)
        else:
            testresp = args.testresp
        if args.cvfolds is not None:
            print(&quot;Ignoring cross-valdation specification (test data given)&quot;)

    # Process addtional keyword arguments. These are always added as strings
    kw_args = {}
    for kw in args.keyword_args:
        kw_arg = kw.split(&#39;=&#39;)
    
        exec(&quot;kw_args.update({&#39;&quot; +  kw_arg[0] + &quot;&#39; : &quot; + 
                              &quot;&#39;&quot; + str(kw_arg[1]) + &quot;&#39;&quot; + &quot;})&quot;)
    
    return respfile, maskfile, covfile, cvfolds, \
            testcov, testresp, args.func, args.alg, \
            args.configparam, kw_args</div>
            

<div class="viewcode-block" id="evaluate"><a class="viewcode-back" href="../pages/modindex.html#normative.evaluate">[docs]</a>def evaluate(Y, Yhat, S2=None, mY=None, sY=None, nlZ=None, nm=None, Xz_tr=None, alg=None,
             metrics = [&#39;Rho&#39;, &#39;RMSE&#39;, &#39;SMSE&#39;, &#39;EXPV&#39;, &#39;MSLL&#39;]):
    &#39;&#39;&#39; Compute error metrics
    This function will compute error metrics based on a set of predictions Yhat
    and a set of true response variables Y, namely:
    
    * Rho: Pearson correlation
    * RMSE: root mean squared error
    * SMSE: standardized mean squared error
    * EXPV: explained variance
        
    If the predictive variance is also specified the log loss will be computed
    (which also takes into account the predictive variance). If the mean and 
    standard deviation are also specified these will be used to standardize 
    this, yielding the mean standardized log loss
    
    :param Y: N x P array of true response variables
    :param Yhat: N x P array of predicted response variables
    :param S2: predictive variance
    :param mY: mean of the training set
    :param sY: standard deviation of the training set

    :returns metrics: evaluation metrics
    
    &#39;&#39;&#39;
    
    feature_num = Y.shape[1]
    
    # Remove metrics that cannot be computed with only a single data point 
    if Y.shape[0] == 1:
        if &#39;MSLL&#39; in metrics:
            metrics.remove(&#39;MSLL&#39;)
        if &#39;SMSE&#39; in metrics:
            metrics.remove(&#39;SMSE&#39;)
    
    # find and remove bad variables from the response variables
    nz = np.where(np.bitwise_and(np.isfinite(Y).any(axis=0),
                                 np.var(Y, axis=0) != 0))[0]
    
    MSE = np.mean((Y - Yhat)**2, axis=0)
    
    results = dict()
    
    if &#39;RMSE&#39; in metrics:
        RMSE = np.sqrt(MSE)
        results[&#39;RMSE&#39;] = RMSE
    
    if &#39;Rho&#39; in metrics:
        Rho = np.zeros(feature_num)
        pRho = np.ones(feature_num)    
        Rho[nz], pRho[nz] = compute_pearsonr(Y[:,nz], Yhat[:,nz])
        results[&#39;Rho&#39;] = Rho
        results[&#39;pRho&#39;] = pRho
        
    if &#39;SMSE&#39; in metrics:
        SMSE = np.zeros_like(MSE)
        SMSE[nz] = MSE[nz] / np.var(Y[:,nz], axis=0)
        results[&#39;SMSE&#39;] = SMSE
    
    if &#39;EXPV&#39; in metrics:
        EXPV = np.zeros(feature_num)
        EXPV[nz] = explained_var(Y[:,nz], Yhat[:,nz])
        results[&#39;EXPV&#39;] = EXPV
        
    if &#39;MSLL&#39; in metrics:
        if ((S2 is not None) and (mY is not None) and (sY is not None)):
            MSLL = np.zeros(feature_num)
            MSLL[nz] = compute_MSLL(Y[:,nz], Yhat[:,nz], S2[:,nz], 
                                    mY.reshape(-1,1).T, 
                                    (sY**2).reshape(-1,1).T)
            results[&#39;MSLL&#39;] = MSLL
            
    if &#39;NLL&#39; in metrics:
        results[&#39;NLL&#39;] = nlZ
    
    if &#39;BIC&#39; in metrics:
        if hasattr(getattr(nm, alg), &#39;hyp&#39;):
            n = Xz_tr.shape[0]
            k = len(getattr(nm, alg).hyp)
            BIC = k * np.log(n) + 2 * nlZ
            results[&#39;BIC&#39;] = BIC    
    
    return results</div>

<div class="viewcode-block" id="save_results"><a class="viewcode-back" href="../pages/modindex.html#normative.save_results">[docs]</a>def save_results(respfile, Yhat, S2, maskvol, Z=None, outputsuffix=None, 
                 results=None, save_path=&#39;&#39;):
    
    print(&quot;Writing outputs ...&quot;)
    if respfile is None:
        exfile = None
        file_ext = &#39;.pkl&#39;
    else:
        if fileio.file_type(respfile) == &#39;cifti&#39; or \
           fileio.file_type(respfile) == &#39;nifti&#39;:
            exfile = respfile
        else:
            exfile = None
        file_ext = fileio.file_extension(respfile)

    if outputsuffix is not None:
        ext = str(outputsuffix) + file_ext
    else:
        ext = file_ext

    fileio.save(Yhat, os.path.join(save_path, &#39;yhat&#39; + ext), example=exfile, 
                                   mask=maskvol)
    fileio.save(S2, os.path.join(save_path, &#39;ys2&#39; + ext), example=exfile, 
                mask=maskvol)
    if Z is not None:
        fileio.save(Z, os.path.join(save_path, &#39;Z&#39; + ext), example=exfile, 
                    mask=maskvol)

    if results is not None:        
        for metric in list(results.keys()):
            if (metric == &#39;NLL&#39; or metric == &#39;BIC&#39;) and file_ext == &#39;.nii.gz&#39;:
                fileio.save(results[metric], os.path.join(save_path, metric + str(outputsuffix) + &#39;.pkl&#39;), 
                        example=exfile, mask=maskvol)
            else:
                fileio.save(results[metric], os.path.join(save_path, metric + ext), 
                            example=exfile, mask=maskvol)</div>

<div class="viewcode-block" id="estimate"><a class="viewcode-back" href="../pages/modindex.html#normative.estimate">[docs]</a>def estimate(covfile, respfile, **kwargs):
    &quot;&quot;&quot; Estimate a normative model

    This will estimate a model in one of two settings according to 
    theparticular parameters specified (see below)
        
    * under k-fold cross-validation.
      requires respfile, covfile and cvfolds&gt;=2
    * estimating a training dataset then applying to a second test dataset.
      requires respfile, covfile, testcov and testresp.
    * estimating on a training dataset ouput of forward maps mean and se. 
      requires respfile, covfile and testcov

    The models are estimated on the basis of data stored on disk in ascii or
    neuroimaging data formats (nifti or cifti). Ascii data should be in
    tab or space delimited format with the number of subjects in rows and the
    number of variables in columns. Neuroimaging data will be reshaped
    into the appropriate format

    Basic usage::

        estimate(covfile, respfile, [extra_arguments])

    where the variables are defined below. Note that either the cfolds
    parameter or (testcov, testresp) should be specified, but not both.

    :param respfile: response variables for the normative model
    :param covfile: covariates used to predict the response variable
    :param maskfile: mask used to apply to the data (nifti only)
    :param cvfolds: Number of cross-validation folds
    :param testcov: Test covariates
    :param testresp: Test responses
    :param alg: Algorithm for normative model
    :param configparam: Parameters controlling the estimation algorithm
    :param saveoutput: Save the output to disk? Otherwise returned as arrays
    :param outputsuffix: Text string to add to the output filenames
    :param inscale: Scaling approach for input covariates, could be &#39;None&#39; (Default), 
                    &#39;standardize&#39;, &#39;minmax&#39;, or &#39;robminmax&#39;.
    :param outscale: Scaling approach for output responses, could be &#39;None&#39; (Default), 
                    &#39;standardize&#39;, &#39;minmax&#39;, or &#39;robminmax&#39;.

    All outputs are written to disk in the same format as the input. These are:

    :outputs: * yhat - predictive mean
              * ys2 - predictive variance
              * nm - normative model
              * Z - deviance scores
              * Rho - Pearson correlation between true and predicted responses
              * pRho - parametric p-value for this correlation
              * rmse - root mean squared error between true/predicted responses
              * smse - standardised mean squared error

    The outputsuffix may be useful to estimate multiple normative models in the
    same directory (e.g. for custom cross-validation schemes)
    &quot;&quot;&quot;
    
    # parse keyword arguments 
    maskfile = kwargs.pop(&#39;maskfile&#39;,None)
    cvfolds = kwargs.pop(&#39;cvfolds&#39;, None)
    testcov = kwargs.pop(&#39;testcov&#39;, None)
    testresp = kwargs.pop(&#39;testresp&#39;,None)
    alg = kwargs.pop(&#39;alg&#39;,&#39;gpr&#39;)
    outputsuffix = kwargs.pop(&#39;outputsuffix&#39;,&#39;estimate&#39;)
    outputsuffix = &quot;_&quot; + outputsuffix.replace(&quot;_&quot;, &quot;&quot;)  # Making sure there is only one 
                                                        # &#39;_&#39; is in the outputsuffix to 
                                                        # avoid file name parsing problem.
    inscaler = kwargs.pop(&#39;inscaler&#39;,&#39;None&#39;)
    outscaler = kwargs.pop(&#39;outscaler&#39;,&#39;None&#39;)
    warp = kwargs.get(&#39;warp&#39;, None)

    # convert from strings if necessary
    saveoutput = kwargs.pop(&#39;saveoutput&#39;,&#39;True&#39;)
    if type(saveoutput) is str:
        saveoutput = saveoutput==&#39;True&#39;
    savemodel = kwargs.pop(&#39;savemodel&#39;,&#39;False&#39;)
    if type(savemodel) is str:
        savemodel = savemodel==&#39;True&#39;
    
    if savemodel and not os.path.isdir(&#39;Models&#39;):
        os.mkdir(&#39;Models&#39;)

    # load data
    print(&quot;Processing data in &quot; + respfile)
    X = fileio.load(covfile)
    Y, maskvol = load_response_vars(respfile, maskfile)
    if len(Y.shape) == 1:
        Y = Y[:, np.newaxis]
    if len(X.shape) == 1:
        X = X[:, np.newaxis]
    Nmod = Y.shape[1]
    
    if (testcov is not None) and (cvfolds is None): # a separate test dataset
        
        run_cv = False
        cvfolds = 1
        Xte = fileio.load(testcov)
        if len(Xte.shape) == 1:
            Xte = Xte[:, np.newaxis]
        if testresp is not None:
            Yte, testmask = load_response_vars(testresp, maskfile)
            if len(Yte.shape) == 1:
                Yte = Yte[:, np.newaxis]
        else:
            sub_te = Xte.shape[0]
            Yte = np.zeros([sub_te, Nmod])
            
        # treat as a single train-test split
        testids = range(X.shape[0], X.shape[0]+Xte.shape[0])
        splits = CustomCV((range(0, X.shape[0]),), (testids,))

        Y = np.concatenate((Y, Yte), axis=0)
        X = np.concatenate((X, Xte), axis=0)
        
    else:
        run_cv = True
        # we are running under cross-validation
        splits = KFold(n_splits=cvfolds, shuffle=True)
        testids = range(0, X.shape[0])
        if alg==&#39;hbr&#39;:
           trbefile = kwargs.get(&#39;trbefile&#39;, None) 
           if trbefile is not None:
                be = fileio.load(trbefile)
                if len(be.shape) == 1:
                    be = be[:, np.newaxis]
           else:
                print(&#39;No batch-effects file! Initilizing all as zeros!&#39;)
                be = np.zeros([X.shape[0],1])

    # find and remove bad variables from the response variables
    # note: the covariates are assumed to have already been checked
    nz = np.where(np.bitwise_and(np.isfinite(Y).any(axis=0),
                                 np.var(Y, axis=0) != 0))[0]

    # run cross-validation loop
    Yhat = np.zeros_like(Y)
    S2 = np.zeros_like(Y)
    Z = np.zeros_like(Y)
    nlZ = np.zeros((Nmod, cvfolds))
    
    scaler_resp = []
    scaler_cov = []
    mean_resp = [] # this is just for computing MSLL
    std_resp = []   # this is just for computing MSLL
    
    if warp is not None:
        Ywarp = np.zeros_like(Yhat)
        mean_resp_warp = [np.zeros(Y.shape[1]) for s in range(splits.n_splits)]
        std_resp_warp = [np.zeros(Y.shape[1]) for s in range(splits.n_splits)]

    for idx in enumerate(splits.split(X)):

        fold = idx[0]
        tr = idx[1][0]
        ts = idx[1][1]

        # standardize responses and covariates, ignoring invalid entries
        iy_tr, jy_tr = np.ix_(tr, nz)
        iy_ts, jy_ts = np.ix_(ts, nz)
        mY = np.mean(Y[iy_tr, jy_tr], axis=0)
        sY = np.std(Y[iy_tr, jy_tr], axis=0)
        mean_resp.append(mY)
        std_resp.append(sY)
        
        if inscaler in [&#39;standardize&#39;, &#39;minmax&#39;, &#39;robminmax&#39;]:
            X_scaler = scaler(inscaler)
            Xz_tr = X_scaler.fit_transform(X[tr, :])
            Xz_ts = X_scaler.transform(X[ts, :])
            scaler_cov.append(X_scaler)
        else:
            Xz_tr = X[tr, :]
            Xz_ts = X[ts, :]
            
        if outscaler in [&#39;standardize&#39;, &#39;minmax&#39;, &#39;robminmax&#39;]:
            Y_scaler = scaler(outscaler)
            Yz_tr = Y_scaler.fit_transform(Y[iy_tr, jy_tr])
            scaler_resp.append(Y_scaler)
        else:
            Yz_tr = Y[iy_tr, jy_tr]
        
        if (run_cv==True and alg==&#39;hbr&#39;):
            fileio.save(be[tr,:], &#39;be_kfold_tr_tempfile.pkl&#39;)
            fileio.save(be[ts,:], &#39;be_kfold_ts_tempfile.pkl&#39;)
            kwargs[&#39;trbefile&#39;] = &#39;be_kfold_tr_tempfile.pkl&#39;
            kwargs[&#39;tsbefile&#39;] = &#39;be_kfold_ts_tempfile.pkl&#39;
        
        # estimate the models for all subjects
        for i in range(0, len(nz)):  
            print(&quot;Estimating model &quot;, i+1, &quot;of&quot;, len(nz))
            nm = norm_init(Xz_tr, Yz_tr[:, i], alg=alg, **kwargs)
                
            try:
                nm = nm.estimate(Xz_tr, Yz_tr[:, i], **kwargs)     
                yhat, s2 = nm.predict(Xz_ts, Xz_tr, Yz_tr[:, i], **kwargs)
                
                if savemodel:
                    nm.save(&#39;Models/NM_&#39; + str(fold) + &#39;_&#39; + str(nz[i]) + 
                            outputsuffix + &#39;.pkl&#39; )
                
                if outscaler == &#39;standardize&#39;: 
                    Yhat[ts, nz[i]] = Y_scaler.inverse_transform(yhat, index=i)
                    S2[ts, nz[i]] = s2 * sY[i]**2
                elif outscaler in [&#39;minmax&#39;, &#39;robminmax&#39;]:
                    Yhat[ts, nz[i]] = Y_scaler.inverse_transform(yhat, index=i)
                    S2[ts, nz[i]] = s2 * (Y_scaler.max[i] - Y_scaler.min[i])**2
                else:
                    Yhat[ts, nz[i]] = yhat
                    S2[ts, nz[i]] = s2
                    
                nlZ[nz[i], fold] = nm.neg_log_lik
                
                if (run_cv or testresp is not None):
                    # warp the labels?
                    # TODO: Warping for scaled data
                    if warp is not None:
                        warp_param = nm.blr.hyp[1:nm.blr.warp.get_n_params()+1] 
                        Ywarp[ts, nz[i]] = nm.blr.warp.f(Y[ts, nz[i]], warp_param)
                        Ytest = Ywarp[ts, nz[i]]
                        
                        # Save warped mean of the training data (for MSLL)
                        yw = nm.blr.warp.f(Y[tr, nz[i]], warp_param)
                        mean_resp_warp[fold][i] = np.mean(yw)
                        std_resp_warp[fold][i] = np.std(yw)
                    else:
                        Ytest = Y[ts, nz[i]] 
                    
                    Z[ts, nz[i]] = (Ytest - Yhat[ts, nz[i]]) / \
                                    np.sqrt(S2[ts, nz[i]])       
                    
            except Exception as e:
                exc_type, exc_obj, exc_tb = sys.exc_info()
                fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
                print(&quot;Model &quot;, i+1, &quot;of&quot;, len(nz),
                      &quot;FAILED!..skipping and writing NaN to outputs&quot;)
                print(&quot;Exception:&quot;)
                print(e)
                print(exc_type, fname, exc_tb.tb_lineno)

                Yhat[ts, nz[i]] = float(&#39;nan&#39;)
                S2[ts, nz[i]] = float(&#39;nan&#39;)
                nlZ[nz[i], fold] = float(&#39;nan&#39;)
                if testcov is None:
                    Z[ts, nz[i]] = float(&#39;nan&#39;)
                else:
                    if testresp is not None:
                        Z[ts, nz[i]] = float(&#39;nan&#39;)


    if savemodel:
        print(&#39;Saving model meta-data...&#39;)
        with open(&#39;Models/meta_data.md&#39;, &#39;wb&#39;) as file:
            pickle.dump({&#39;valid_voxels&#39;:nz, &#39;fold_num&#39;:cvfolds, 
                         &#39;mean_resp&#39;:mean_resp, &#39;std_resp&#39;:std_resp, 
                         &#39;scaler_cov&#39;:scaler_cov, &#39;scaler_resp&#39;:scaler_resp, 
                         &#39;regressor&#39;:alg, &#39;inscaler&#39;:inscaler, 
                         &#39;outscaler&#39;:outscaler}, file, protocol=PICKLE_PROTOCOL)    

    # compute performance metrics
    if (run_cv or testresp is not None):
        print(&quot;Evaluating the model ...&quot;)
        if warp is None:
            results = evaluate(Y[testids, :], Yhat[testids, :], 
                               S2=S2[testids, :], mY=mean_resp[0], 
                               sY=std_resp[0], nlZ=nlZ, nm=nm, Xz_tr=Xz_tr, alg=alg,
                               metrics = [&#39;Rho&#39;, &#39;RMSE&#39;, &#39;SMSE&#39;, &#39;EXPV&#39;,
                                          &#39;MSLL&#39;, &#39;NLL&#39;, &#39;BIC&#39;])
        else:
            results = evaluate(Ywarp[testids, :], Yhat[testids, :], 
                               S2=S2[testids, :], mY=mean_resp_warp[0], 
                               sY=std_resp_warp[0], nlZ=nlZ, nm=nm, Xz_tr=Xz_tr,
                               alg=alg, metrics = [&#39;Rho&#39;, &#39;RMSE&#39;, &#39;SMSE&#39;,
                                                   &#39;EXPV&#39;, &#39;MSLL&#39;,
                                                   &#39;NLL&#39;, &#39;BIC&#39;])
            
        
    # Set writing options
    if saveoutput:
        if (run_cv or testresp is not None):
            save_results(respfile, Yhat[testids, :], S2[testids, :], maskvol, 
                         Z=Z[testids, :], results=results, 
                         outputsuffix=outputsuffix)
            
        else:
            save_results(respfile, Yhat[testids, :], S2[testids, :], maskvol,
                         outputsuffix=outputsuffix)
                
    else:
        if (run_cv or testresp is not None):
            output = (Yhat[testids, :], S2[testids, :], nm, Z[testids, :], 
                      results)
        else:
            output = (Yhat[testids, :], S2[testids, :], nm)
        
        return output</div>


<div class="viewcode-block" id="fit"><a class="viewcode-back" href="../pages/modindex.html#normative.fit">[docs]</a>def fit(covfile, respfile, **kwargs):
    
    # parse keyword arguments 
    maskfile = kwargs.pop(&#39;maskfile&#39;,None)
    alg = kwargs.pop(&#39;alg&#39;,&#39;gpr&#39;)
    savemodel = kwargs.pop(&#39;savemodel&#39;,&#39;True&#39;)==&#39;True&#39;
    outputsuffix = kwargs.pop(&#39;outputsuffix&#39;,&#39;fit&#39;)
    outputsuffix = &quot;_&quot; + outputsuffix.replace(&quot;_&quot;, &quot;&quot;)
    inscaler = kwargs.pop(&#39;inscaler&#39;,&#39;None&#39;)
    outscaler = kwargs.pop(&#39;outscaler&#39;,&#39;None&#39;)
    
    if savemodel and not os.path.isdir(&#39;Models&#39;):
        os.mkdir(&#39;Models&#39;)

    # load data
    print(&quot;Processing data in &quot; + respfile)
    X = fileio.load(covfile)
    Y, maskvol = load_response_vars(respfile, maskfile)
    if len(Y.shape) == 1:
        Y = Y[:, np.newaxis]
    if len(X.shape) == 1:
        X = X[:, np.newaxis]
    
    # find and remove bad variables from the response variables
    # note: the covariates are assumed to have already been checked
    nz = np.where(np.bitwise_and(np.isfinite(Y).any(axis=0),
                                 np.var(Y, axis=0) != 0))[0]        
    
    scaler_resp = []
    scaler_cov = []
    mean_resp = [] # this is just for computing MSLL
    std_resp = []   # this is just for computing MSLL
    
    # standardize responses and covariates, ignoring invalid entries
    mY = np.mean(Y[:, nz], axis=0)
    sY = np.std(Y[:, nz], axis=0)
    mean_resp.append(mY)
    std_resp.append(sY)
    
    if inscaler in [&#39;standardize&#39;, &#39;minmax&#39;, &#39;robminmax&#39;]:
        X_scaler = scaler(inscaler)
        Xz = X_scaler.fit_transform(X)
        scaler_cov.append(X_scaler)
    else:
        Xz = X
        
    if outscaler in [&#39;standardize&#39;, &#39;minmax&#39;, &#39;robminmax&#39;]:
        Yz = np.zeros_like(Y)
        Y_scaler = scaler(outscaler)
        Yz[:, nz] = Y_scaler.fit_transform(Y[:, nz])
        scaler_resp.append(Y_scaler)
    else:
        Yz = Y

    # estimate the models for all subjects
    for i in range(0, len(nz)):  
        print(&quot;Estimating model &quot;, i+1, &quot;of&quot;, len(nz))
        nm = norm_init(Xz, Yz[:, nz[i]], alg=alg, **kwargs)
        nm = nm.estimate(Xz, Yz[:, nz[i]], **kwargs)     
            
        if savemodel:
            nm.save(&#39;Models/NM_&#39; + str(0) + &#39;_&#39; + str(nz[i]) + outputsuffix + 
                    &#39;.pkl&#39; )

    if savemodel:
        print(&#39;Saving model meta-data...&#39;)
        with open(&#39;Models/meta_data.md&#39;, &#39;wb&#39;) as file:
            pickle.dump({&#39;valid_voxels&#39;:nz,
                         &#39;mean_resp&#39;:mean_resp, &#39;std_resp&#39;:std_resp, 
                         &#39;scaler_cov&#39;:scaler_cov, &#39;scaler_resp&#39;:scaler_resp, 
                         &#39;regressor&#39;:alg, &#39;inscaler&#39;:inscaler,
                         &#39;outscaler&#39;:outscaler}, file, protocol=PICKLE_PROTOCOL)
        
    return nm</div>

    
<div class="viewcode-block" id="predict"><a class="viewcode-back" href="../pages/modindex.html#normative.predict">[docs]</a>def predict(covfile, respfile, maskfile=None, **kwargs):
    &#39;&#39;&#39;
    Make predictions on the basis of a pre-estimated normative model 
    If only the covariates are specified then only predicted mean and variance 
    will be returned. If the test responses are also specified then quantities
    That depend on those will also be returned (Z scores and error metrics)

    Basic usage::

        predict(covfile, [extra_arguments])

    where the variables are defined below.

    :param covfile: test covariates used to predict the response variable
    :param respfile: test response variables for the normative model
    :param maskfile: mask used to apply to the data (nifti only)
    :param model_path: Directory containing the normative model and metadata.
     When using parallel prediction, do not pass the model path. It will be 
     automatically decided.
    :param outputsuffix: Text string to add to the output filenames
    :param batch_size: batch size (for use with normative_parallel)
    :param job_id: batch id

    All outputs are written to disk in the same format as the input. These are:

    :outputs: * Yhat - predictive mean
              * S2 - predictive variance
              * Z - Z scores
    &#39;&#39;&#39;
    
    
    model_path = kwargs.pop(&#39;model_path&#39;, &#39;Models&#39;)
    job_id = kwargs.pop(&#39;job_id&#39;, None)
    batch_size = kwargs.pop(&#39;batch_size&#39;, None)
    outputsuffix = kwargs.pop(&#39;outputsuffix&#39;, &#39;predict&#39;)
    outputsuffix = &quot;_&quot; + outputsuffix.replace(&quot;_&quot;, &quot;&quot;)
    inputsuffix = kwargs.pop(&#39;inputsuffix&#39;, &#39;estimate&#39;)
    inputsuffix = &quot;_&quot; + inputsuffix.replace(&quot;_&quot;, &quot;&quot;)
    alg = kwargs.pop(&#39;alg&#39;)
        
    if respfile is not None and not os.path.exists(respfile):
        print(&quot;Response file does not exist. Only returning predictions&quot;)
        respfile = None
    if not os.path.isdir(model_path):
        print(&#39;Models directory does not exist!&#39;)
        return
    else:
        if os.path.exists(os.path.join(model_path, &#39;meta_data.md&#39;)):
            with open(os.path.join(model_path, &#39;meta_data.md&#39;), &#39;rb&#39;) as file:
                meta_data = pickle.load(file)
            inscaler = meta_data[&#39;inscaler&#39;]
            outscaler = meta_data[&#39;outscaler&#39;]
            mY = meta_data[&#39;mean_resp&#39;]
            sY = meta_data[&#39;std_resp&#39;]
            scaler_cov = meta_data[&#39;scaler_cov&#39;]
            scaler_resp = meta_data[&#39;scaler_resp&#39;]
            meta_data = True
        else:
            print(&quot;No meta-data file is found!&quot;)
            inscaler = &#39;None&#39;
            outscaler = &#39;None&#39;
            meta_data = False

    if batch_size is not None:
        batch_size = int(batch_size)
        job_id = int(job_id) - 1

    
    # load data
    print(&quot;Loading data ...&quot;)
    X = fileio.load(covfile)
    if len(X.shape) == 1:
        X = X[:, np.newaxis]
    
    sample_num = X.shape[0]
    feature_num = len(glob.glob(os.path.join(model_path, &#39;NM_*&#39; + inputsuffix + 
                                             &#39;.pkl&#39;)))

    Yhat = np.zeros([sample_num, feature_num])
    S2 = np.zeros([sample_num, feature_num])
    Z = np.zeros([sample_num, feature_num])
    
    if inscaler in [&#39;standardize&#39;, &#39;minmax&#39;, &#39;robminmax&#39;]:
        Xz = scaler_cov[0].transform(X)
    else:
        Xz = X
    
    # estimate the models for all subjects
    for i in range(feature_num):
        print(&quot;Prediction by model &quot;, i+1, &quot;of&quot;, feature_num)      
        nm = norm_init(Xz)
        nm = nm.load(os.path.join(model_path, &#39;NM_&#39; + str(0) + &#39;_&#39; + 
                                  str(i) + inputsuffix + &#39;.pkl&#39;))
        if (alg!=&#39;hbr&#39; or nm.configs[&#39;transferred&#39;]==False):
            yhat, s2 = nm.predict(Xz, **kwargs)
        else:
            tsbefile = kwargs.get(&#39;tsbefile&#39;) 
            batch_effects_test = fileio.load(tsbefile)
            yhat, s2 = nm.predict_on_new_sites(Xz, batch_effects_test)
        
        if outscaler == &#39;standardize&#39;: 
            Yhat[:, i] = scaler_resp[0].inverse_transform(yhat, index=i)
            S2[:, i] = s2.squeeze() * sY[0][i]**2
        elif outscaler in [&#39;minmax&#39;, &#39;robminmax&#39;]:
            Yhat[:, i] = scaler_resp[0].inverse_transform(yhat, index=i)
            S2[:, i] = s2 * (scaler_resp[0].max[i] - scaler_resp[0].min[i])**2
        else:
            Yhat[:, i] = yhat.squeeze()
            S2[:, i] = s2.squeeze()

    if respfile is None:
        save_results(None, Yhat, S2, None, outputsuffix=outputsuffix)
        
        return (Yhat, S2)
    
    else:
        Y, maskvol = load_response_vars(respfile, maskfile)
        if len(Y.shape) == 1:
            Y = Y[:, np.newaxis]
        
        # warp the targets?
        if &#39;blr&#39; in dir(nm):
            if nm.blr.warp is not None:
                warp_param = nm.blr.hyp[1:nm.blr.warp.get_n_params()+1] 
                Y = nm.blr.warp.f(Y, warp_param)
        
        Z = (Y - Yhat) / np.sqrt(S2)
        
        print(&quot;Evaluating the model ...&quot;)
        if meta_data:
            results = evaluate(Y, Yhat, S2=S2, mY=mY[0], sY=sY[0])
        else:    
            results = evaluate(Y, Yhat, S2=S2, 
                           metrics = [&#39;Rho&#39;, &#39;RMSE&#39;, &#39;SMSE&#39;, &#39;EXPV&#39;])
        
        print(&quot;Evaluations Writing outputs ...&quot;)
        save_results(respfile, Yhat, S2, maskvol, Z=Z, 
                     outputsuffix=outputsuffix, results=results)
        
        return (Yhat, S2, Z)</div>

    
<div class="viewcode-block" id="transfer"><a class="viewcode-back" href="../pages/modindex.html#normative.transfer">[docs]</a>def transfer(covfile, respfile, testcov=None, testresp=None, maskfile=None, 
             **kwargs):
    &#39;&#39;&#39;
    Transfer learning on the basis of a pre-estimated normative model by using 
    the posterior distribution over the parameters as an informed prior for 
    new data. currently only supported for HBR.
    
    Basic usage::

        transfer(covfile, respfile [extra_arguments])

    where the variables are defined below.

    :param covfile: test covariates used to predict the response variable
    :param respfile: test response variables for the normative model
    :param maskfile: mask used to apply to the data (nifti only)
    :param testcov: Test covariates
    :param testresp: Test responses
    :param model_path: Directory containing the normative model and metadata
    :param trbefile: Training batch effects file
    :param batch_size: batch size (for use with normative_parallel)
    :param job_id: batch id

    All outputs are written to disk in the same format as the input. These are:

    :outputs: * Yhat - predictive mean
              * S2 - predictive variance
              * Z - Z scores
    &#39;&#39;&#39;
    
    alg = kwargs.pop(&#39;alg&#39;)
    if alg != &#39;hbr&#39;:
        print(&#39;Model transferring is only possible for HBR models.&#39;)
        return
    elif (not &#39;model_path&#39; in list(kwargs.keys())) or \
        (not &#39;output_path&#39; in list(kwargs.keys())) or \
        (not &#39;trbefile&#39; in list(kwargs.keys())):
            print(&#39;InputError: Some mandatory arguments are missing.&#39;)
            return
    else:
        model_path = kwargs.pop(&#39;model_path&#39;)
        output_path = kwargs.pop(&#39;output_path&#39;)
        trbefile = kwargs.pop(&#39;trbefile&#39;)
        batch_effects_train = fileio.load(trbefile)
    
    outputsuffix = kwargs.pop(&#39;outputsuffix&#39;, &#39;transfer&#39;)
    outputsuffix = &quot;_&quot; + outputsuffix.replace(&quot;_&quot;, &quot;&quot;)
    inputsuffix = kwargs.pop(&#39;inputsuffix&#39;, &#39;estimate&#39;)
    inputsuffix = &quot;_&quot; + inputsuffix.replace(&quot;_&quot;, &quot;&quot;)
    tsbefile = kwargs.pop(&#39;tsbefile&#39;, None)
    
    job_id = kwargs.pop(&#39;job_id&#39;, None)
    batch_size = kwargs.pop(&#39;batch_size&#39;, None)
    if batch_size is not None:
        batch_size = int(batch_size)
        job_id = int(job_id) - 1
    
    if not os.path.isdir(model_path):
        print(&#39;Models directory does not exist!&#39;)
        return
    else:
        if os.path.exists(os.path.join(model_path, &#39;meta_data.md&#39;)):
            with open(os.path.join(model_path, &#39;meta_data.md&#39;), &#39;rb&#39;) as file:
                meta_data = pickle.load(file)
            inscaler = meta_data[&#39;inscaler&#39;]
            outscaler = meta_data[&#39;outscaler&#39;]
            scaler_cov = meta_data[&#39;scaler_cov&#39;]
            scaler_resp = meta_data[&#39;scaler_resp&#39;]
            meta_data = True
        else:
            print(&quot;No meta-data file is found!&quot;)
            inscaler = &#39;None&#39;
            outscaler = &#39;None&#39;
            meta_data = False
    
    if not os.path.isdir(output_path):
        os.mkdir(output_path)    
       
    # load data
    print(&quot;Loading data ...&quot;)
    X = fileio.load(covfile)
    Y, maskvol = load_response_vars(respfile, maskfile)
    if len(Y.shape) == 1:
        Y = Y[:, np.newaxis]
    if len(X.shape) == 1:
        X = X[:, np.newaxis]
        
    if inscaler in [&#39;standardize&#39;, &#39;minmax&#39;, &#39;robminmax&#39;]:
        X = scaler_cov[0].transform(X)
    
    feature_num = Y.shape[1]
    mY = np.mean(Y, axis=0)
    sY = np.std(Y, axis=0)  
    
    if outscaler in [&#39;standardize&#39;, &#39;minmax&#39;, &#39;robminmax&#39;]:
        Y = scaler_resp[0].transform(Y)
    
    if testcov is not None:
        # we have a separate test dataset
        Xte = fileio.load(testcov)
        if len(Xte.shape) == 1:
            Xte = Xte[:, np.newaxis]
        ts_sample_num = Xte.shape[0]
        if inscaler in [&#39;standardize&#39;, &#39;minmax&#39;, &#39;robminmax&#39;]:
            Xte = scaler_cov[0].transform(Xte)
        
        if testresp is not None:
            Yte, testmask = load_response_vars(testresp, maskfile)
            if len(Yte.shape) == 1:
                Yte = Yte[:, np.newaxis]
        else:
            Yte = np.zeros([ts_sample_num, feature_num])
        
        if tsbefile is not None:
            batch_effects_test = fileio.load(tsbefile)
        else:
            batch_effects_test = np.zeros([Xte.shape[0],2])

    Yhat = np.zeros([ts_sample_num, feature_num])
    S2 = np.zeros([ts_sample_num, feature_num])
    Z = np.zeros([ts_sample_num, feature_num])
    
    # estimate the models for all subjects
    for i in range(feature_num):
              
        nm = norm_init(X)
        if batch_size is not None: # when using normative_parallel
            print(&quot;Transferring model &quot;, job_id*batch_size+i)
            nm = nm.load(os.path.join(model_path, &#39;NM_0_&#39; + 
                                      str(job_id*batch_size+i) + inputsuffix + 
                                      &#39;.pkl&#39;))
        else:
            print(&quot;Transferring model &quot;, i+1, &quot;of&quot;, feature_num)
            nm = nm.load(os.path.join(model_path, &#39;NM_0_&#39; + str(i) + 
                                      inputsuffix + &#39;.pkl&#39;))
        
        nm = nm.estimate_on_new_sites(X, Y[:,i], batch_effects_train)
        if batch_size is not None: 
            nm.save(os.path.join(output_path, &#39;NM_0_&#39; + 
                             str(job_id*batch_size+i) + outputsuffix + &#39;.pkl&#39;))
            nm.save(os.path.join(&#39;Models&#39;, &#39;NM_0_&#39; + 
                             str(i) + outputsuffix + &#39;.pkl&#39;))
        else:
            nm.save(os.path.join(output_path, &#39;NM_0_&#39; + 
                             str(i) + outputsuffix + &#39;.pkl&#39;))
        
        if testcov is not None:
            yhat, s2 = nm.predict_on_new_sites(Xte, batch_effects_test)
            if outscaler == &#39;standardize&#39;: 
                Yhat[:, i] = scaler_resp[0].inverse_transform(yhat, index=i)
                S2[:, i] = s2.squeeze() * sY[0][i]**2
            elif outscaler in [&#39;minmax&#39;, &#39;robminmax&#39;]:
                Yhat[:, i] = scaler_resp[0].inverse_transform(yhat, index=i)
                S2[:, i] = s2 * (scaler_resp[0].max[i] - scaler_resp[0].min[i])**2
            else:
                Yhat[:, i] = yhat.squeeze()
                S2[:, i] = s2.squeeze()
   
    if testresp is None:
        save_results(respfile, Yhat, S2, maskvol, outputsuffix=outputsuffix)
        return (Yhat, S2)
    else:
        Z = (Yte - Yhat) / np.sqrt(S2)
    
        print(&quot;Evaluating the model ...&quot;)
        results = evaluate(Yte, Yhat, S2=S2, mY=mY, sY=sY)
                
        save_results(respfile, Yhat, S2, maskvol, Z=Z, results=results,
                     outputsuffix=outputsuffix)
        
        return (Yhat, S2, Z)</div>


<div class="viewcode-block" id="extend"><a class="viewcode-back" href="../pages/modindex.html#normative.extend">[docs]</a>def extend(covfile, respfile, maskfile=None, **kwargs):
    
    alg = kwargs.pop(&#39;alg&#39;)
    if alg != &#39;hbr&#39;:
        print(&#39;Model extention is only possible for HBR models.&#39;)
        return
    elif (not &#39;model_path&#39; in list(kwargs.keys())) or \
        (not &#39;output_path&#39; in list(kwargs.keys())) or \
        (not &#39;trbefile&#39; in list(kwargs.keys())) or \
        (not &#39;dummycovfile&#39; in list(kwargs.keys()))or \
        (not &#39;dummybefile&#39; in list(kwargs.keys())):
            print(&#39;InputError: Some mandatory arguments are missing.&#39;)
            return
    else:
        model_path = kwargs.pop(&#39;model_path&#39;)
        output_path = kwargs.pop(&#39;output_path&#39;)
        trbefile = kwargs.pop(&#39;trbefile&#39;)
        dummycovfile = kwargs.pop(&#39;dummycovfile&#39;)
        dummybefile = kwargs.pop(&#39;dummybefile&#39;)
    
    outputsuffix = kwargs.pop(&#39;outputsuffix&#39;, &#39;extend&#39;)
    outputsuffix = &quot;_&quot; + outputsuffix.replace(&quot;_&quot;, &quot;&quot;)
    inputsuffix = kwargs.pop(&#39;inputsuffix&#39;, &#39;estimate&#39;)
    inputsuffix = &quot;_&quot; + inputsuffix.replace(&quot;_&quot;, &quot;&quot;)
    informative_prior = kwargs.pop(&#39;informative_prior&#39;, &#39;False&#39;) == &#39;True&#39;
    generation_factor = int(kwargs.pop(&#39;generation_factor&#39;, &#39;10&#39;))
    job_id = kwargs.pop(&#39;job_id&#39;, None)
    batch_size = kwargs.pop(&#39;batch_size&#39;, None)
    if batch_size is not None:
        batch_size = int(batch_size)
        job_id = int(job_id) - 1
     
    if not os.path.isdir(model_path):
        print(&#39;Models directory does not exist!&#39;)
        return
    else:
        if os.path.exists(os.path.join(model_path, &#39;meta_data.md&#39;)):
            with open(os.path.join(model_path, &#39;meta_data.md&#39;), &#39;rb&#39;) as file:
                meta_data = pickle.load(file)
            if (meta_data[&#39;inscaler&#39;] != &#39;None&#39; or 
                meta_data[&#39;outscaler&#39;] != &#39;None&#39;):
                print(&#39;Models extention on scaled data is not possible!&#39;)
                return
    
    if not os.path.isdir(output_path):
        os.mkdir(output_path)
            
    # load data
    print(&quot;Loading data ...&quot;)
    X = fileio.load(covfile)    
    Y, maskvol = load_response_vars(respfile, maskfile)
    batch_effects_train = fileio.load(trbefile)
    X_dummy = fileio.load(dummycovfile)
    batch_effects_dummy = fileio.load(dummybefile)
    
    if len(Y.shape) == 1:
        Y = Y[:, np.newaxis]
    if len(X.shape) == 1:
        X = X[:, np.newaxis]
    if len(X_dummy.shape) == 1:
        X_dummy = X_dummy[:, np.newaxis]
    feature_num = Y.shape[1]
    
    # estimate the models for all subjects
    for i in range(feature_num):
              
        nm = norm_init(X)
        if batch_size is not None: # when using nirmative_parallel
            print(&quot;Extending model &quot;, job_id*batch_size+i)
            nm = nm.load(os.path.join(model_path, &#39;NM_0_&#39; + 
                                      str(job_id*batch_size+i) + inputsuffix + 
                                      &#39;.pkl&#39;))
        else:
            print(&quot;Extending model &quot;, i+1, &quot;of&quot;, feature_num)
            nm = nm.load(os.path.join(model_path, &#39;NM_0_&#39; + str(i) + 
                                      inputsuffix +&#39;.pkl&#39;))
        
        nm = nm.extend(X, Y[:,i:i+1], batch_effects_train, X_dummy, 
                       batch_effects_dummy, samples=generation_factor, 
                       informative_prior=informative_prior)
        
        if batch_size is not None: 
            nm.save(os.path.join(output_path, &#39;NM_0_&#39; + 
                             str(job_id*batch_size+i) + outputsuffix + &#39;.pkl&#39;))
            nm.save(os.path.join(&#39;Models&#39;, &#39;NM_0_&#39; + 
                             str(i) + outputsuffix + &#39;.pkl&#39;))
        else:
            nm.save(os.path.join(output_path, &#39;NM_0_&#39; + 
                             str(i) + outputsuffix + &#39;.pkl&#39;))</div>


<div class="viewcode-block" id="main"><a class="viewcode-back" href="../pages/modindex.html#normative.main">[docs]</a>def main(*args):
    &quot;&quot;&quot; Parse arguments and estimate model
    &quot;&quot;&quot;

    np.seterr(invalid=&#39;ignore&#39;)

    rfile, mfile, cfile, cv, tcfile, trfile, func, alg, cfg, kw = get_args(args)
    
    # collect required arguments
    pos_args = [&#39;cfile&#39;, &#39;rfile&#39;]
    
    # collect basic keyword arguments controlling model estimation
    kw_args = [&#39;maskfile=mfile&#39;,
               &#39;cvfolds=cv&#39;,
               &#39;testcov=tcfile&#39;,
               &#39;testresp=trfile&#39;,
               &#39;alg=alg&#39;,
               &#39;configparam=cfg&#39;]
    
    # add additional keyword arguments
    for k in kw:
        kw_args.append(k + &#39;=&#39; + &quot;&#39;&quot; + kw[k] + &quot;&#39;&quot;)
    all_args = &#39;, &#39;.join(pos_args + kw_args)

    # Executing the target function
    exec(func + &#39;(&#39; + all_args + &#39;)&#39;)</div>

# For running from the command line:
if __name__ == &quot;__main__&quot;:
    main(sys.argv[1:])
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, Andre F. Marquand.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>