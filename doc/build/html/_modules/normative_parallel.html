<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>normative_parallel &mdash; Predictive Clinical Neuroscience Toolkit 0.20 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../_static/pages/css/pcntoolkit_tabs.css" type="text/css" />
      <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../_static/pages/css/pcntoolkit.css" type="text/css" />
      <link rel="stylesheet" href="../_static/pages/css/pcntoolkit_nomaxwidth.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/tabs.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html">
            <img src="../_static/pcn-logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pages/installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Background</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pages/pcntoolkit_background.html">PCNtoolkit Background</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Function &amp; Class Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modindex.html">Module Index</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pages/normative_modelling_walkthrough.html">Gaussian Process Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/HBR_NormativeModel_FCONdata_Tutorial.html">Hierarchical Bayesian Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/apply_normative_models.html">Braincharts: transfer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/BLR_normativemodel_protocol.html">Bayesian Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/visualizations.html">Visualization of normative modeling outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/post_hoc_analysis.html">Post-hoc analysis on normative modeling outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/other_predictive_models.html">Predictive modeling using deviation scores</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Useful Stuff</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pages/FAQs.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/citing.html">How to cite PCNtoolkit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pages/acknowledgements.html">Acknowledgements</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Predictive Clinical Neuroscience Toolkit</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">Module code</a> &raquo;</li>
      <li>normative_parallel</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for normative_parallel</h1><div class="highlight"><pre>
<span></span>#!/.../anaconda/bin/python/

# -----------------------------------------------------------------------------
# Run parallel normative modelling.
# All processing takes place in the processing directory (processing_dir)
# All inputs should be text files or binaries and space seperated
#
# It is possible to run these functions using...
#
# * k-fold cross-validation
# * estimating a training dataset then applying to a second test dataset
#
# First,the data is split for parallel processing.
# Second, the splits are submitted to the cluster.
# Third, the output is collected and combined.
#
# witten by (primarily) T Wolfers, (adaptated) SM Kia, H Huijsdens, L Parks, 
# S Rutherford, AF Marquand
# -----------------------------------------------------------------------------

from __future__ import print_function
from __future__ import division

import os
import sys
import glob
import shutil
import pickle
import fileinput
import time
import numpy as np
import pandas as pd
from subprocess import call, check_output


try:
    import pcntoolkit as ptk
    import pcntoolkit.dataio.fileio as fileio
    from pcntoolkit import configs
    from pcntoolkit.util.utils import yes_or_no 
    ptkpath = ptk.__path__[0] 
except ImportError:
    pass
    ptkpath = os.path.abspath(os.path.dirname(__file__))
    if ptkpath not in sys.path:
        sys.path.append(ptkpath)
    import dataio.fileio as fileio
    import configs
    from util.utils import yes_or_no 
    
    
PICKLE_PROTOCOL = configs.PICKLE_PROTOCOL


<div class="viewcode-block" id="execute_nm"><a class="viewcode-back" href="../modindex.html#normative_parallel.execute_nm">[docs]</a>def execute_nm(processing_dir,
               python_path,
               job_name,
               covfile_path,
               respfile_path,
               batch_size,
               memory,
               duration,
               normative_path=None,
               func=&#39;estimate&#39;,
               interactive=False,
               **kwargs):

    &#39;&#39;&#39; Execute parallel normative models
    This function is a mother function that executes all parallel normative
    modelling routines. Different specifications are possible using the sub-
    functions.

    Basic usage::

        execute_nm(processing_dir, python_path, job_name, covfile_path, respfile_path, batch_size, memory, duration)

    :param processing_dir: Full path to the processing dir
    :param python_path: Full path to the python distribution
    :param normative_path: Full path to the normative.py. If None (default) then it will automatically retrieves the path from the installed packeage.
    :param job_name: Name for the bash script that is the output of this function
    :param covfile_path: Full path to a .txt file that contains all covariats (subjects x covariates) for the responsefile
    :param respfile_path: Full path to a .txt that contains all features (subjects x features)
    :param batch_size: Number of features in each batch
    :param memory: Memory requirements written as string for example 4gb or 500mb
    :param duation: The approximate duration of the job, a string with HH:MM:SS for example 01:01:01
    :param cv_folds: Number of cross validations
    :param testcovfile_path: Full path to a .txt file that contains all covariates (subjects x covariates) for the test response file
    :param testrespfile_path: Full path to a .txt file that contains all test features
    :param log_path: Path for saving log files
    :param binary: If True uses binary format for response file otherwise it is text

    written by (primarily) T Wolfers, (adapted) SM Kia, (adapted) S Rutherford.
    &#39;&#39;&#39;
    
    if normative_path is None:
        normative_path = ptkpath + &#39;/normative.py&#39;
        
    cv_folds = kwargs.get(&#39;cv_folds&#39;, None)
    testcovfile_path = kwargs.get(&#39;testcovfile_path&#39;, None)
    testrespfile_path= kwargs.get(&#39;testrespfile_path&#39;, None)
    outputsuffix = kwargs.get(&#39;outputsuffix&#39;, &#39;estimate&#39;)
    cluster_spec = kwargs.pop(&#39;cluster_spec&#39;, &#39;torque&#39;)
    log_path = kwargs.pop(&#39;log_path&#39;, None)
    binary = kwargs.pop(&#39;binary&#39;, False)
    
    split_nm(processing_dir,
             respfile_path,
             batch_size,
             binary,
             **kwargs)

    batch_dir = glob.glob(processing_dir + &#39;batch_*&#39;)
    # print(batch_dir)
    number_of_batches = len(batch_dir)
    # print(number_of_batches)

    if binary:
        file_extentions = &#39;.pkl&#39;
    else:
        file_extentions = &#39;.txt&#39;
    
    kwargs.update({&#39;batch_size&#39;:str(batch_size)})
    job_ids = []
    for n in range(1, number_of_batches+1):
        kwargs.update({&#39;job_id&#39;:str(n)})
        if testrespfile_path is not None:
            if cv_folds is not None:
                raise(ValueError, &quot;&quot;&quot;If the response file is specified
                                     cv_folds must be equal to None&quot;&quot;&quot;)
            else:
                # specified train/test split
                batch_processing_dir = processing_dir + &#39;batch_&#39; + str(n) + &#39;/&#39;
                batch_job_name = job_name + &#39;_&#39; + str(n) + &#39;.sh&#39;
                batch_respfile_path = (batch_processing_dir + &#39;resp_batch_&#39; +
                                       str(n) + file_extentions)
                batch_testrespfile_path = (batch_processing_dir +
                                           &#39;testresp_batch_&#39; +
                                           str(n) + file_extentions)
                batch_job_path = batch_processing_dir + batch_job_name
                if cluster_spec == &#39;torque&#39;:
                    # update the response file 
                    kwargs.update({&#39;testrespfile_path&#39;: \
                                   batch_testrespfile_path})
                    bashwrap_nm(batch_processing_dir,
                                python_path,
                                normative_path,
                                batch_job_name,
                                covfile_path,
                                batch_respfile_path,
                                func=func,
                                **kwargs)
                    job_id = qsub_nm(job_path=batch_job_path,
                            log_path=log_path,
                            memory=memory,
                            duration=duration)
                    job_ids.append(job_id)
                elif cluster_spec == &#39;sbatch&#39;:
                    # update the response file 
                    kwargs.update({&#39;testrespfile_path&#39;: \
                                   batch_testrespfile_path})
                    sbatchwrap_nm(batch_processing_dir,
                                python_path,
                                normative_path,
                                batch_job_name,
                                covfile_path,
                                batch_respfile_path,
                                func=func,
                                memory=memory,
                                duration=duration,
                                **kwargs)
                    sbatch_nm(job_path=batch_job_path,
                            log_path=log_path)
                elif cluster_spec == &#39;new&#39;:
                    # this part requires addition in different envioronment [
                    sbatchwrap_nm(processing_dir=batch_processing_dir, 
                                  func=func, **kwargs)
                    sbatch_nm(processing_dir=batch_processing_dir)
                    # ]
        if testrespfile_path is None:
            if testcovfile_path is not None:
                # forward model
                batch_processing_dir = processing_dir + &#39;batch_&#39; + str(n) + &#39;/&#39;
                batch_job_name = job_name + &#39;_&#39; + str(n) + &#39;.sh&#39;
                batch_respfile_path = (batch_processing_dir + &#39;resp_batch_&#39; +
                                       str(n) + file_extentions)
                batch_job_path = batch_processing_dir + batch_job_name
                if cluster_spec == &#39;torque&#39;:
                    bashwrap_nm(batch_processing_dir,
                                python_path,
                                normative_path,
                                batch_job_name,
                                covfile_path,
                                batch_respfile_path,
                                func=func,
                                **kwargs)
                    job_id = qsub_nm(job_path=batch_job_path,
                            log_path=log_path,
                            memory=memory,
                            duration=duration)
                    job_ids.append(job_id)
                elif cluster_spec == &#39;sbatch&#39;:
                    sbatchwrap_nm(batch_processing_dir,
                                python_path,
                                normative_path,
                                batch_job_name,
                                covfile_path,
                                batch_respfile_path,
                                func=func,
                                memory=memory,
                                duration=duration,
                                **kwargs)
                    sbatch_nm(job_path=batch_job_path,
                              log_path=log_path)
                elif cluster_spec == &#39;new&#39;:
                    # this part requires addition in different envioronment [
                    bashwrap_nm(processing_dir=batch_processing_dir, func=func,
                                **kwargs)
                    qsub_nm(processing_dir=batch_processing_dir)
                    # ]
            else:
                # cross-validation
                batch_processing_dir = (processing_dir + &#39;batch_&#39; +
                                        str(n) + &#39;/&#39;)
                batch_job_name = job_name + &#39;_&#39; + str(n) + &#39;.sh&#39;
                batch_respfile_path = (batch_processing_dir +
                                       &#39;resp_batch_&#39; + str(n) +
                                       file_extentions)
                batch_job_path = batch_processing_dir + batch_job_name
                if cluster_spec == &#39;torque&#39;:
                    bashwrap_nm(batch_processing_dir,
                                python_path,
                                normative_path,
                                batch_job_name,
                                covfile_path,
                                batch_respfile_path,
                                func=func,
                                **kwargs)
                    job_id = qsub_nm(job_path=batch_job_path,
                            log_path=log_path,
                            memory=memory,
                            duration=duration)
                    job_ids.append(job_id)
                elif cluster_spec == &#39;sbatch&#39;:
                    sbatchwrap_nm(batch_processing_dir,
                                python_path,
                                normative_path,
                                batch_job_name,
                                covfile_path,
                                batch_respfile_path,
                                func=func,
                                memory=memory,
                                duration=duration,
                                **kwargs)
                    sbatch_nm(job_path=batch_job_path,
                            log_path=log_path)
                elif cluster_spec == &#39;new&#39;:
                    # this part requires addition in different envioronment [
                    bashwrap_nm(processing_dir=batch_processing_dir, func=func,
                                **kwargs)
                    qsub_nm(processing_dir=batch_processing_dir)
                    # ]

    if interactive:
        
        check_jobs(job_ids, delay=60)
        
        success = False
        while (not success):
            success = collect_nm(processing_dir,
                           job_name,
                           func=func,
                           collect=False,
                           binary=binary,
                           batch_size=batch_size,
                           outputsuffix=outputsuffix)
            if success:
                break
            else:
                if interactive == &#39;query&#39;:
                    response = yes_or_no(&#39;Rerun the failed jobs?&#39;)
                    if response:
                        rerun_nm(processing_dir, log_path=log_path, memory=memory, 
                                 duration=duration, binary=binary, 
                                 interactive=interactive)
                    else:
                        success = True
                else:
                    print(&#39;Reruning the failed jobs ...&#39;)
                    rerun_nm(processing_dir, log_path=log_path, memory=memory, 
                                 duration=duration, binary=binary, 
                                 interactive=interactive)
                    
        if interactive == &#39;query&#39;:
            response = yes_or_no(&#39;Collect the results?&#39;)
            if response:
                success = collect_nm(processing_dir,
                               job_name,
                               func=func,
                               collect=True,
                               binary=binary,
                               batch_size=batch_size,
                               outputsuffix=outputsuffix)
        else:
            print(&#39;Collecting the results ...&#39;)
            success = collect_nm(processing_dir,
                               job_name,
                               func=func,
                               collect=True,
                               binary=binary,
                               batch_size=batch_size,
                               outputsuffix=outputsuffix)</div>


&quot;&quot;&quot;routines that are environment independent&quot;&quot;&quot;

<div class="viewcode-block" id="split_nm"><a class="viewcode-back" href="../modindex.html#normative_parallel.split_nm">[docs]</a>def split_nm(processing_dir,
             respfile_path,
             batch_size,
             binary,
             **kwargs):

    &#39;&#39;&#39; This function prepares the input files for normative_parallel.
    
    Basic usage::

        split_nm(processing_dir, respfile_path, batch_size, binary, testrespfile_path)

    :param processing_dir: Full path to the processing dir
    :param respfile_path: Full path to the responsefile.txt (subjects x features)
    :param batch_size: Number of features in each batch
    :param testrespfile_path: Full path to the test responsefile.txt (subjects x features)
    :param binary: If True binary file

    :outputs: The creation of a folder struture for batch-wise processing.

    witten by (primarily) T Wolfers (adapted) SM Kia, (adapted) S Rutherford.
    &#39;&#39;&#39; 
    
    testrespfile_path = kwargs.pop(&#39;testrespfile_path&#39;, None)

    dummy, respfile_extension = os.path.splitext(respfile_path)
    if (binary and respfile_extension != &#39;.pkl&#39;):
        raise(ValueError, &quot;&quot;&quot;If binary is True the file format for the
              testrespfile file must be .pkl&quot;&quot;&quot;)
    elif (binary==False and respfile_extension != &#39;.txt&#39;):
        raise(ValueError, &quot;&quot;&quot;If binary is False the file format for the
              testrespfile file must be .txt&quot;&quot;&quot;)

    # splits response into batches
    if testrespfile_path is None:
        if (binary==False):
            respfile = fileio.load_ascii(respfile_path)
        else:
            respfile = pd.read_pickle(respfile_path)

        respfile = pd.DataFrame(respfile)

        numsub = respfile.shape[1]
        batch_vec = np.arange(0,
                              numsub,
                              batch_size)
        batch_vec = np.append(batch_vec,
                              numsub)
        
        for n in range(0, (len(batch_vec) - 1)):
            resp_batch = respfile.iloc[:, (batch_vec[n]): batch_vec[n + 1]]
            os.chdir(processing_dir)
            resp = str(&#39;resp_batch_&#39; + str(n+1))
            batch = str(&#39;batch_&#39; + str(n+1))
            if not os.path.exists(processing_dir + batch):
                os.makedirs(processing_dir + batch)
                os.makedirs(processing_dir + batch + &#39;/Models/&#39;)
            if (binary==False):
                fileio.save_pd(resp_batch,
                               processing_dir + batch + &#39;/&#39; +
                               resp + &#39;.txt&#39;)
            else:
                resp_batch.to_pickle(processing_dir + batch + &#39;/&#39; +
                                     resp + &#39;.pkl&#39;, protocol=PICKLE_PROTOCOL)

    # splits response and test responsefile into batches
    else:
        dummy, testrespfile_extension = os.path.splitext(testrespfile_path)
        if (binary and testrespfile_extension != &#39;.pkl&#39;):
            raise(ValueError, &quot;&quot;&quot;If binary is True the file format for the
                  testrespfile file must be .pkl&quot;&quot;&quot;)
        elif(binary==False and testrespfile_extension != &#39;.txt&#39;):
            raise(ValueError, &quot;&quot;&quot;If binary is False the file format for the
                  testrespfile file must be .txt&quot;&quot;&quot;)

        if (binary==False):
            respfile = fileio.load_ascii(respfile_path)
            testrespfile = fileio.load_ascii(testrespfile_path)
        else:
            respfile = pd.read_pickle(respfile_path)
            testrespfile = pd.read_pickle(testrespfile_path)

        respfile = pd.DataFrame(respfile)
        testrespfile = pd.DataFrame(testrespfile)

        numsub = respfile.shape[1]
        batch_vec = np.arange(0, numsub,
                              batch_size)
        batch_vec = np.append(batch_vec,
                              numsub)
        for n in range(0, (len(batch_vec) - 1)):
            resp_batch = respfile.iloc[:, (batch_vec[n]): batch_vec[n + 1]]
            testresp_batch = testrespfile.iloc[:, (batch_vec[n]): batch_vec[n +
                                             1]]
            os.chdir(processing_dir)
            resp = str(&#39;resp_batch_&#39; + str(n+1))
            testresp = str(&#39;testresp_batch_&#39; + str(n+1))
            batch = str(&#39;batch_&#39; + str(n+1))
            if not os.path.exists(processing_dir + batch):
                os.makedirs(processing_dir + batch)
                os.makedirs(processing_dir + batch + &#39;/Models/&#39;)
            if (binary==False):
                fileio.save_pd(resp_batch,
                               processing_dir + batch + &#39;/&#39; +
                               resp + &#39;.txt&#39;)
                fileio.save_pd(testresp_batch,
                               processing_dir + batch + &#39;/&#39; + testresp +
                               &#39;.txt&#39;)
            else:
                resp_batch.to_pickle(processing_dir + batch + &#39;/&#39; +
                                     resp + &#39;.pkl&#39;, protocol=PICKLE_PROTOCOL)
                testresp_batch.to_pickle(processing_dir + batch + &#39;/&#39; +
                                         testresp + &#39;.pkl&#39;, 
                                         protocol=PICKLE_PROTOCOL)</div>


<div class="viewcode-block" id="collect_nm"><a class="viewcode-back" href="../modindex.html#normative_parallel.collect_nm">[docs]</a>def collect_nm(processing_dir,
               job_name,
               func=&#39;estimate&#39;,
               collect=False,
               binary=False,
               batch_size=None,
               outputsuffix=&#39;_estimate&#39;):
    
    &#39;&#39;&#39;Function to checks and collects all batches.

    Basic usage::

        collect_nm(processing_dir, job_name)


    :param processing_dir: Full path to the processing directory
    :param collect: If True data is checked for failed batches and collected; if False data is just checked
    :param binary: Results in pkl format

    :outputs: Text files containing all results accross all batches the combined output (written to disk).

    :returns 0: if batches fail
    :returns 1: if bathches complete successfully

    written by (primarily) T Wolfers, (adapted) SM Kia, (adapted) S Rutherford.
    &#39;&#39;&#39;

    if binary:
        file_extentions = &#39;.pkl&#39;
    else:
        file_extentions = &#39;.txt&#39;

    # detect number of subjects, batches, hyperparameters and CV
    batches = glob.glob(processing_dir + &#39;batch_*/&#39;)
    
    count = 0
    batch_fail = []
    
    if (func!=&#39;fit&#39; and func!=&#39;extend&#39; and func!=&#39;merge&#39; and func!=&#39;tune&#39;):
        file_example = []
        # TODO: Collect_nm only depends on yhat, thus does not work when no 
        # prediction is made (when test cov is not specified). 
        for batch in batches:
            if file_example == []:
                file_example = glob.glob(batch + &#39;yhat&#39; + outputsuffix 
                                         + file_extentions)
            else:
                break
        if binary is False:
            file_example = fileio.load(file_example[0])
        else:
            file_example = pd.read_pickle(file_example[0])
        numsubjects = file_example.shape[0]
        batch_size = file_example.shape[1]
    
        # artificially creates files for batches that were not executed
        batch_dirs = glob.glob(processing_dir + &#39;batch_*/&#39;)
        batch_dirs = fileio.sort_nicely(batch_dirs)
        for batch in batch_dirs:
            filepath = glob.glob(batch + &#39;yhat&#39; + outputsuffix + &#39;*&#39;)
            if filepath == []:
                count = count+1
                batch1 = glob.glob(batch + &#39;/&#39; + job_name + &#39;*.sh&#39;)
                print(batch1)
                batch_fail.append(batch1)
                if collect is True:
                    pRho = np.ones(batch_size)
                    pRho = pRho.transpose()
                    pRho = pd.Series(pRho)
                    fileio.save(pRho, batch + &#39;pRho&#39; + outputsuffix + 
                                file_extentions)
                    
                    Rho = np.zeros(batch_size)
                    Rho = Rho.transpose()
                    Rho = pd.Series(Rho)
                    fileio.save(Rho, batch + &#39;Rho&#39; + outputsuffix + 
                                file_extentions)
                    
                    rmse = np.zeros(batch_size)
                    rmse = rmse.transpose()
                    rmse = pd.Series(rmse)
                    fileio.save(rmse, batch + &#39;RMSE&#39; + outputsuffix + 
                                file_extentions)
                    
                    smse = np.zeros(batch_size)
                    smse = smse.transpose()
                    smse = pd.Series(smse)
                    fileio.save(smse, batch + &#39;SMSE&#39; + outputsuffix + 
                                file_extentions)
                    
                    expv = np.zeros(batch_size)
                    expv = expv.transpose()
                    expv = pd.Series(expv)
                    fileio.save(expv, batch + &#39;EXPV&#39; + outputsuffix + 
                                file_extentions)
                    
                    msll = np.zeros(batch_size)
                    msll = msll.transpose()
                    msll = pd.Series(msll)
                    fileio.save(msll, batch + &#39;MSLL&#39; + outputsuffix + 
                                file_extentions)
    
                    yhat = np.zeros([numsubjects, batch_size])
                    yhat = pd.DataFrame(yhat)
                    fileio.save(yhat, batch + &#39;yhat&#39; + outputsuffix + 
                                file_extentions)
    
                    ys2 = np.zeros([numsubjects, batch_size])
                    ys2 = pd.DataFrame(ys2)
                    fileio.save(ys2, batch + &#39;ys2&#39; + outputsuffix + 
                                file_extentions)
    
                    Z = np.zeros([numsubjects, batch_size])
                    Z = pd.DataFrame(Z)
                    fileio.save(Z, batch + &#39;Z&#39; + outputsuffix + 
                                file_extentions)
                    
                    nll = np.zeros(batch_size)
                    nll = nll.transpose()
                    nll = pd.Series(nll)
                    fileio.save(nll, batch + &#39;NLL&#39; + outputsuffix + 
                                file_extentions)
                    
                    bic = np.zeros(batch_size)
                    bic = bic.transpose()
                    bic = pd.Series(bic)
                    fileio.save(bic, batch + &#39;BIC&#39; + outputsuffix + 
                                file_extentions)
    
                    if not os.path.isdir(batch + &#39;Models&#39;):
                        os.mkdir(&#39;Models&#39;)
                        
                        
            else: # if more than 10% of yhat is nan then it is a failed batch
                yhat = fileio.load(filepath[0])
                if np.count_nonzero(~np.isnan(yhat))/(np.prod(yhat.shape))&lt;0.9:
                    count = count+1
                    batch1 = glob.glob(batch + &#39;/&#39; + job_name + &#39;*.sh&#39;)
                    print(&#39;More than 10% nans in &#39;+ batch1[0])
                    batch_fail.append(batch1)
    
    else:
        batch_dirs = glob.glob(processing_dir + &#39;batch_*/&#39;)
        batch_dirs = fileio.sort_nicely(batch_dirs)
        for batch in batch_dirs:
            filepath = glob.glob(batch + &#39;Models/&#39; + &#39;NM_&#39; + &#39;*&#39; + outputsuffix 
                                 + &#39;*&#39;)
            if len(filepath) &lt; batch_size:
                count = count+1
                batch1 = glob.glob(batch + &#39;/&#39; + job_name + &#39;*.sh&#39;)
                print(batch1)
                batch_fail.append(batch1)
    
    # combines all output files across batches
    if collect is True:
        pRho_filenames = glob.glob(processing_dir + &#39;batch_*/&#39; + &#39;pRho&#39; + 
                                   outputsuffix + &#39;*&#39;)
        if pRho_filenames:
            pRho_filenames = fileio.sort_nicely(pRho_filenames)
            pRho_dfs = []
            for pRho_filename in pRho_filenames:
                pRho_dfs.append(pd.DataFrame(fileio.load(pRho_filename)))
            pRho_dfs = pd.concat(pRho_dfs, ignore_index=True, axis=0)
            fileio.save(pRho_dfs, processing_dir + &#39;pRho&#39; + outputsuffix +
                        file_extentions)
            del pRho_dfs

        Rho_filenames = glob.glob(processing_dir + &#39;batch_*/&#39; + &#39;Rho&#39; + 
                                   outputsuffix + &#39;*&#39;)
        if Rho_filenames:
            Rho_filenames = fileio.sort_nicely(Rho_filenames)
            Rho_dfs = []
            for Rho_filename in Rho_filenames:
                Rho_dfs.append(pd.DataFrame(fileio.load(Rho_filename)))
            Rho_dfs = pd.concat(Rho_dfs, ignore_index=True, axis=0)
            fileio.save(Rho_dfs, processing_dir + &#39;Rho&#39; + outputsuffix +
                        file_extentions)
            del Rho_dfs

        Z_filenames = glob.glob(processing_dir + &#39;batch_*/&#39; + &#39;Z&#39; + 
                                   outputsuffix + &#39;*&#39;)
        if Z_filenames:
            Z_filenames = fileio.sort_nicely(Z_filenames)
            Z_dfs = []
            for Z_filename in Z_filenames:
                Z_dfs.append(pd.DataFrame(fileio.load(Z_filename)))
            Z_dfs = pd.concat(Z_dfs, ignore_index=True, axis=1)
            fileio.save(Z_dfs, processing_dir + &#39;Z&#39; + outputsuffix +
                        file_extentions)
            del Z_dfs
            
        yhat_filenames = glob.glob(processing_dir + &#39;batch_*/&#39; + &#39;yhat&#39; + 
                                   outputsuffix + &#39;*&#39;)
        if yhat_filenames:
            yhat_filenames = fileio.sort_nicely(yhat_filenames)
            yhat_dfs = []
            for yhat_filename in yhat_filenames:
                yhat_dfs.append(pd.DataFrame(fileio.load(yhat_filename)))
            yhat_dfs = pd.concat(yhat_dfs, ignore_index=True, axis=1)
            fileio.save(yhat_dfs, processing_dir + &#39;yhat&#39; + outputsuffix +
                        file_extentions)
            del yhat_dfs

        ys2_filenames = glob.glob(processing_dir + &#39;batch_*/&#39; + &#39;ys2&#39; + 
                                   outputsuffix + &#39;*&#39;)
        if ys2_filenames:
            ys2_filenames = fileio.sort_nicely(ys2_filenames)
            ys2_dfs = []
            for ys2_filename in ys2_filenames:
                ys2_dfs.append(pd.DataFrame(fileio.load(ys2_filename)))
            ys2_dfs = pd.concat(ys2_dfs, ignore_index=True, axis=1)
            fileio.save(ys2_dfs, processing_dir + &#39;ys2&#39; + outputsuffix +
                        file_extentions)
            del ys2_dfs

        rmse_filenames = glob.glob(processing_dir + &#39;batch_*/&#39; + &#39;RMSE&#39; + 
                                   outputsuffix + &#39;*&#39;)
        if rmse_filenames:
            rmse_filenames = fileio.sort_nicely(rmse_filenames)
            rmse_dfs = []
            for rmse_filename in rmse_filenames:
                rmse_dfs.append(pd.DataFrame(fileio.load(rmse_filename)))
            rmse_dfs = pd.concat(rmse_dfs, ignore_index=True, axis=0)
            fileio.save(rmse_dfs, processing_dir + &#39;RMSE&#39; + outputsuffix +
                        file_extentions)
            del rmse_dfs

        smse_filenames = glob.glob(processing_dir + &#39;batch_*/&#39; + &#39;SMSE&#39; + 
                                   outputsuffix + &#39;*&#39;)
        if smse_filenames:
            smse_filenames = fileio.sort_nicely(smse_filenames)
            smse_dfs = []
            for smse_filename in smse_filenames:
                smse_dfs.append(pd.DataFrame(fileio.load(smse_filename)))
            smse_dfs = pd.concat(smse_dfs, ignore_index=True, axis=0)
            fileio.save(smse_dfs, processing_dir + &#39;SMSE&#39; + outputsuffix +
                        file_extentions)
            del smse_dfs
            
        expv_filenames = glob.glob(processing_dir + &#39;batch_*/&#39; + &#39;EXPV&#39; + 
                                   outputsuffix + &#39;*&#39;)
        if expv_filenames:
            expv_filenames = fileio.sort_nicely(expv_filenames)
            expv_dfs = []
            for expv_filename in expv_filenames:
                expv_dfs.append(pd.DataFrame(fileio.load(expv_filename)))
            expv_dfs = pd.concat(expv_dfs, ignore_index=True, axis=0)
            fileio.save(expv_dfs, processing_dir + &#39;EXPV&#39; + outputsuffix +
                        file_extentions)
            del expv_dfs
            
        msll_filenames = glob.glob(processing_dir + &#39;batch_*/&#39; + &#39;MSLL&#39; + 
                                   outputsuffix + &#39;*&#39;)
        if msll_filenames:
            msll_filenames = fileio.sort_nicely(msll_filenames)
            msll_dfs = []
            for msll_filename in msll_filenames:
                msll_dfs.append(pd.DataFrame(fileio.load(msll_filename)))
            msll_dfs = pd.concat(msll_dfs, ignore_index=True, axis=0)
            fileio.save(msll_dfs, processing_dir + &#39;MSLL&#39; + outputsuffix +
                        file_extentions)
            del msll_dfs
            
        nll_filenames = glob.glob(processing_dir + &#39;batch_*/&#39; + &#39;NLL&#39; +
                                  outputsuffix + &#39;*&#39;)
        if nll_filenames:
            nll_filenames = fileio.sort_nicely(nll_filenames)
            nll_dfs = []
            for nll_filename in nll_filenames:
                nll_dfs.append(pd.DataFrame(fileio.load(nll_filename)))
            nll_dfs = pd.concat(nll_dfs, ignore_index=True, axis=0)
            fileio.save(nll_dfs, processing_dir + &#39;NLL&#39; + outputsuffix +
                        file_extentions)
            del nll_dfs

        bic_filenames = glob.glob(processing_dir + &#39;batch_*/&#39; + &#39;BIC&#39; +
                                  outputsuffix + &#39;*&#39;)
        if bic_filenames:
            bic_filenames = fileio.sort_nicely(bic_filenames)
            bic_dfs = []
            for bic_filename in bic_filenames:
                bic_dfs.append(pd.DataFrame(fileio.load(bic_filename)))
            bic_dfs = pd.concat(bic_dfs, ignore_index=True, axis=0)
            fileio.save(bic_dfs, processing_dir + &#39;BIC&#39; + outputsuffix +
                        file_extentions)
            del bic_dfs
        
        if (func!=&#39;predict&#39; and func!=&#39;extend&#39; and func!=&#39;merge&#39; and func!=&#39;tune&#39;):
            if not os.path.isdir(processing_dir + &#39;Models&#39;) and \
               os.path.exists(os.path.join(batches[0], &#39;Models&#39;)):
                os.mkdir(processing_dir + &#39;Models&#39;)
                
            meta_filenames = glob.glob(processing_dir + &#39;batch_*/Models/&#39; + 
                                       &#39;meta_data.md&#39;)
            mY = []
            sY = []
            X_scalers = []
            Y_scalers = []
            if meta_filenames:
                meta_filenames = fileio.sort_nicely(meta_filenames)
                with open(meta_filenames[0], &#39;rb&#39;) as file:
                    meta_data = pickle.load(file)
                
                for meta_filename in meta_filenames:
                    with open(meta_filename, &#39;rb&#39;) as file:
                        meta_data = pickle.load(file)
                    mY.append(meta_data[&#39;mean_resp&#39;])
                    sY.append(meta_data[&#39;std_resp&#39;])
                    if meta_data[&#39;inscaler&#39;] in [&#39;standardize&#39;, &#39;minmax&#39;, 
                                &#39;robminmax&#39;]:
                        X_scalers.append(meta_data[&#39;scaler_cov&#39;])
                    if meta_data[&#39;outscaler&#39;] in [&#39;standardize&#39;, &#39;minmax&#39;, 
                                &#39;robminmax&#39;]:
                        Y_scalers.append(meta_data[&#39;scaler_resp&#39;])
                meta_data[&#39;mean_resp&#39;] = np.squeeze(np.column_stack(mY)) 
                meta_data[&#39;std_resp&#39;] = np.squeeze(np.column_stack(sY))
                meta_data[&#39;scaler_cov&#39;] = X_scalers 
                meta_data[&#39;scaler_resp&#39;] = Y_scalers
                
                with open(os.path.join(processing_dir, &#39;Models&#39;, 
                                       &#39;meta_data.md&#39;), &#39;wb&#39;) as file:
                    pickle.dump(meta_data, file, protocol=PICKLE_PROTOCOL)
            
            batch_dirs = glob.glob(processing_dir + &#39;batch_*/&#39;)
            if batch_dirs:
                batch_dirs = fileio.sort_nicely(batch_dirs)
                for b, batch_dir in enumerate(batch_dirs):
                    src_files = glob.glob(batch_dir + &#39;Models/NM*&#39; + 
                                          outputsuffix + &#39;.pkl&#39;)
                    if src_files:
                        src_files = fileio.sort_nicely(src_files)
                        for f, full_file_name in enumerate(src_files):
                            if os.path.isfile(full_file_name):
                                file_name = full_file_name.split(&#39;/&#39;)[-1]
                                n = file_name.split(&#39;_&#39;)
                                n[-2] = str(b * batch_size + f)
                                n = &#39;_&#39;.join(n)
                                shutil.copy(full_file_name, processing_dir + 
                                            &#39;Models/&#39; + n)
                    elif func==&#39;fit&#39;:
                        count = count+1
                        batch1 = glob.glob(batch_dir + &#39;/&#39; + job_name + &#39;*.sh&#39;)
                        print(&#39;Failed batch: &#39; + batch1[0])
                        batch_fail.append(batch1)
                        
    # list batches that were not executed
    print(&#39;Number of batches that failed:&#39; + str(count))
    batch_fail_df = pd.DataFrame(batch_fail)
    if file_extentions == &#39;.txt&#39;:
        fileio.save_pd(batch_fail_df, processing_dir + &#39;failed_batches&#39;+
                file_extentions)
    else:
        fileio.save(batch_fail_df, processing_dir +
            &#39;failed_batches&#39; +
            file_extentions)

    if not batch_fail:
        return True
    else:
        return False</div>

<div class="viewcode-block" id="delete_nm"><a class="viewcode-back" href="../modindex.html#normative_parallel.delete_nm">[docs]</a>def delete_nm(processing_dir,
              binary=False):
    &#39;&#39;&#39;This function deletes all processing for normative modelling and just keeps the combined output.

    Basic usage::

        collect_nm(processing_dir)

    :param processing_dir: Full path to the processing directory.
    :param binary: Results in pkl format.

    written by (primarily) T Wolfers, (adapted) SM Kia, (adapted) S Rutherford.
    &#39;&#39;&#39;
    
    if binary:
        file_extentions = &#39;.pkl&#39;
    else:
        file_extentions = &#39;.txt&#39;
    for file in glob.glob(processing_dir + &#39;batch_*/&#39;):
        shutil.rmtree(file)
    if os.path.exists(processing_dir + &#39;failed_batches&#39; + file_extentions):
        os.remove(processing_dir + &#39;failed_batches&#39; + file_extentions)</div>


# all routines below are envronment dependent and require adaptation in novel
# environments -&gt; copy those routines and adapt them in accrodance with your
# environment

<div class="viewcode-block" id="bashwrap_nm"><a class="viewcode-back" href="../modindex.html#normative_parallel.bashwrap_nm">[docs]</a>def bashwrap_nm(processing_dir,
                python_path,
                normative_path,
                job_name,
                covfile_path,
                respfile_path,
                func=&#39;estimate&#39;,
                **kwargs):

    &#39;&#39;&#39; This function wraps normative modelling into a bash script to run it
    on a torque cluster system.

    Basic usage::

        bashwrap_nm(processing_dir, python_path, normative_path, job_name, covfile_path, respfile_path)

    :param processing_dir: Full path to the processing dir
    :param python_path: Full path to the python distribution
    :param normative_path: Full path to the normative.py
    :param job_name: Name for the bash script that is the output of this function
    :param covfile_path: Full path to a .txt file that contains all covariates (subjects x covariates) for the responsefile
    :param respfile_path: Full path to a .txt that contains all features (subjects x features)
    :param cv_folds: Number of cross validations
    :param testcovfile_path: Full path to a .txt file that contains all covariates (subjects x covariates) for the testresponse file
    :param testrespfile_path: Full path to a .txt file that contains all test features
    :param alg: which algorithm to use
    :param configparam: configuration parameters for this algorithm

    :outputs: A bash.sh file containing the commands for normative modelling saved to the processing directory (written to disk).

    written by (primarily) T Wolfers, (adapted) S Rutherford.
    &#39;&#39;&#39;
    
    # here we use pop not get to remove the arguments as they used 
    cv_folds = kwargs.pop(&#39;cv_folds&#39;,None)
    testcovfile_path = kwargs.pop(&#39;testcovfile_path&#39;, None)
    testrespfile_path = kwargs.pop(&#39;testrespfile_path&#39;, None)
    alg = kwargs.pop(&#39;alg&#39;, None)
    configparam = kwargs.pop(&#39;configparam&#39;, None)
    
    # change to processing dir
    os.chdir(processing_dir)
    output_changedir = [&#39;cd &#39; + processing_dir + &#39;\n&#39;]

    bash_lines = &#39;#!/bin/bash\n&#39;
    bash_cores = &#39;export OMP_NUM_THREADS=1\n&#39;
    bash_environment = [bash_lines + bash_cores]

    # creates call of function for normative modelling
    if (testrespfile_path is not None) and (testcovfile_path is not None):
        job_call = [python_path + &#39; &#39; + normative_path + &#39; -c &#39; +
                    covfile_path + &#39; -t &#39; + testcovfile_path + &#39; -r &#39; +
                    testrespfile_path + &#39; -f &#39; + func]
    elif (testrespfile_path is None) and (testcovfile_path is not None):
        job_call = [python_path + &#39; &#39; + normative_path + &#39; -c &#39; +
                    covfile_path + &#39; -t &#39; + testcovfile_path + &#39; -f &#39; + func]
    elif cv_folds is not None:
        job_call = [python_path + &#39; &#39; + normative_path + &#39; -c &#39; +
                    covfile_path + &#39; -k &#39; + str(cv_folds) +  &#39; -f &#39; + func]
    elif func != &#39;estimate&#39;:
        job_call = [python_path + &#39; &#39; + normative_path + &#39; -c &#39; +
                    covfile_path +  &#39; -f &#39; + func]
    else:
        raise(ValueError, &quot;&quot;&quot;For &#39;estimate&#39; function either testcov or cvfold
              must be specified.&quot;&quot;&quot;)
        
    # add algorithm-specific parameters
    if alg is not None:
        job_call = [job_call[0] + &#39; -a &#39; + alg]
        if configparam is not None:
            job_call = [job_call[0] + &#39; -x &#39; + str(configparam)]
    
    # add standardization flag if it is false
    # if not standardize:
    #     job_call = [job_call[0] + &#39; -s&#39;]
    
    # add responses file
    job_call = [job_call[0] + &#39; &#39; + respfile_path]
    
    # add in optional arguments. 
    for k in kwargs:
        job_call = [job_call[0] + &#39; &#39; + k + &#39;=&#39; + kwargs[k]]

    # writes bash file into processing dir
    with open(processing_dir+job_name, &#39;w&#39;) as bash_file:
        bash_file.writelines(bash_environment + output_changedir + \
                             job_call + [&quot;\n&quot;])

    # changes permissoins for bash.sh file
    os.chmod(processing_dir + job_name, 0o700)</div>


<div class="viewcode-block" id="qsub_nm"><a class="viewcode-back" href="../modindex.html#normative_parallel.qsub_nm">[docs]</a>def qsub_nm(job_path,
            log_path,
            memory,
            duration):
    
    &#39;&#39;&#39;This function submits a job.sh scipt to the torque custer using the qsub command.
    
    Basic usage::


        qsub_nm(job_path, log_path, memory, duration)

    :param job_path: Full path to the job.sh file.
    :param memory: Memory requirements written as string for example 4gb or 500mb.
    :param duation: The approximate duration of the job, a string with HH:MM:SS for example 01:01:01.

    :outputs: Submission of the job to the (torque) cluster.

    written by (primarily) T Wolfers, (adapted) SM Kia, (adapted) S Rutherford.
    &#39;&#39;&#39;
  
    # created qsub command
    if log_path is None:
        qsub_call = [&#39;echo &#39; + job_path + &#39; | qsub -N &#39; + job_path + &#39; -l &#39; +
                     &#39;procs=1&#39; + &#39;,mem=&#39; + memory + &#39;,walltime=&#39; + duration]
    else:
        qsub_call = [&#39;echo &#39; + job_path + &#39; | qsub -N &#39; + job_path +
                     &#39; -l &#39; + &#39;procs=1&#39; + &#39;,mem=&#39; + memory + &#39;,walltime=&#39; + 
                     duration + &#39; -o &#39; + log_path + &#39; -e &#39; + log_path]

    # submits job to cluster
    #call(qsub_call, shell=True)
    job_id = check_output(qsub_call, shell=True).decode(sys.stdout.encoding).replace(&quot;\n&quot;, &quot;&quot;)
    
    return job_id</div>


def rerun_nm(processing_dir,
             log_path,
             memory,
             duration,
             binary=False, 
             interactive=False):
    &#39;&#39;&#39;This function reruns all failed batched in processing_dir after collect_nm has identified the failed batches.
    Basic usage::           

        rerun_nm(processing_dir, log_path, memory, duration)

    :param processing_dir: Full path to the processing directory
    :param memory: Memory requirements written as string for example 4gb or 500mb.
    :param duration: The approximate duration of the job, a string with HH:MM:SS for example 01:01:01.

    written by (primarily) T Wolfers, (adapted) SM Kia, (adapted) S Rutherford.
    &#39;&#39;&#39;
    
    job_ids = []
    

    if binary:
        file_extentions = &#39;.pkl&#39;
        failed_batches = fileio.load(processing_dir +
                                       &#39;failed_batches&#39; + file_extentions)
        shape = failed_batches.shape
        for n in range(0, shape[0]):
            jobpath = failed_batches[n, 0]
            print(jobpath)
            job_id = qsub_nm(job_path=jobpath,
                    log_path=log_path,
                    memory=memory,
                    duration=duration)
            job_ids.append(job_id)
    else:
        file_extentions = &#39;.txt&#39;
        failed_batches = fileio.load_pd(processing_dir +
                                       &#39;failed_batches&#39; + file_extentions)
        shape = failed_batches.shape
        for n in range(0, shape[0]):
            jobpath = failed_batches.iloc[n, 0]
            print(jobpath)
            job_id = qsub_nm(job_path=jobpath,
                    log_path=log_path,
                    memory=memory,
                    duration=duration)
            job_ids.append(job_id)
            
    if interactive: 
        check_jobs(job_ids, delay=60)
        

# COPY the rotines above here and aadapt those to your cluster
# bashwarp_nm; qsub_nm; rerun_nm

<div class="viewcode-block" id="sbatchwrap_nm"><a class="viewcode-back" href="../modindex.html#normative_parallel.sbatchwrap_nm">[docs]</a>def sbatchwrap_nm(processing_dir,
                  python_path,
                  normative_path,
                  job_name,
                  covfile_path,
                  respfile_path,
                  memory,
                  duration,
                  func=&#39;estimate&#39;,
                  **kwargs):

    &#39;&#39;&#39;This function wraps normative modelling into a bash script to run it
    on a torque cluster system.

    Basic usage::

        sbatchwrap_nm(processing_dir, python_path, normative_path, job_name, covfile_path, respfile_path, memory, duration)

    :param processing_dir: Full path to the processing dir
    :param python_path: Full path to the python distribution
    :param normative_path: Full path to the normative.py
    :param job_name: Name for the bash script that is the output of this function
    :param covfile_path: Full path to a .txt file that contains all covariates (subjects x covariates) for the responsefile
    :param respfile_path: Full path to a .txt that contains all features (subjects x features)
    :param cv_folds: Number of cross validations
    :param testcovfile_path: Full path to a .txt file that contains all covariates (subjects x covariates) for the testresponse file
    :param testrespfile_path: Full path to a .txt file that contains all test features
    :param alg: which algorithm to use
    :param configparam: configuration parameters for this algorithm

    :outputs: A bash.sh file containing the commands for normative modelling saved to the processing directory (written to disk).

    written by (primarily) T Wolfers, (adapted) S Rutherford
    &#39;&#39;&#39;
    
    # here we use pop not get to remove the arguments as they used 
    cv_folds = kwargs.pop(&#39;cv_folds&#39;,None)
    testcovfile_path = kwargs.pop(&#39;testcovfile_path&#39;, None)
    testrespfile_path = kwargs.pop(&#39;testrespfile_path&#39;, None)
    alg = kwargs.pop(&#39;alg&#39;, None)
    configparam = kwargs.pop(&#39;configparam&#39;, None)
    
    # change to processing dir
    os.chdir(processing_dir)
    output_changedir = [&#39;cd &#39; + processing_dir + &#39;\n&#39;]

    sbatch_init=&#39;#!/bin/bash\n&#39;
    sbatch_jobname=&#39;#SBATCH --job-name=&#39; + processing_dir + &#39;\n&#39;
    sbatch_account=&#39;#SBATCH --account=p33_norment\n&#39;
    sbatch_nodes=&#39;#SBATCH --nodes=1\n&#39;
    sbatch_tasks=&#39;#SBATCH --ntasks=1\n&#39;
    sbatch_time=&#39;#SBATCH --time=&#39; + str(duration) + &#39;\n&#39;
    sbatch_memory=&#39;#SBATCH --mem-per-cpu=&#39; + str(memory) + &#39;\n&#39;
    sbatch_module=&#39;module purge\n&#39;
    sbatch_anaconda=&#39;module load anaconda3\n&#39;
    sbatch_exit=&#39;set -o errexit\n&#39;

    #echo -n &quot;This script is running on &quot;
    #hostname
    
    bash_environment = [sbatch_init + 
                        sbatch_jobname +
                        sbatch_account +
                        sbatch_nodes +
                        sbatch_tasks +
                        sbatch_time +
                        sbatch_module +
                        sbatch_anaconda]

    # creates call of function for normative modelling
    if (testrespfile_path is not None) and (testcovfile_path is not None):
        job_call = [python_path + &#39; &#39; + normative_path + &#39; -c &#39; +
                    covfile_path + &#39; -t &#39; + testcovfile_path + &#39; -r &#39; +
                    testrespfile_path + &#39; -f &#39; + func]
    elif (testrespfile_path is None) and (testcovfile_path is not None):
        job_call = [python_path + &#39; &#39; + normative_path + &#39; -c &#39; +
                    covfile_path + &#39; -t &#39; + testcovfile_path + &#39; -f &#39; + func]
    elif cv_folds is not None:
        job_call = [python_path + &#39; &#39; + normative_path + &#39; -c &#39; +
                    covfile_path + &#39; -k &#39; + str(cv_folds) +  &#39; -f &#39; + func]
    elif func != &#39;estimate&#39;:
        job_call = [python_path + &#39; &#39; + normative_path + &#39; -c &#39; +
                    covfile_path +  &#39; -f &#39; + func]
    else:
        raise(ValueError, &quot;&quot;&quot;For &#39;estimate&#39; function either testcov or cvfold
              must be specified.&quot;&quot;&quot;)
        
    # add algorithm-specific parameters
    if alg is not None:
        job_call = [job_call[0] + &#39; -a &#39; + alg]
        if configparam is not None:
            job_call = [job_call[0] + &#39; -x &#39; + str(configparam)]
    
    # add standardization flag if it is false
    # if not standardize:
    #     job_call = [job_call[0] + &#39; -s&#39;]
    
    # add responses file
    job_call = [job_call[0] + &#39; &#39; + respfile_path]
    
    # add in optional arguments. 
    for k in kwargs:
        job_call = [job_call[0] + &#39; &#39; + k + &#39;=&#39; + kwargs[k]]

    # writes bash file into processing dir
    with open(processing_dir+job_name, &#39;w&#39;) as bash_file:
        bash_file.writelines(bash_environment + output_changedir + \
                             job_call + [&quot;\n&quot;] + [sbatch_exit])

    # changes permissoins for bash.sh file
    os.chmod(processing_dir + job_name, 0o700)</div>

<div class="viewcode-block" id="sbatch_nm"><a class="viewcode-back" href="../modindex.html#normative_parallel.sbatch_nm">[docs]</a>def sbatch_nm(job_path,
              log_path):
    
    &#39;&#39;&#39;This function submits a job.sh scipt to the torque custer using the qsub
    command.

    Basic usage::

        sbatch_nm(job_path, log_path)

    :param job_path: Full path to the job.sh file
    :param log_path: The logs are currently stored in the working dir

    :outputs: Submission of the job to the (torque) cluster.

    written by (primarily) T Wolfers, (adapted) S Rutherford.
    &#39;&#39;&#39;

    # created qsub command
    sbatch_call = [&#39;sbatch &#39; + job_path]

    # submits job to cluster
    call(sbatch_call, shell=True)</div>
    
<div class="viewcode-block" id="rerun_nm"><a class="viewcode-back" href="../modindex.html#normative_parallel.rerun_nm">[docs]</a>def rerun_nm(processing_dir,
                 memory,
                 duration,
                 new_memory=False,
                 new_duration=False,
                 binary=False,
                 **kwargs):
        
    &#39;&#39;&#39;This function reruns all failed batched in processing_dir after collect_nm has identified he failed batches.
    
    Basic usage::

        rerun_nm(processing_dir, memory, duration)

    :param processing_dir: Full path to the processing directory.
    :param memory: Memory requirements written as string, for example 4gb or 500mb.
    :param duration: The approximate duration of the job, a string with HH:MM:SS for example 01:01:01.
    :param new_memory: If you want to change the memory you have to indicate it here.
    :param new_duration: If you want to change the duration you have to indicate it here.

    :outputs: Re-runs failed batches. 
    
     written by (primarily) T Wolfers, (adapted) S Rutherford.
    &#39;&#39;&#39;
    log_path = kwargs.pop(&#39;log_path&#39;, None)
    
    if binary:
        file_extentions = &#39;.pkl&#39;
        failed_batches = fileio.load(processing_dir + &#39;failed_batches&#39; +  file_extentions)
        shape = failed_batches.shape
        for n in range(0, shape[0]):
            jobpath = failed_batches[n, 0]
            print(jobpath)
            if new_duration != False:
                with fileinput.FileInput(jobpath, inplace=True) as file:
                    for line in file:
                        print(line.replace(duration, new_duration), end=&#39;&#39;)
                if new_memory != False:
                    with fileinput.FileInput(jobpath, inplace=True) as file:
                        for line in file:
                            print(line.replace(memory, new_memory), end=&#39;&#39;)
                sbatch_nm(jobpath, log_path)

    else:
        file_extentions = &#39;.txt&#39;
        failed_batches = fileio.load_pd(processing_dir + &#39;failed_batches&#39; + file_extentions)
        shape = failed_batches.shape
        for n in range(0, shape[0]):
            jobpath = failed_batches.iloc[n, 0]
            print(jobpath)
            if new_duration != False:
                with fileinput.FileInput(jobpath, inplace=True) as file:
                    for line in file:
                        print(line.replace(duration, new_duration), end=&#39;&#39;)
                if new_memory != False:
                    with fileinput.FileInput(jobpath, inplace=True) as file:
                        for line in file:
                            print(line.replace(memory, new_memory), end=&#39;&#39;)
                sbatch_nm(jobpath,
                          log_path)</div>


<div class="viewcode-block" id="retrieve_jobs"><a class="viewcode-back" href="../modindex.html#normative_parallel.retrieve_jobs">[docs]</a>def retrieve_jobs():
    &quot;&quot;&quot;
    A utility function to retrieve task status from the outputs of qstat.
    
    :return: a dictionary of jobs.

    &quot;&quot;&quot;
    
    output = check_output(&#39;qstat&#39;, shell=True).decode(sys.stdout.encoding)
    output = output.split(&#39;\n&#39;)
    jobs = dict()
    for line in output[2:-1]:
        (Job_ID, Job_Name, User, Wall_Time, Status, Queue) = line.split()
        jobs[Job_ID] = dict()
        jobs[Job_ID][&#39;name&#39;] = Job_Name
        jobs[Job_ID][&#39;walltime&#39;] = Wall_Time
        jobs[Job_ID][&#39;status&#39;] = Status
        
    return jobs</div>
        

<div class="viewcode-block" id="check_job_status"><a class="viewcode-back" href="../modindex.html#normative_parallel.check_job_status">[docs]</a>def check_job_status(jobs):
    &quot;&quot;&quot;
    A utility function to count the tasks with different status.
    
    :param jobs: List of job ids.
    :return: returns the number of taks athat are queued, running, completed,
    and other status.
    
    &quot;&quot;&quot;
    running_jobs = retrieve_jobs()
    
    r = 0
    c = 0
    q = 0
    u = 0
    for job in jobs:
        try:
            if running_jobs[job][&#39;status&#39;] == &#39;C&#39;:
                c += 1
            elif running_jobs[job][&#39;status&#39;] == &#39;Q&#39;:
                q += 1
            elif running_jobs[job][&#39;status&#39;] == &#39;R&#39;:
                r += 1
            else:
                u += 1
        except: # probably meanwhile the job is finished.
            c += 1 
            continue
                 
    print(&#39;Total Jobs:%d, Queued:%d, Running:%d, Completed:%d, Unknown:%d&#39; 
          %(len(jobs), q, r, c, u))
    return q,r,c,u</div>
    

<div class="viewcode-block" id="check_jobs"><a class="viewcode-back" href="../modindex.html#normative_parallel.check_jobs">[docs]</a>def check_jobs(jobs, delay=60):
    &quot;&quot;&quot;
    A utility function for chacking the status of submitted jobs.
    
    :param jobs: list of job ids.
    :param delay: the delay (in seconds) between two consequative checks, 
    defaults to 60.

    &quot;&quot;&quot;
    
    n = len(jobs)
    
    while(True):
        q,r,c,u = check_job_status(jobs)
        if c == n:
            print(&#39;All jobs are completed!&#39;)
            break
        time.sleep(delay)</div>
        

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Andre F. Marquand.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>