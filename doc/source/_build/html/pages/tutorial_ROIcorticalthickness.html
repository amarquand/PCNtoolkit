<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Bayesian Linear Regression &mdash; Predictive Clinical Neuroscience Toolkit 0.20 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../_static/pages/css/pcntoolkit_tabs.css" type="text/css" />
      <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../_static/pages/css/pcntoolkit.css" type="text/css" />
      <link rel="stylesheet" href="../_static/pages/css/pcntoolkit_nomaxwidth.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Hierarchical Bayesian Regression" href="tutorial_HBR.html" />
    <link rel="prev" title="Gaussian Process Regression" href="tutorial_CPC2020.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html">
            <img src="../_static/pcn-logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Background</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="pcntoolkit_background.html">PCNtoolkit Background</a></li>
<li class="toctree-l1"><a class="reference internal" href="pcntoolkit_background.html#intro-to-normative-modelling">Intro to normative modelling</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Function &amp; Class Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modindex.html">Module Index</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Current Events</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="updates.html">Updates</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="tutorial_CPC2020.html">Gaussian Process Regression</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Bayesian Linear Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#step-0-install-necessary-libraries-grab-data-files">Step 0: Install necessary libraries &amp; grab data files</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-1-prepare-covariate-data">Step 1: Prepare covariate data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-2-prepare-brain-data">Step 2: Prepare brain data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-3-combine-covariate-cortical-thickness-dataframes">Step 3: Combine covariate &amp; cortical thickness dataframes</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-4-format-dataframes-to-run-normative-models">Step 4: Format dataframes to run normative models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#create-train-test-split">Create train/test split</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#step-5-run-normative-model">Step 5: Run normative model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-6-interpreting-model-performance">Step 6: Interpreting model performance</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tutorial_HBR.html">Hierarchical Bayesian Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial_braincharts_fit_nm.html">Braincharts: fit model</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial_braincharts_apply_nm.html">Braincharts: apply (transfer to new data)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Useful Stuff</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="FAQs.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="citing.html">How to cite PCNtoolkit</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="acknowledgements.html">Acknowledgements</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Predictive Clinical Neuroscience Toolkit</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Bayesian Linear Regression</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/pages/tutorial_ROIcorticalthickness.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="bayesian-linear-regression">
<h1>Bayesian Linear Regression<a class="headerlink" href="#bayesian-linear-regression" title="Permalink to this headline"></a></h1>
<p>Normative Modeling Tutorial Using Multi-Site Cortical Thickness Data and Bayesian Linear Regression.</p>
<p>This notebook will prepare the data for normative modelling (assembling
data matrices from different datasets, preparing the covariates etc).</p>
<p>View on <a class="reference external" href="https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo">GitHub</a></p>
<p>Run in <a class="reference external" href="https://colab.research.google.com/github/predictive-clinical-neuroscience/PCNtoolkit-demo/blob/main/tutorials/ROI_blr_cortthick/NormativeModelTutorial.ipynb">Google Colab</a></p>
<p>Created by <a class="reference external" href="https://twitter.com/being_saige">Saige Rutherford</a></p>
<div></div><div class="section" id="step-0-install-necessary-libraries-grab-data-files">
<h2>Step 0: Install necessary libraries &amp; grab data files<a class="headerlink" href="#step-0-install-necessary-libraries-grab-data-files" title="Permalink to this headline"></a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>! git clone https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo.git
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import os
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># set this path to the git cloned PCNtoolkit-demo repository --&gt; Uncomment whichever line you need for either running on your own computer or on Google Colab.
#os.chdir(&#39;/Users/saigerutherford/repos/PCNtoolkit-demo/&#39;) # if running on your own computer, use this line (but obvi change the path)
#os.chdir(&#39;PCNtoolkit-demo/&#39;) # if running on Google Colab, use this line
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>! pip install -r requirements.txt
</pre></div>
</div>
</div>
<div class="section" id="step-1-prepare-covariate-data">
<h2>Step 1: Prepare covariate data<a class="headerlink" href="#step-1-prepare-covariate-data" title="Permalink to this headline"></a></h2>
<p>For this tutorial we will use data from the <a class="reference external" href="https://www.humanconnectome.org/study/hcp-young-adult">Human Connectome Project
Young Adult
study</a>,
<a class="reference external" href="https://www.cam-can.org/">CAMCAN</a>, and
<a class="reference external" href="https://brain-development.org/ixi-dataset/">IXI</a> to create a
multi-site dataset.</p>
<p>Our first step is to prepare and combine the covariate (age &amp; sex) data
from each site.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joypy
from sklearn.model_selection import train_test_split
from pcntoolkit.normative import estimate, evaluate
from pcntoolkit.utils import create_bspline_basis, compute_MSLL
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>hcp = pd.read_csv(&#39;data/HCP1200_age_gender.csv&#39;)
cam = pd.read_csv(&#39;data/cam_age_gender.csv&#39;)
ixi = pd.read_csv(&#39;data/IXI_age_gender.csv&#39;)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>cam_hcp = pd.merge(hcp, cam, how=&#39;outer&#39;)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>cov = pd.merge(cam_hcp, ixi, how=&#39;outer&#39;)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>sns.set(font_scale=1.5, style=&#39;darkgrid&#39;)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>sns.displot(cov, x=&quot;age&quot;, hue=&quot;site&quot;, multiple=&quot;stack&quot;, height=6)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>cov.groupby([&#39;site&#39;]).describe()
</pre></div>
</div>
</div>
<div class="section" id="step-2-prepare-brain-data">
<h2>Step 2: Prepare brain data<a class="headerlink" href="#step-2-prepare-brain-data" title="Permalink to this headline"></a></h2>
<p>Next we will format and combine the MRI data. We are using cortical
thickness maps that are created by running recon-all from Freesurfer 6.
We need to merge together the left and right hemisphere text files for
each site, and then combine the different sites into a single dataframe.
We reduce the dimensionality of our data by using ROIs from the
Desikan-Killiany atlas.</p>
<p>Here is some psuedo-code (run from a terminal in the folder that has all
subject’s recon-all output folders) that was used to extract these ROIs:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>export SUBJECTS_DIR=/path/to/study/freesurfer_data/
aparcstats2table --subject sub-* --hemi lh --meas thickness --tablefile HCP1200_aparc_lh_thickness.txt
aparcstats2table --subject sub-* --hemi rh --meas thickness --tablefile HCP1200_aparc_rh_thickness.txt
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>cam = pd.read_csv(&#39;data/CAMCAN_aparc_thickness.csv&#39;)
hcpya = pd.read_csv(&#39;data/HCP1200_aparc_thickness.csv&#39;)
ixi = pd.read_csv(&#39;data/IXI_aparc_thickness.csv&#39;)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>hcpya_cam = pd.merge(hcpya, cam, how=&#39;outer&#39;)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>brain_all = pd.merge(ixi, hcpya_cam, how=&#39;outer&#39;)
</pre></div>
</div>
<p>We also want to include the <a class="reference external" href="https://mathworld.wolfram.com/EulerCharacteristic.html">Euler
number</a> as a
covariate. So we extracted the euler number from each subject’s
recon-all output folder into a text file and we now need to format and
combine these into our brain dataframe.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>hcp_euler = pd.read_csv(&#39;data/hcp-ya_euler.csv&#39;)
cam_euler = pd.read_csv(&#39;data/cam_euler.csv&#39;)
ixi_euler = pd.read_csv(&#39;data/ixi_euler.csv&#39;)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>hcp_euler[&#39;site&#39;] = &#39;hcp&#39;
cam_euler[&#39;site&#39;] = &#39;cam&#39;
ixi_euler[&#39;site&#39;] = &#39;ixi&#39;
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>hcp_euler.replace(r&#39;^\s*$&#39;, np.nan, regex=True, inplace=True)
cam_euler.replace(r&#39;^\s*$&#39;, np.nan, regex=True, inplace=True)
ixi_euler.replace(r&#39;^\s*$&#39;, np.nan, regex=True, inplace=True)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>hcp_euler.dropna(inplace=True)
cam_euler.dropna(inplace=True)
ixi_euler.dropna(inplace=True)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>hcp_euler[&#39;rh_euler&#39;] = hcp_euler[&#39;rh_euler&#39;].astype(int)
hcp_euler[&#39;lh_euler&#39;] = hcp_euler[&#39;lh_euler&#39;].astype(int)
cam_euler[&#39;rh_euler&#39;] = cam_euler[&#39;rh_euler&#39;].astype(int)
cam_euler[&#39;lh_euler&#39;] = cam_euler[&#39;lh_euler&#39;].astype(int)
ixi_euler[&#39;rh_euler&#39;] = ixi_euler[&#39;rh_euler&#39;].astype(int)
ixi_euler[&#39;lh_euler&#39;] = ixi_euler[&#39;lh_euler&#39;].astype(int)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>hcp_cam_euler = pd.merge(hcp_euler, cam_euler, how=&#39;outer&#39;)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>df_euler = pd.merge(ixi_euler, hcp_cam_euler, how=&#39;outer&#39;)
</pre></div>
</div>
<p>Finally, we need to center the euler number for each site. The euler
number is very site-specific so in order to use the same exclusion
threshold across sites we need to center the site by subtracting the
site median from all subjects at a site. Then we will take the square
root and multiply by negative one and exclude any subjects with a square
root above 10. This choice of threshold is fairly random. If possible
all of your data should be visually inspected to verify that the data
inclusion is not too strict or too lenient.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>df_euler[&#39;avg_euler&#39;] = df_euler[[&#39;lh_euler&#39;,&#39;rh_euler&#39;]].mean(axis=1)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>df_euler.groupby(by=&#39;site&#39;).median()
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>df_euler[&#39;site_median&#39;] = df_euler[&#39;site&#39;]
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>df_euler[&#39;site_median&#39;] = df_euler[&#39;site_median&#39;].replace({&#39;hcp&#39;:-43,&#39;cam&#39;:-61,&#39;ixi&#39;:-56})
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>df_euler[&#39;avg_euler_centered&#39;] = df_euler[&#39;avg_euler&#39;] - df_euler[&#39;site_median&#39;]
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>df_euler[&#39;avg_euler_centered_neg&#39;] = df_euler[&#39;avg_euler_centered&#39;]*-1
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>df_euler[&#39;avg_euler_centered_neg_sqrt&#39;] = np.sqrt(np.absolute(df_euler[&#39;avg_euler_centered_neg&#39;]))
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>pd.set_option(&#39;display.max_rows&#39;, 500)
pd.set_option(&#39;display.max_columns&#39;, 500)
pd.set_option(&#39;display.width&#39;, 1000)
#create a color gradent function to be used in the colormap parameter
def color_gradient(x=0.0, start=(0, 0, 0), stop=(1, 1, 1)):
    r = np.interp(x, [0, 1], [start[0], stop[0]])
    g = np.interp(x, [0, 1], [start[1], stop[1]])
    b = np.interp(x, [0, 1], [start[2], stop[2]])
    return r, g, b#show the table
#plot the figure
plt.figure(dpi=380)
fig, axes = joypy.joyplot(df_euler, column=[&#39;avg_euler_centered_neg_sqrt&#39;], overlap=2.5, by=&quot;site&quot;, ylim=&#39;own&#39;, fill=True, figsize=(6,6)
                          , legend=False, xlabels=True, ylabels=True, colormap=lambda x: color_gradient(x, start=(.08, .45, .8),stop=(.8, .34, .44))
                          , alpha=0.6, linewidth=.5, linecolor=&#39;w&#39;, fade=True)
plt.title(&#39;sqrt(-Euler Number), median centered&#39;, fontsize=18, color=&#39;black&#39;, alpha=1)
plt.xlabel(&#39;sqrt(-Euler number)&#39;, fontsize=14, color=&#39;black&#39;, alpha=1)
plt.ylabel(&#39;Site&#39;, fontsize=14, color=&#39;black&#39;, alpha=1)
plt.show
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>brain = pd.merge(df_euler, brain_all, how=&#39;inner&#39;)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>len(brain)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>brain_good = brain.query(&#39;avg_euler_centered_neg_sqrt &lt; 10&#39;)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>len(brain_good)
</pre></div>
</div>
<p>We lose 63 subjects because they have a large euler number.</p>
</div>
<div class="section" id="step-3-combine-covariate-cortical-thickness-dataframes">
<h2>Step 3: Combine covariate &amp; cortical thickness dataframes<a class="headerlink" href="#step-3-combine-covariate-cortical-thickness-dataframes" title="Permalink to this headline"></a></h2>
<p>Even though the normative modeling code needs the covariate and features
(cortical thickness) in separate text files, we first need to merge them
together to make sure that we have the same subjects in each file and
that the rows (representing subjects) align.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># make sure to use how=&quot;inner&quot; so that we only include subjects that have data in both the covariate and the cortical thickness files
all_data = pd.merge(brain_good, cov, how=&#39;inner&#39;)
</pre></div>
</div>
</div>
<div class="section" id="step-4-format-dataframes-to-run-normative-models">
<h2>Step 4: Format dataframes to run normative models<a class="headerlink" href="#step-4-format-dataframes-to-run-normative-models" title="Permalink to this headline"></a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from sklearn.model_selection import train_test_split
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Remove any subjects that have NaN variables in any of the columns
all_data.dropna(subset=[&#39;lh_bankssts_thickness&#39;,
       &#39;lh_caudalanteriorcingulate_thickness&#39;,
       &#39;lh_caudalmiddlefrontal_thickness&#39;, &#39;lh_cuneus_thickness&#39;,
       &#39;lh_entorhinal_thickness&#39;, &#39;lh_fusiform_thickness&#39;,
       &#39;lh_inferiorparietal_thickness&#39;, &#39;lh_inferiortemporal_thickness&#39;,
       &#39;lh_isthmuscingulate_thickness&#39;, &#39;lh_lateraloccipital_thickness&#39;,
       &#39;lh_lateralorbitofrontal_thickness&#39;, &#39;lh_lingual_thickness&#39;,
       &#39;lh_medialorbitofrontal_thickness&#39;, &#39;lh_middletemporal_thickness&#39;,
       &#39;lh_parahippocampal_thickness&#39;, &#39;lh_paracentral_thickness&#39;,
       &#39;lh_parsopercularis_thickness&#39;, &#39;lh_parsorbitalis_thickness&#39;,
       &#39;lh_parstriangularis_thickness&#39;, &#39;lh_pericalcarine_thickness&#39;,
       &#39;lh_postcentral_thickness&#39;, &#39;lh_posteriorcingulate_thickness&#39;,
       &#39;lh_precentral_thickness&#39;, &#39;lh_precuneus_thickness&#39;,
       &#39;lh_rostralanteriorcingulate_thickness&#39;,
       &#39;lh_rostralmiddlefrontal_thickness&#39;, &#39;lh_superiorfrontal_thickness&#39;,
       &#39;lh_superiorparietal_thickness&#39;, &#39;lh_superiortemporal_thickness&#39;,
       &#39;lh_supramarginal_thickness&#39;, &#39;lh_frontalpole_thickness&#39;,
       &#39;lh_temporalpole_thickness&#39;, &#39;lh_transversetemporal_thickness&#39;,
       &#39;lh_insula_thickness&#39;, &#39;lh_MeanThickness_thickness&#39;,
       &#39;rh_bankssts_thickness&#39;, &#39;rh_caudalanteriorcingulate_thickness&#39;,
       &#39;rh_caudalmiddlefrontal_thickness&#39;, &#39;rh_cuneus_thickness&#39;,
       &#39;rh_entorhinal_thickness&#39;, &#39;rh_fusiform_thickness&#39;,
       &#39;rh_inferiorparietal_thickness&#39;, &#39;rh_inferiortemporal_thickness&#39;,
       &#39;rh_isthmuscingulate_thickness&#39;, &#39;rh_lateraloccipital_thickness&#39;,
       &#39;rh_lateralorbitofrontal_thickness&#39;, &#39;rh_lingual_thickness&#39;,
       &#39;rh_medialorbitofrontal_thickness&#39;, &#39;rh_middletemporal_thickness&#39;,
       &#39;rh_parahippocampal_thickness&#39;, &#39;rh_paracentral_thickness&#39;,
       &#39;rh_parsopercularis_thickness&#39;, &#39;rh_parsorbitalis_thickness&#39;,
       &#39;rh_parstriangularis_thickness&#39;, &#39;rh_pericalcarine_thickness&#39;,
       &#39;rh_postcentral_thickness&#39;, &#39;rh_posteriorcingulate_thickness&#39;,
       &#39;rh_precentral_thickness&#39;, &#39;rh_precuneus_thickness&#39;,
       &#39;rh_rostralanteriorcingulate_thickness&#39;,
       &#39;rh_rostralmiddlefrontal_thickness&#39;, &#39;rh_superiorfrontal_thickness&#39;,
       &#39;rh_superiorparietal_thickness&#39;, &#39;rh_superiortemporal_thickness&#39;,
       &#39;rh_supramarginal_thickness&#39;, &#39;rh_frontalpole_thickness&#39;,
       &#39;rh_temporalpole_thickness&#39;, &#39;rh_transversetemporal_thickness&#39;,
       &#39;rh_insula_thickness&#39;, &#39;rh_MeanThickness_thickness&#39;,&#39;age&#39;,&#39;sex&#39;], inplace=True)
</pre></div>
</div>
<p>Separate the covariate &amp; features into their own dataframes</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>all_data_features = all_data[[&#39;lh_bankssts_thickness&#39;,
       &#39;lh_caudalanteriorcingulate_thickness&#39;,
       &#39;lh_caudalmiddlefrontal_thickness&#39;, &#39;lh_cuneus_thickness&#39;,
       &#39;lh_entorhinal_thickness&#39;, &#39;lh_fusiform_thickness&#39;,
       &#39;lh_inferiorparietal_thickness&#39;, &#39;lh_inferiortemporal_thickness&#39;,
       &#39;lh_isthmuscingulate_thickness&#39;, &#39;lh_lateraloccipital_thickness&#39;,
       &#39;lh_lateralorbitofrontal_thickness&#39;, &#39;lh_lingual_thickness&#39;,
       &#39;lh_medialorbitofrontal_thickness&#39;, &#39;lh_middletemporal_thickness&#39;,
       &#39;lh_parahippocampal_thickness&#39;, &#39;lh_paracentral_thickness&#39;,
       &#39;lh_parsopercularis_thickness&#39;, &#39;lh_parsorbitalis_thickness&#39;,
       &#39;lh_parstriangularis_thickness&#39;, &#39;lh_pericalcarine_thickness&#39;,
       &#39;lh_postcentral_thickness&#39;, &#39;lh_posteriorcingulate_thickness&#39;,
       &#39;lh_precentral_thickness&#39;, &#39;lh_precuneus_thickness&#39;,
       &#39;lh_rostralanteriorcingulate_thickness&#39;,
       &#39;lh_rostralmiddlefrontal_thickness&#39;, &#39;lh_superiorfrontal_thickness&#39;,
       &#39;lh_superiorparietal_thickness&#39;, &#39;lh_superiortemporal_thickness&#39;,
       &#39;lh_supramarginal_thickness&#39;, &#39;lh_frontalpole_thickness&#39;,
       &#39;lh_temporalpole_thickness&#39;, &#39;lh_transversetemporal_thickness&#39;,
       &#39;lh_insula_thickness&#39;, &#39;lh_MeanThickness_thickness&#39;,
       &#39;rh_bankssts_thickness&#39;, &#39;rh_caudalanteriorcingulate_thickness&#39;,
       &#39;rh_caudalmiddlefrontal_thickness&#39;, &#39;rh_cuneus_thickness&#39;,
       &#39;rh_entorhinal_thickness&#39;, &#39;rh_fusiform_thickness&#39;,
       &#39;rh_inferiorparietal_thickness&#39;, &#39;rh_inferiortemporal_thickness&#39;,
       &#39;rh_isthmuscingulate_thickness&#39;, &#39;rh_lateraloccipital_thickness&#39;,
       &#39;rh_lateralorbitofrontal_thickness&#39;, &#39;rh_lingual_thickness&#39;,
       &#39;rh_medialorbitofrontal_thickness&#39;, &#39;rh_middletemporal_thickness&#39;,
       &#39;rh_parahippocampal_thickness&#39;, &#39;rh_paracentral_thickness&#39;,
       &#39;rh_parsopercularis_thickness&#39;, &#39;rh_parsorbitalis_thickness&#39;,
       &#39;rh_parstriangularis_thickness&#39;, &#39;rh_pericalcarine_thickness&#39;,
       &#39;rh_postcentral_thickness&#39;, &#39;rh_posteriorcingulate_thickness&#39;,
       &#39;rh_precentral_thickness&#39;, &#39;rh_precuneus_thickness&#39;,
       &#39;rh_rostralanteriorcingulate_thickness&#39;,
       &#39;rh_rostralmiddlefrontal_thickness&#39;, &#39;rh_superiorfrontal_thickness&#39;,
       &#39;rh_superiorparietal_thickness&#39;, &#39;rh_superiortemporal_thickness&#39;,
       &#39;rh_supramarginal_thickness&#39;, &#39;rh_frontalpole_thickness&#39;,
       &#39;rh_temporalpole_thickness&#39;, &#39;rh_transversetemporal_thickness&#39;,
       &#39;rh_insula_thickness&#39;, &#39;rh_MeanThickness_thickness&#39;]]
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>all_data_covariates = all_data[[&#39;age&#39;,&#39;sex&#39;,&#39;site&#39;]]
</pre></div>
</div>
<p>Right now, the sites are coded in a single column using a string. We
need to instead dummy encode the site variable so that there is a column
for each site and the columns contain binary variables (0/1). Luckily
pandas has a nice built in function, <code class="docutils literal notranslate"><span class="pre">pd.get_dummies</span></code> to help us
format the site column this way!</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>all_data_covariates = pd.get_dummies(all_data_covariates, columns=[&#39;site&#39;])
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>all_data[&#39;Average_Thickness&#39;] = all_data[[&#39;lh_MeanThickness_thickness&#39;,&#39;rh_MeanThickness_thickness&#39;]].mean(axis=1)
</pre></div>
</div>
<p>Take a sneak peak to see if there are any super obvious site effects. If
there were, we would see a large separation in the fitted regression
line for each site.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>sns.set_theme(style=&quot;darkgrid&quot;,font_scale=1.5)
c = sns.lmplot(data=all_data, x=&quot;age&quot;, y=&quot;Average_Thickness&quot;, hue=&quot;site&quot;, height=6)
plt.ylim(1.5, 3.25)
plt.xlim(15, 95)
plt.show()
</pre></div>
</div>
<div class="section" id="create-train-test-split">
<h3>Create train/test split<a class="headerlink" href="#create-train-test-split" title="Permalink to this headline"></a></h3>
<p>We will use 80% of the data for training and 20% for testing. We
stratify our train/test split using the site variable to make sure that
the train/test sets both contain data from all sites. The model wouldn’t
learn the site effects if all of the data from one site was only in the
test set.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X_train, X_test, y_train, y_test = train_test_split(all_data_covariates, all_data_features, stratify=all_data[&#39;site&#39;], test_size=0.2, random_state=42)
</pre></div>
</div>
<p>Verify that your train &amp; test arrays are the same size</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tr_cov_size = X_train.shape
tr_resp_size = y_train.shape
te_cov_size = X_test.shape
te_resp_size = y_test.shape
print(&quot;Train covariate size is: &quot;, tr_cov_size)
print(&quot;Test covariate size is: &quot;, te_cov_size)
print(&quot;Train response size is: &quot;, tr_resp_size)
print(&quot;Test response size is: &quot;, te_resp_size)
</pre></div>
</div>
<p>Save out each ROI to its own file:</p>
<p>We setup the normative model so that for each Y (brain region) we fit a
separate model. While the estimate function in the pcntoolkit can handle
having all of the Y’s in a single text file, for this tutorial we are
going to organize our Y’s so that they are each in their own text file
and directory.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>os.chdir(&#39;/Users/saigerutherford/repos/PCNToolkit-demo/&#39;)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>cd data/
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>for c in y_train.columns:
    y_train[c].to_csv(&#39;resp_tr_&#39; + c + &#39;.txt&#39;, header=False, index=False)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X_train.to_csv(&#39;cov_tr.txt&#39;, sep = &#39;\t&#39;, header=False, index = False)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y_train.to_csv(&#39;resp_tr.txt&#39;, sep = &#39;\t&#39;, header=False, index = False)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>for c in y_test.columns:
    y_test[c].to_csv(&#39;resp_te_&#39; + c + &#39;.txt&#39;, header=False, index=False)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X_test.to_csv(&#39;cov_te.txt&#39;, sep = &#39;\t&#39;, header=False, index = False)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y_test.to_csv(&#39;resp_te.txt&#39;, sep = &#39;\t&#39;, header=False, index = False)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>! if [[ ! -e data/ROI_models/ ]]; then mkdir data/ROI_models; fi
! if [[ ! -e data/covariate_files/ ]]; then mkdir data/covariate_files; fi
! if [[ ! -e data/response_files/ ]]; then mkdir data/response_files; fi
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>! for i in `cat data/roi_dir_names`; do cd data/ROI_models; mkdir ${i}; cd ../../; cp resp_tr_${i}.txt data/ROI_models/${i}/resp_tr.txt; cp resp_te_${i}.txt data/ROI_models/${i}/resp_te.txt; cp cov_tr.txt data/ROI_models/${i}/cov_tr.txt; cp cov_te.txt data/ROI_models/${i}/cov_te.txt; done
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>! mv resp_*.txt data/response_files/
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>! mv cov_t*.txt data/covariate_files/
</pre></div>
</div>
</div>
</div>
<div class="section" id="step-5-run-normative-model">
<h2>Step 5: Run normative model<a class="headerlink" href="#step-5-run-normative-model" title="Permalink to this headline"></a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># set this path to wherever your ROI_models folder is located (where you copied all of the covariate &amp; response text files to in Step 4)
data_dir = &#39;/Users/saigerutherford/repos/PCNToolkit-demo/data/ROI_models/&#39;
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create a list of all the ROIs you want to run a normative model for
roi_ids = [&#39;lh_MeanThickness_thickness&#39;,
           &#39;rh_MeanThickness_thickness&#39;,
           &#39;lh_bankssts_thickness&#39;,
           &#39;lh_caudalanteriorcingulate_thickness&#39;,
           &#39;lh_superiorfrontal_thickness&#39;,
           &#39;rh_superiorfrontal_thickness&#39;]
</pre></div>
</div>
<p>When we split the data into train and test sets, we did not reset the
index. This means that the row numbers in the train/test matrices are
still the same as before splitting the data. We will need the test set
row numbers of which subjects belong to which site in order to evaluate
per site performance metrics, so we need to reset the row numbers in the
train/test split matrices.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x_col_names = [&#39;age&#39;, &#39;sex&#39;, &#39;site_cam&#39;, &#39;site_hcp&#39;, &#39;site_ixi&#39;]
X_train = pd.read_csv(&#39;data/covariate_files/cov_tr.txt&#39;, sep=&#39;\t&#39;, header=None, names=x_col_names)
X_test = pd.read_csv(&#39;data/covariate_files/cov_te.txt&#39;, sep=&#39;\t&#39;, header=None, names=x_col_names)
y_train = pd.read_csv(&#39;data/response_files/resp_tr.txt&#39;, sep=&#39;\t&#39;, header=None)
y_test = pd.read_csv(&#39;data/response_files/resp_te.txt&#39;, sep=&#39;\t&#39;, header=None)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X_train.reset_index(drop=True, inplace=True)
X_test.reset_index(drop=True, inplace=True)
y_train.reset_index(drop=True, inplace=True)
y_test.reset_index(drop=True, inplace=True)
</pre></div>
</div>
<p>Extract site indices:</p>
<p>Get site ids so that we can evaluate the test metrics independently for
each site</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>cam_idx = X_test.index[X_test[&#39;site_cam&#39; ]== 1].to_list()
hcp_idx = X_test.index[X_test[&#39;site_hcp&#39;] == 1].to_list()
ixi_idx = X_test.index[X_test[&#39;site_ixi&#39;] == 1].to_list()

# Save the site indices into a single list
sites = [cam_idx, hcp_idx, ixi_idx]

# Create a list with sites names to use in evaluating per-site metrics
site_names = [&#39;cam&#39;, &#39;hcp&#39;, &#39;ixi&#39;]
</pre></div>
</div>
<p>Basis expansion:</p>
<p>Now, we set up a B-spline basis set that allows us to perform nonlinear
regression using a linear model. This basis is deliberately chosen to
not to be too flexible so that in can only model relatively slowly
varying trends. To increase the flexibility of the model you can change
the parameterisation (e.g. by adding knot points to the Bspline basis or
increasing the order of the interpolating polynomial).</p>
<p>Note that in the neuroimaging literature, it is more common to use a
polynomial basis expansion for this. Piecewise polynomials like
B-splines are superior because they do not introduce a global curvature.
See the reference below for further information.</p>
<p><a class="reference external" href="https://cran.r-project.org/web/packages/crs/vignettes/spline_primer.pdf">Primer on regression
splines</a></p>
<p><a class="reference external" href="https://www.sciencedirect.com/science/article/abs/pii/S1053811910000832?via%3Dihub">Reference for why polynomials are a bad
idea</a></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create a cubic B-spline basis (used for regression)
xmin = 10#16 # xmin &amp; xmax are the boundaries for ages of participants in the dataset
xmax = 95#90
B = create_bspline_basis(xmin, xmax)

# create the basis expansion for the covariates for each of the
for roi in roi_ids:
    print(&#39;Creating basis expansion for ROI:&#39;, roi)
    roi_dir = os.path.join(data_dir, roi)
    os.chdir(roi_dir)

    # create output dir
    os.makedirs(os.path.join(roi_dir,&#39;blr&#39;), exist_ok=True)

    # load train &amp; test covariate data matrices
    X_tr = np.loadtxt(os.path.join(roi_dir, &#39;cov_tr.txt&#39;))
    X_te = np.loadtxt(os.path.join(roi_dir, &#39;cov_te.txt&#39;))

    # add intercept column
    X_tr = np.concatenate((X_tr, np.ones((X_tr.shape[0],1))), axis=1)
    X_te = np.concatenate((X_te, np.ones((X_te.shape[0],1))), axis=1)
    np.savetxt(os.path.join(roi_dir, &#39;cov_int_tr.txt&#39;), X_tr)
    np.savetxt(os.path.join(roi_dir, &#39;cov_int_te.txt&#39;), X_te)

    # create Bspline basis set
    Phi = np.array([B(i) for i in X_tr[:,0]])
    Phis = np.array([B(i) for i in X_te[:,0]])
    X_tr = np.concatenate((X_tr, Phi), axis=1)
    X_te = np.concatenate((X_te, Phis), axis=1)
    np.savetxt(os.path.join(roi_dir, &#39;cov_bspline_tr.txt&#39;), X_tr)
    np.savetxt(os.path.join(roi_dir, &#39;cov_bspline_te.txt&#39;), X_te)
</pre></div>
</div>
<p>Prepare output structures:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Create pandas dataframes with header names to save out the overall and per-site model evaluation metrics
blr_metrics = pd.DataFrame(columns = [&#39;ROI&#39;, &#39;MSLL&#39;, &#39;EV&#39;, &#39;SMSE&#39;, &#39;RMSE&#39;, &#39;Rho&#39;])
blr_site_metrics = pd.DataFrame(columns = [&#39;ROI&#39;, &#39;site&#39;, &#39;y_mean&#39;, &#39;y_var&#39;, &#39;yhat_mean&#39;, &#39;yhat_var&#39;, &#39;MSLL&#39;, &#39;EV&#39;, &#39;SMSE&#39;, &#39;RMSE&#39;, &#39;Rho&#39;])
</pre></div>
</div>
<p>Estimate the normative models:</p>
<p>In this step, we estimate the normative models one at a time. In
principle we could also do this on the whole data matrix at once
(e.g. with the response variables stored in a n_subjects x
n_brain_measures numpy array). However, doing it this way gives us some
extra flexibility in that it does not require that the subjects are
exactly the same for each of the brain measures.</p>
<p>This code fragment will loop through each region of interest in the
roi_ids list (set a few code blocks above) using Bayesian linear
regression and evaluate the model on the independent test set. It will
then compute error metrics such as the explained variance, mean
standardized log loss and Pearson correlation between true and predicted
test responses separately for each scanning site.</p>
<p>We supply the estimate function with a few specific arguments that are
worthy of commenting on: * alg = ‘blr’ : specifies we should use
Bayesian linear regression * optimizer = ‘powell’ : use Powell’s
derivative-free optimization method (faster in this case than L-BFGS) *
savemodel = False : do not write out the final estimated model to disk
* saveoutput = False : return the outputs directly rather than writing
them to disk * standardize = False : Do not standardize the covariates
or response variables</p>
<p>One important consideration is whether or not to standardize. Whilst
this generally only has a minor effect on the final model accuracy, it
has implications for the interpretation of models and how they are
configured. If the covariates and responses are both standardized, the
model will return standardized coefficients. If (as in this case) the
response variables are not standardized, then the scaling both
covariates and responses will be reflected in the estimated
coefficients. Also, under the linear modelling approach employed here,
if the coefficients are unstandardized and do not have a zero mean, it
is necessary to add an intercept column to the design matrix. This is
done in the code block above.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Loop through ROIs
for roi in roi_ids:
    print(&#39;Running ROI:&#39;, roi)
    roi_dir = os.path.join(data_dir, roi)
    os.chdir(roi_dir)

    # configure the covariates to use. Change *_bspline_* to *_int_* to
    cov_file_tr = os.path.join(roi_dir, &#39;cov_bspline_tr.txt&#39;)
    cov_file_te = os.path.join(roi_dir, &#39;cov_bspline_te.txt&#39;)

    # load train &amp; test response files
    resp_file_tr = os.path.join(roi_dir, &#39;resp_tr.txt&#39;)
    resp_file_te = os.path.join(roi_dir, &#39;resp_te.txt&#39;)

    # run a basic model
    yhat_te, s2_te, nm, Z, metrics_te = estimate(cov_file_tr,
                                                 resp_file_tr,
                                                 testresp=resp_file_te,
                                                 testcov=cov_file_te,
                                                 alg = &#39;blr&#39;,
                                                 optimizer = &#39;powell&#39;,
                                                 savemodel = False,
                                                 saveoutput = False,
                                                 standardize = False)
    # display and save metrics
    print(&#39;EV=&#39;, metrics_te[&#39;EXPV&#39;][0])
    print(&#39;RHO=&#39;, metrics_te[&#39;Rho&#39;][0])
    print(&#39;MSLL=&#39;, metrics_te[&#39;MSLL&#39;][0])
    blr_metrics.loc[len(blr_metrics)] = [roi, metrics_te[&#39;MSLL&#39;][0], metrics_te[&#39;EXPV&#39;][0], metrics_te[&#39;SMSE&#39;][0],
                                         metrics_te[&#39;RMSE&#39;][0], metrics_te[&#39;Rho&#39;][0]]

    # Compute metrics per site in test set, save to pandas df
    # load true test data
    X_te = np.loadtxt(cov_file_te)
    y_te = np.loadtxt(resp_file_te)
    y_te = y_te[:, np.newaxis] # make sure it is a 2-d array

    # load training data (required to compute the MSLL)
    y_tr = np.loadtxt(resp_file_tr)
    y_tr = y_tr[:, np.newaxis]

    for num, site in enumerate(sites):
        y_mean_te_site = np.array([[np.mean(y_te[site])]])
        y_var_te_site = np.array([[np.var(y_te[site])]])
        yhat_mean_te_site = np.array([[np.mean(yhat_te[site])]])
        yhat_var_te_site = np.array([[np.var(yhat_te[site])]])

        metrics_te_site = evaluate(y_te[site], yhat_te[site], s2_te[site], y_mean_te_site, y_var_te_site)

        site_name = site_names[num]
        blr_site_metrics.loc[len(blr_site_metrics)] = [roi, site_names[num],
                                                       y_mean_te_site[0],
                                                       y_var_te_site[0],
                                                       yhat_mean_te_site[0],
                                                       yhat_var_te_site[0],
                                                       metrics_te_site[&#39;MSLL&#39;][0],
                                                       metrics_te_site[&#39;EXPV&#39;][0],
                                                       metrics_te_site[&#39;SMSE&#39;][0],
                                                       metrics_te_site[&#39;RMSE&#39;][0],
                                                       metrics_te_site[&#39;Rho&#39;][0]]
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>os.chdir(data_dir)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Save per site test set metrics variable to CSV file
blr_site_metrics.to_csv(&#39;blr_site_metrics.csv&#39;, index=False, index_label=None)
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Save overall test set metrics to CSV file
blr_metrics.to_csv(&#39;blr_metrics.csv&#39;, index=False, index_label=None)
</pre></div>
</div>
</div>
<div class="section" id="step-6-interpreting-model-performance">
<h2>Step 6: Interpreting model performance<a class="headerlink" href="#step-6-interpreting-model-performance" title="Permalink to this headline"></a></h2>
<p>Output evaluation metrics definitions</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 14%" />
<col style="width: 86%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>key value</strong></p></td>
<td><p><strong>Description</strong></p></td>
</tr>
<tr class="row-even"><td><p>yhat</p></td>
<td><p>predictive mean</p></td>
</tr>
<tr class="row-odd"><td><p>ys2</p></td>
<td><p>predictive variance</p></td>
</tr>
<tr class="row-even"><td><p>nm</p></td>
<td><p>normative model</p></td>
</tr>
<tr class="row-odd"><td><p>Z</p></td>
<td><p>deviance scores</p></td>
</tr>
<tr class="row-even"><td><p>Rho</p></td>
<td><p>Pearson correlation between true and predicted responses</p></td>
</tr>
<tr class="row-odd"><td><p>pRho</p></td>
<td><p>parametric p-value for this correlation</p></td>
</tr>
<tr class="row-even"><td><p>RMSE</p></td>
<td><p>root mean squared error between true/predicted responses</p></td>
</tr>
<tr class="row-odd"><td><p>SMSE</p></td>
<td><p>standardised mean squared error</p></td>
</tr>
<tr class="row-even"><td><p>EV</p></td>
<td><p>explained variance</p></td>
</tr>
<tr class="row-odd"><td><p>MSLL</p></td>
<td><p>mean standardized log loss <a class="reference external" href="http://www.gaussianprocess.org/gpml/chapters/RW2.pdf">See page 23</a></p></td>
</tr>
</tbody>
</table>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="tutorial_CPC2020.html" class="btn btn-neutral float-left" title="Gaussian Process Regression" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tutorial_HBR.html" class="btn btn-neutral float-right" title="Hierarchical Bayesian Regression" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Andre F. Marquand.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>