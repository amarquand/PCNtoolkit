{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "N_samples = 10000\n",
    "N_features = 2\n",
    "N_covariates = 2\n",
    "X = np.random.randn(N_samples, N_covariates)\n",
    "Y = np.random.randn(N_samples, N_features)\n",
    "sites = np.random.choice(a=3, size=N_samples, replace=True) + 1\n",
    "sex = np.random.choice(a=2, size=N_samples, replace=True) + 1\n",
    "batch_effects = np.stack([sites, sex], axis=1)\n",
    "\n",
    "# Split data into original and transfer\n",
    "original_data_mask = batch_effects[:, 0] != 3\n",
    "X_or = X[original_data_mask]\n",
    "Y_or = Y[original_data_mask]\n",
    "be_or = batch_effects[original_data_mask]\n",
    "\n",
    "transfer_data_mask = batch_effects[:, 0] == 3\n",
    "X_tr = X[transfer_data_mask]\n",
    "Y_tr = Y[transfer_data_mask]\n",
    "be_tr = batch_effects[transfer_data_mask]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_tr_or, X_ts_or, Y_tr_or, Y_ts_or, be_tr_or, be_ts_or = (\n",
    "    sklearn.model_selection.train_test_split(\n",
    "        X_or, Y_or, be_or, test_size=0.2, random_state=1, stratify=be_or[:, 1]\n",
    "    )\n",
    ")\n",
    "\n",
    "X_tr_tr, X_ts_tr, Y_tr_tr, Y_ts_tr, be_tr_tr, be_ts_tr = (\n",
    "    sklearn.model_selection.train_test_split(\n",
    "        X_tr, Y_tr, be_tr, test_size=0.2, random_state=1, stratify=be_tr[:, 1]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save to file\n",
    "tempdir = \"/Users/stijndeboer/temp\"\n",
    "os.makedirs(tempdir, exist_ok=True)\n",
    "\n",
    "# Original data\n",
    "with open(os.path.join(tempdir, \"X_tr_or.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(X_tr_or), f)\n",
    "with open(os.path.join(tempdir, \"Y_tr_or.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(Y_tr_or), f)\n",
    "with open(os.path.join(tempdir, \"be_tr_or.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(be_tr_or), f)\n",
    "with open(os.path.join(tempdir, \"X_ts_or.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(X_ts_or), f)\n",
    "with open(os.path.join(tempdir, \"Y_ts_or.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(Y_ts_or), f)\n",
    "with open(os.path.join(tempdir, \"be_ts_or.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(be_ts_or), f)\n",
    "\n",
    "# Transfer data\n",
    "with open(os.path.join(tempdir, \"X_tr_tr.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(X_tr_tr), f)\n",
    "with open(os.path.join(tempdir, \"Y_tr_tr.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(Y_tr_tr), f)\n",
    "with open(os.path.join(tempdir, \"be_tr_tr.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(be_tr_tr), f)\n",
    "with open(os.path.join(tempdir, \"X_ts_tr.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(X_ts_tr), f)\n",
    "with open(os.path.join(tempdir, \"Y_ts_tr.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(Y_ts_tr), f)\n",
    "with open(os.path.join(tempdir, \"be_ts_tr.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(pd.DataFrame(be_ts_tr), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inscaler: standardize\n",
      "outscaler: standardize\n",
      "Processing data in /Users/stijndeboer/temp/Y_tr_or.pkl\n",
      "Estimating model  1 of 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag_grad...\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [mu_slope_mu, sigma_slope_mu, offset_slope_mu, mu_intercept_mu, sigma_intercept_mu, offset_intercept_mu, mu_sigma, sigma_sigma, sigma]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221e41c9fafc490fa0b8e59cc7f8ecb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pcntoolkit as ptk\n",
    "\n",
    "ptk.normative.estimate(\n",
    "    covfile=os.path.join(tempdir, \"X_tr_or.pkl\"),\n",
    "    respfile=os.path.join(tempdir, \"Y_tr_or.pkl\"),\n",
    "    trbefile=os.path.join(tempdir, \"be_tr_or.pkl\"),\n",
    "    testcov=os.path.join(tempdir, \"X_ts_or.pkl\"),\n",
    "    testresp=os.path.join(tempdir, \"Y_ts_or.pkl\"),\n",
    "    tsbefile=os.path.join(tempdir, \"be_ts_or.pkl\"),\n",
    "    inscaler=\"standardize\",\n",
    "    outscaler=\"standardize\",\n",
    "    savemodel=True,\n",
    "    alg=\"hbr\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Using HBR transform...\n",
      "Transferring model  1 of 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag_grad...\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [mu_slope_mu, sigma_slope_mu, offset_slope_mu, mu_intercept_mu, sigma_intercept_mu, offset_intercept_mu, mu_sigma, sigma_sigma, sigma]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f89b509c627455ab13970f532d40024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 1 chain for 500 tune and 1_000 draw iterations (500 + 1_000 draws total) took 12 seconds.\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sampling: [y_like]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d734d1e5cb4467d84b1051908c23d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using HBR transform...\n",
      "Transferring model  2 of 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag_grad...\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [mu_slope_mu, sigma_slope_mu, offset_slope_mu, mu_intercept_mu, sigma_intercept_mu, offset_intercept_mu, mu_sigma, sigma_sigma, sigma]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a330680001d4ae581f8d00fbf51d32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 1 chain for 500 tune and 1_000 draw iterations (500 + 1_000 draws total) took 12 seconds.\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sampling: [y_like]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8d4134734c447483848a4bff370f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model ...\n",
      "Writing outputs ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.0110398 , -0.06328177],\n",
       "        [-0.03766446,  0.02061658],\n",
       "        [-0.03117988, -0.04809136],\n",
       "        ...,\n",
       "        [ 0.03585486, -0.07153488],\n",
       "        [-0.04672195, -0.01762015],\n",
       "        [-0.03315978, -0.01259515]]),\n",
       " array([[0.95314577, 1.00393766],\n",
       "        [0.91900985, 0.94084869],\n",
       "        [1.03862792, 0.90314728],\n",
       "        ...,\n",
       "        [1.03915003, 0.95055646],\n",
       "        [0.98424361, 1.0096423 ],\n",
       "        [1.0244577 , 1.10469841]]),\n",
       " array([[ 1.33417502, -0.20998889],\n",
       "        [-1.29727415, -0.58594985],\n",
       "        [-0.02144152, -0.09143316],\n",
       "        ...,\n",
       "        [-1.53414995,  0.04511795],\n",
       "        [-0.80707082,  0.04739652],\n",
       "        [-2.10294282, -0.14015961]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptk.normative.transfer(\n",
    "    covfile=os.path.join(tempdir, \"X_tr_tr.pkl\"),\n",
    "    respfile=os.path.join(tempdir, \"Y_tr_tr.pkl\"),\n",
    "    trbefile=os.path.join(tempdir, \"be_tr_tr.pkl\"),\n",
    "    testcov=os.path.join(tempdir, \"X_ts_tr.pkl\"),\n",
    "    testresp=os.path.join(tempdir, \"Y_ts_tr.pkl\"),\n",
    "    tsbefile=os.path.join(tempdir, \"be_ts_tr.pkl\"),\n",
    "    alg=\"hbr\",\n",
    "    inscaler=\"standardize\",\n",
    "    outscaler=\"standardize\",\n",
    "    model_path=\"/Users/stijndeboer/Projects/PCN/PCNtoolkit/tests/Models\",\n",
    "    output_path=\"/Users/stijndeboer/Projects/PCN/PCNtoolkit/tests/Models\",\n",
    "    outputsuffix=\"_transfer\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valid_voxels': array([0, 1]), 'fold_num': 1, 'mean_resp': [array([0.01032056, 0.00951302])], 'std_resp': [array([1.01259045, 1.00453476])], 'scaler_cov': [<pcntoolkit.util.utils.scaler object at 0x17f41b9e0>], 'scaler_resp': [<pcntoolkit.util.utils.scaler object at 0x17fb26390>], 'regressor': 'hbr', 'inscaler': 'standardize', 'outscaler': 'standardize', 'versions': {'Python': '3.12.0', 'pytensor': '2.26.3', 'PyMC': '5.18.2', 'PCNtoolkit': ''}}\n",
      "{'valid_voxels': array([0, 1]), 'fold_num': 1, 'mean_resp': [array([0.01032056, 0.00951302])], 'std_resp': [array([1.01259045, 1.00453476])], 'scaler_cov': <pcntoolkit.util.utils.scaler object at 0x17fd7a0c0>, 'scaler_resp': <pcntoolkit.util.utils.scaler object at 0x17f52aff0>, 'regressor': 'hbr', 'inscaler': 'standardize', 'outscaler': 'standardize', 'versions': {'Python': '3.12.0', 'pytensor': '2.26.3', 'PyMC': '5.18.2', 'PCNtoolkit': ''}}\n"
     ]
    }
   ],
   "source": [
    "# Load the original metadata\n",
    "with open(os.path.join(\"Models\", \"meta_data.md\"), \"rb\") as f:\n",
    "    meta_data = pickle.load(f)\n",
    "\n",
    "print(meta_data)\n",
    "\n",
    "# Load the transfer metadata\n",
    "with open(os.path.join(\"Models\", \"meta_data_transfer.md\"), \"rb\") as f:\n",
    "    meta_data_transfer = pickle.load(f)\n",
    "\n",
    "print(meta_data_transfer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the train data using the original scalers\n",
    "X_tr_or_scaled = meta_data[\"scaler_cov\"][0].transform(X_tr_or)\n",
    "Y_tr_or_scaled = meta_data[\"scaler_resp\"][0].transform(Y_tr_or)\n",
    "\n",
    "# Scale the combined train data using the transfer scalers\n",
    "X_all_scaled = meta_data_transfer[\"scaler_cov\"].transform(\n",
    "    np.concatenate([X_tr_or, X_tr_tr], axis=0)\n",
    ")\n",
    "Y_all_scaled = meta_data_transfer[\"scaler_resp\"].transform(\n",
    "    np.concatenate([Y_tr_or, Y_tr_tr], axis=0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "atol = 1e-3\n",
    "\n",
    "print(\n",
    "    np.allclose(\n",
    "        X_tr_or_scaled.mean(axis=0), np.zeros(X_tr_or_scaled.shape[1]), atol=atol\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    np.allclose(X_tr_or_scaled.std(axis=0), np.ones(X_tr_or_scaled.shape[1]), atol=atol)\n",
    ")\n",
    "print(\n",
    "    np.allclose(\n",
    "        Y_tr_or_scaled.mean(axis=0), np.zeros(Y_tr_or_scaled.shape[1]), atol=atol\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    np.allclose(Y_tr_or_scaled.std(axis=0), np.ones(Y_tr_or_scaled.shape[1]), atol=atol)\n",
    ")\n",
    "\n",
    "print(\n",
    "    np.allclose(X_all_scaled.mean(axis=0), np.zeros(X_all_scaled.shape[1]), atol=atol)\n",
    ")\n",
    "print(np.allclose(X_all_scaled.std(axis=0), np.ones(X_all_scaled.shape[1]), atol=atol))\n",
    "print(\n",
    "    np.allclose(Y_all_scaled.mean(axis=0), np.zeros(Y_all_scaled.shape[1]), atol=atol)\n",
    ")\n",
    "print(np.allclose(Y_all_scaled.std(axis=0), np.ones(Y_all_scaled.shape[1]), atol=atol))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcntk_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
