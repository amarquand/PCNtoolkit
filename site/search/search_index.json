{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PCNToolkit Documentation","text":""},{"location":"#installation","title":"Installation","text":"<p>You can install PCNToolkit using pip: </p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#pcntoolkit","title":"<code>pcntoolkit</code>","text":""},{"location":"api/#pcntoolkit.dataio","title":"<code>dataio</code>","text":""},{"location":"api/#pcntoolkit.dataio.basis_expansions","title":"<code>basis_expansions</code>","text":""},{"location":"api/#pcntoolkit.dataio.basis_expansions.create_bspline_basis","title":"<code>create_bspline_basis(xmin: float | int, xmax: float | int, p: int = 3, nknots: int = 5) -&gt; bspline.Bspline</code>","text":"<p>Compute a B-spline basis.</p> <p>Parameters:</p> Name Type Description Default <code>xmin</code> <code>float | int</code> <p>Lower bound of the input domain</p> required <code>xmax</code> <code>float | int</code> <p>Upper bound of the input domain</p> required <code>p</code> <code>int</code> <p>Order of the BSpline, by default 3</p> <code>3</code> <code>nknots</code> <code>int</code> <p>Number of Knots, by default 5</p> <code>5</code> <p>Returns:</p> Type Description <code>Bspline</code> <p>The BSpline basis</p> Source code in <code>pcntoolkit/dataio/basis_expansions.py</code> <pre><code>def create_bspline_basis(\n    xmin: float | int, xmax: float | int, p: int = 3, nknots: int = 5\n) -&gt; bspline.Bspline:\n    \"\"\"Compute a B-spline basis.\n\n    Parameters\n    ----------\n    xmin : float | int\n        Lower bound of the input domain\n    xmax : float | int\n        Upper bound of the input domain\n    p : int, optional\n        Order of the BSpline, by default 3\n    nknots : int, optional\n        Number of Knots, by default 5\n\n    Returns\n    -------\n    bspline.Bspline\n        The BSpline basis\n    \"\"\"\n\n    knots = np.linspace(xmin, xmax, nknots)\n    k = splinelab.augknt(knots, p)  # pad the knot vector\n    B = bspline.Bspline(k, p)\n    return B\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.basis_expansions.create_poly_basis","title":"<code>create_poly_basis(X: np.ndarray, dimpoly: int) -&gt; np.ndarray</code>","text":"<p>Creates a polynomial basis matrix for the given input matrix.</p> <p>This function takes an input matrix <code>X</code> and a degree <code>dimpoly</code>, and returns a new matrix where each column is <code>X</code> raised to the power of a degree. The degrees range from 1 to <code>dimpoly</code>. If <code>X</code> is a 1D array, it is reshaped into a 2D array with one column.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input matrix, a 2D array where each row is a sample and each column is a feature. If <code>X</code> is a 1D array, it is reshaped into a 2D array with one column.</p> required <code>dimpoly</code> <code>int</code> <p>The degree of the polynomial basis. The output matrix will have <code>dimpoly</code> times as many columns as <code>X</code>.</p> required <p>Returns:</p> Name Type Description <code>Phi</code> <code>ndarray</code> <p>The polynomial basis matrix, a 2D array where each row is a sample and each column is a feature raised to a degree. The degrees range from 1 to <code>dimpoly</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; create_poly_basis(X, 2)\narray([[ 1.,  2.,  1.,  4.],\n       [ 3.,  4.,  9., 16.],\n       [ 5.,  6., 25., 36.]])\n</code></pre> Source code in <code>pcntoolkit/dataio/basis_expansions.py</code> <pre><code>def create_poly_basis(X: np.ndarray, dimpoly: int) -&gt; np.ndarray:\n    \"\"\"\n    Creates a polynomial basis matrix for the given input matrix.\n\n    This function takes an input matrix `X` and a degree `dimpoly`, and returns a new matrix where each column is `X` raised to the power of a degree. The degrees range from 1 to `dimpoly`. If `X` is a 1D array, it is reshaped into a 2D array with one column.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        The input matrix, a 2D array where each row is a sample and each column is a feature. If `X` is a 1D array, it is reshaped into a 2D array with one column.\n    dimpoly : int\n        The degree of the polynomial basis. The output matrix will have `dimpoly` times as many columns as `X`.\n\n    Returns\n    -------\n    Phi : numpy.ndarray\n        The polynomial basis matrix, a 2D array where each row is a sample and each column is a feature raised to a degree. The degrees range from 1 to `dimpoly`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n    &gt;&gt;&gt; create_poly_basis(X, 2)\n    array([[ 1.,  2.,  1.,  4.],\n           [ 3.,  4.,  9., 16.],\n           [ 5.,  6., 25., 36.]])\n    \"\"\"\n    # TODO: Check this oneliner : Phi = np.power.outer(X.reshape(-1,1), np.arange(1,dimpoly+1)).reshape(X.shape[0], -1)\n    if len(X.shape) == 1:\n        X = X[:, np.newaxis]\n    D = X.shape[1]\n    Phi = np.zeros((X.shape[0], D * dimpoly))\n    colid = np.arange(0, D)\n    for d in range(1, dimpoly + 1):\n        Phi[:, colid] = X**d\n        colid += D\n\n    return Phi\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio","title":"<code>fileio</code>","text":""},{"location":"api/#pcntoolkit.dataio.fileio.alphanum_key","title":"<code>alphanum_key(s)</code>","text":"<p>Turn a string into a list of numbers</p> <p>Basic usage::</p> <pre><code>            alphanum_key(s)\n</code></pre> <p>:param s: string to convert</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def alphanum_key(s):\n    \"\"\"\n    Turn a string into a list of numbers\n\n    Basic usage::\n\n                    alphanum_key(s) \n\n    :param s: string to convert\n    \"\"\"\n    return [tryint(c) for c in re.split('([0-9]+)', s)]\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.create_mask","title":"<code>create_mask(data_array, mask, verbose=False)</code>","text":"<p>Create a mask from a data array or a nifti file</p> <p>Basic usage::</p> <pre><code>    create_mask(data_array, mask, verbose)\n</code></pre> <p>:param data_array: numpy array containing the data to write out :param mask: nifti image containing a mask for the image :param verbose: verbose output</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def create_mask(data_array, mask, verbose=False):\n    \"\"\"\n    Create a mask from a data array or a nifti file\n\n    Basic usage::\n\n            create_mask(data_array, mask, verbose)\n\n    :param data_array: numpy array containing the data to write out\n    :param mask: nifti image containing a mask for the image\n    :param verbose: verbose output\n    \"\"\"\n\n    # create a (volumetric) mask either from an input nifti or the nifti itself\n\n    if mask is not None:\n        if verbose:\n            print('Loading ROI mask ...')\n        maskvol = load_nifti(mask, vol=True)\n        maskvol = maskvol != 0\n    else:\n        if len(data_array.shape) &lt; 4:\n            dim = data_array.shape[0:3] + (1,)\n        else:\n            dim = data_array.shape[0:3] + (data_array.shape[3],)\n\n        if verbose:\n            print('Generating mask automatically ...')\n        if dim[3] == 1:\n            maskvol = data_array[:, :, :] != 0\n        else:\n            maskvol = data_array[:, :, :, 0] != 0\n\n    return maskvol\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.file_extension","title":"<code>file_extension(filename)</code>","text":"<p>Determine the file extension of a file (e.g. .nii.gz)</p> <p>Basic usage::</p> <pre><code>                file_extension(filename)\n</code></pre> <p>:param filename: name of the file to check</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def file_extension(filename):\n    \"\"\"\n    Determine the file extension of a file (e.g. .nii.gz)\n\n    Basic usage::\n\n                        file_extension(filename)\n\n    :param filename: name of the file to check\n    \"\"\"\n\n    # routine to get the full file extension (e.g. .nii.gz, not just .gz)\n\n    parts = filename.split(os.extsep)\n\n    if parts[-1] == 'gz':\n        if parts[-2] == 'nii' or parts[-2] == 'img' or parts[-2] == 'hdr':\n            ext = parts[-2] + '.' + parts[-1]\n        else:\n            ext = parts[-1]\n    elif parts[-1] == 'nii':\n        if parts[-2] in CIFTI_MAPPINGS:\n            ext = parts[-2] + '.' + parts[-1]\n        else:\n            ext = parts[-1]\n    else:\n        ext = parts[-1]\n\n    ext = '.' + ext\n    return ext\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.file_stem","title":"<code>file_stem(filename)</code>","text":"<p>Determine the file stem of a file (e.g. /path/to/file.nii.gz -&gt; file)</p> <p>Basic usage::</p> <pre><code>                        file_stem(filename)\n</code></pre> <p>:param filename: name of the file to check</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def file_stem(filename):\n    \"\"\"\n    Determine the file stem of a file (e.g. /path/to/file.nii.gz -&gt; file)\n\n    Basic usage::\n\n                                file_stem(filename)\n\n    :param filename: name of the file to check\n    \"\"\"\n    idx = filename.find(file_extension(filename))\n    stm = filename[0:idx]\n\n    return stm\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.file_type","title":"<code>file_type(filename)</code>","text":"<p>Determine the file type of a file</p> <p>Basic usage::</p> <pre><code>            file_type(filename)\n</code></pre> <p>:param filename: name of the file to check</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def file_type(filename):\n    \"\"\"\n    Determine the file type of a file\n\n    Basic usage::\n\n                    file_type(filename)\n\n    :param filename: name of the file to check\n    \"\"\"\n    # routine to determine filetype\n\n    if filename.endswith(('.dtseries.nii', '.dscalar.nii', '.dlabel.nii')):\n        ftype = 'cifti'\n    elif filename.endswith(('.nii.gz', '.nii', '.img', '.hdr')):\n        ftype = 'nifti'\n    elif filename.endswith(('.txt', '.csv', '.tsv', '.asc')):\n        ftype = 'text'\n    elif filename.endswith(('.pkl')):\n        ftype = 'binary'\n    else:\n        raise ValueError(\"I don't know what to do with \" + filename)\n\n    return ftype\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.load","title":"<code>load(filename, mask=None, text=False, vol=True)</code>","text":"<p>Load a numpy array from a file</p> <p>Basic usage::</p> <pre><code>            load(filename, mask, text, vol)\n</code></pre> <p>:param filename: name of the file to load :param mask: nifti image containing a mask for the image :param text: whether to write out a text file :param vol: whether to load the image as a volume</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def load(filename, mask=None, text=False, vol=True):\n    \"\"\"\n    Load a numpy array from a file\n\n    Basic usage::\n\n                    load(filename, mask, text, vol)\n\n    :param filename: name of the file to load\n    :param mask: nifti image containing a mask for the image\n    :param text: whether to write out a text file\n    :param vol: whether to load the image as a volume\n    \"\"\"\n\n    if file_type(filename) == 'cifti':\n        x = load_cifti(filename, vol=vol)\n    elif file_type(filename) == 'nifti':\n        x = load_nifti(filename, mask, vol=vol)\n    elif text or file_type(filename) == 'text':\n        x = load_ascii(filename)\n    elif file_type(filename) == 'binary':\n        x = pd.read_pickle(filename)\n        x = x.to_numpy()\n    return x\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.load_ascii","title":"<code>load_ascii(filename)</code>","text":"<p>Load an ascii file into a numpy array</p> <p>Basic usage::</p> <pre><code>    load_ascii(filename)\n</code></pre> <p>:param filename: name of the file to load</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def load_ascii(filename):\n    \"\"\"\n    Load an ascii file into a numpy array\n\n    Basic usage::\n\n            load_ascii(filename)\n\n    :param filename: name of the file to load\n    \"\"\"\n\n    # based on pandas\n    x = np.loadtxt(filename)\n    return x\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.load_cifti","title":"<code>load_cifti(filename, vol=False, mask=None, rmtmp=True)</code>","text":"<p>Load a cifti file into a numpy array </p> <p>Basic usage::</p> <pre><code>                load_cifti(filename, vol, mask, rmtmp)\n</code></pre> <p>:param filename: name of the file to load :param vol: whether to load the image as a volume :param mask: nifti image containing a mask for the image :param rmtmp: whether to remove temporary files</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def load_cifti(filename, vol=False, mask=None, rmtmp=True):\n    \"\"\"\n    Load a cifti file into a numpy array \n\n    Basic usage::\n\n                        load_cifti(filename, vol, mask, rmtmp)\n\n    :param filename: name of the file to load\n    :param vol: whether to load the image as a volume\n    :param mask: nifti image containing a mask for the image\n    :param rmtmp: whether to remove temporary files\n    \"\"\"\n    # parse the name\n    dnam, fnam = os.path.split(filename)\n    fpref = file_stem(fnam)\n    outstem = os.path.join(tempfile.gettempdir(),\n                           str(os.getpid()) + \"-\" + fpref)\n\n    # extract surface data from the cifti file\n    print(\"Extracting cifti surface data to \", outstem, '-*.func.gii', sep=\"\")\n    giinamel = outstem + '-left.func.gii'\n    giinamer = outstem + '-right.func.gii'\n    os.system('wb_command -cifti-separate ' + filename +\n              ' COLUMN -metric CORTEX_LEFT ' + giinamel)\n    os.system('wb_command -cifti-separate ' + filename +\n              ' COLUMN -metric CORTEX_RIGHT ' + giinamer)\n\n    # load the surface data\n    giil = nib.load(giinamel)\n    giir = nib.load(giinamer)\n    Nimg = len(giil.darrays)\n    Nvert = len(giil.darrays[0].data)\n    if Nimg == 1:\n        out = np.concatenate((giil.darrays[0].data, giir.darrays[0].data),\n                             axis=0)\n    else:\n        Gl = np.zeros((Nvert, Nimg))\n        Gr = np.zeros((Nvert, Nimg))\n        for i in range(0, Nimg):\n            Gl[:, i] = giil.darrays[i].data\n            Gr[:, i] = giir.darrays[i].data\n        out = np.concatenate((Gl, Gr), axis=0)\n    if rmtmp:\n        # clean up temporary files\n        os.remove(giinamel)\n        os.remove(giinamer)\n\n    if vol:\n        niiname = outstem + '-vol.nii'\n        print(\"Extracting cifti volume data to \", niiname, sep=\"\")\n        os.system('wb_command -cifti-separate ' + filename +\n                  ' COLUMN -volume-all ' + niiname)\n        vol = load_nifti(niiname, vol=True)\n        volmask = create_mask(vol)\n        out = np.concatenate((out, vol2vec(vol, volmask)), axis=0)\n        if rmtmp:\n            os.remove(niiname)\n\n    return out\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.load_nifti","title":"<code>load_nifti(datafile, mask=None, vol=False, verbose=False)</code>","text":"<p>Load a nifti file into a numpy array</p> <p>Basic usage::</p> <pre><code>            load_nifti(datafile, mask, vol, verbose)\n</code></pre> <p>:param datafile: name of the file to load :param mask: nifti image containing a mask for the image :param vol: whether to load the image as a volume :param verbose: verbose output</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def load_nifti(datafile, mask=None, vol=False, verbose=False):\n    \"\"\"\n    Load a nifti file into a numpy array\n\n    Basic usage::\n\n                    load_nifti(datafile, mask, vol, verbose)\n\n    :param datafile: name of the file to load\n    :param mask: nifti image containing a mask for the image\n    :param vol: whether to load the image as a volume\n    :param verbose: verbose output\n    \"\"\"\n\n    if verbose:\n        print('Loading nifti: ' + datafile + ' ...')\n    img = nib.load(datafile)\n    dat = img.get_data()\n\n    if mask is not None:\n        mask = load_nifti(mask, vol=True)\n\n    if not vol:\n        dat = vol2vec(dat, mask)\n\n    return dat\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.load_pd","title":"<code>load_pd(filename)</code>","text":"<p>Load a csv file into a pandas dataframe</p> <p>Basic usage::</p> <pre><code>            load_pd(filename)\n</code></pre> <p>:param filename: name of the file to load</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def load_pd(filename):\n    \"\"\"\n    Load a csv file into a pandas dataframe\n\n    Basic usage::\n\n                    load_pd(filename)\n\n    :param filename: name of the file to load\n    \"\"\"\n\n    # based on pandas\n    x = pd.read_csv(filename,\n                    sep=' ',\n                    header=None)\n    return x\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.predictive_interval","title":"<code>predictive_interval(s2_forward, cov_forward, multiplicator)</code>","text":"<p>Calculates a predictive interval for the forward model</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def predictive_interval(s2_forward,\n                        cov_forward,\n                        multiplicator):\n    \"\"\"\n    Calculates a predictive interval for the forward model\n    \"\"\"\n  # calculates a predictive interval\n\n    PI = np.zeros(len(cov_forward))\n    for i, xdot in enumerate(cov_forward):\n        s = np.sqrt(s2_forward[i])\n        PI[i] = multiplicator*s\n    return PI\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.save","title":"<code>save(data, filename, example=None, mask=None, text=False, dtype=None)</code>","text":"<p>Save a numpy array to a file</p> <p>Basic usage::</p> <pre><code>        save(data, filename, example, mask, text, dtype)\n</code></pre> <p>:param data: numpy array containing the data to write out :param filename: where to store it :param example: example file to copy the geometry from :param mask: nifti image containing a mask for the image :param text: whether to write out a text file :param dtype: data type for the output image (if different from the image)</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def save(data, filename, example=None, mask=None, text=False, dtype=None):\n    \"\"\"\n    Save a numpy array to a file\n\n    Basic usage::\n\n                save(data, filename, example, mask, text, dtype)\n\n    :param data: numpy array containing the data to write out\n    :param filename: where to store it\n    :param example: example file to copy the geometry from\n    :param mask: nifti image containing a mask for the image\n    :param text: whether to write out a text file\n    :param dtype: data type for the output image (if different from the image)\n    \"\"\"\n\n    if file_type(filename) == 'cifti':\n        save_cifti(data.T, filename, example, vol=True)\n    elif file_type(filename) == 'nifti':\n        save_nifti(data.T, filename, example, mask, dtype=dtype)\n    elif text or file_type(filename) == 'text':\n        save_ascii(data, filename)\n    elif file_type(filename) == 'binary':\n        data = pd.DataFrame(data)\n        data.to_pickle(filename, protocol=PICKLE_PROTOCOL)\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.save_ascii","title":"<code>save_ascii(data, filename)</code>","text":"<p>Save a numpy array to an ascii file</p> <p>Basic usage::</p> <pre><code>save_ascii(data, filename)\n</code></pre> <p>:param data: numpy array containing the data to write out :param filename: where to store it</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def save_ascii(data, filename):\n    \"\"\"\n    Save a numpy array to an ascii file\n\n    Basic usage::\n\n        save_ascii(data, filename)\n\n    :param data: numpy array containing the data to write out\n    :param filename: where to store it\n    \"\"\"\n    # based on pandas\n    np.savetxt(filename, data)\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.save_cifti","title":"<code>save_cifti(data, filename, example, mask=None, vol=True, volatlas=None)</code>","text":"<p>Save a cifti file from a numpy array</p> <p>Basic usage::</p> <pre><code>                    save_cifti(data, filename, example, mask, vol, volatlas)\n</code></pre> <p>:param data: numpy array containing the data to write out :param filename: where to store it :param example: example file to copy the geometry from :param mask: nifti image containing a mask for the image :param vol: whether to load the image as a volume :param volatlas: atlas to use for the volume</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def save_cifti(data, filename, example, mask=None, vol=True, volatlas=None):\n    \"\"\"\n    Save a cifti file from a numpy array\n\n    Basic usage::\n\n                            save_cifti(data, filename, example, mask, vol, volatlas)\n\n    :param data: numpy array containing the data to write out\n    :param filename: where to store it\n    :param example: example file to copy the geometry from\n    :param mask: nifti image containing a mask for the image\n    :param vol: whether to load the image as a volume\n    :param volatlas: atlas to use for the volume\n    \"\"\"\n\n    # do some sanity checks\n    if data.dtype == 'float32' or \\\n       data.dtype == 'float' or \\\n       data.dtype == 'float64':\n        data = data.astype('float32')  # force 32 bit output\n        dtype = 'NIFTI_TYPE_FLOAT32'\n    else:\n        raise ValueError('Only float data types currently handled')\n\n    if len(data.shape) == 1:\n        Nimg = 1\n        data = data[:, np.newaxis]\n    else:\n        Nimg = data.shape[1]\n\n    # get the base filename\n    dnam, fnam = os.path.split(filename)\n    fstem = file_stem(fnam)\n\n    # Split the template\n    estem = os.path.join(tempfile.gettempdir(), str(os.getpid()) + \"-\" + fstem)\n    giiexnamel = estem + '-left.func.gii'\n    giiexnamer = estem + '-right.func.gii'\n    os.system('wb_command -cifti-separate ' + example +\n              ' COLUMN -metric CORTEX_LEFT ' + giiexnamel)\n    os.system('wb_command -cifti-separate ' + example +\n              ' COLUMN -metric CORTEX_RIGHT ' + giiexnamer)\n\n    # write left hemisphere\n    giiexl = nib.load(giiexnamel)\n    Nvertl = len(giiexl.darrays[0].data)\n    garraysl = []\n    for i in range(0, Nimg):\n        garraysl.append(\n            nib.gifti.gifti.GiftiDataArray(data=data[0:Nvertl, i],\n                                           datatype=dtype))\n    giil = nib.gifti.gifti.GiftiImage(darrays=garraysl)\n    fnamel = fstem + '-left.func.gii'\n    nib.save(giil, fnamel)\n\n    # write right hemisphere\n    giiexr = nib.load(giiexnamer)\n    Nvertr = len(giiexr.darrays[0].data)\n    garraysr = []\n    for i in range(0, Nimg):\n        garraysr.append(\n            nib.gifti.gifti.GiftiDataArray(data=data[Nvertl:Nvertl+Nvertr, i],\n                                           datatype=dtype))\n    giir = nib.gifti.gifti.GiftiImage(darrays=garraysr)\n    fnamer = fstem + '-right.func.gii'\n    nib.save(giir, fnamer)\n\n    tmpfiles = [fnamer, fnamel, giiexnamel, giiexnamer]\n\n    # process volumetric data\n    if vol:\n        niiexname = estem + '-vol.nii'\n        os.system('wb_command -cifti-separate ' + example +\n                  ' COLUMN -volume-all ' + niiexname)\n        niivol = load_nifti(niiexname, vol=True)\n        if mask is None:\n            mask = create_mask(niivol)\n\n        if volatlas is None:\n            volatlas = CIFTI_VOL_ATLAS\n        fnamev = fstem + '-vol.nii'\n\n        save_nifti(data[Nvertr+Nvertl:, :], fnamev, niiexname, mask)\n        tmpfiles.extend([fnamev, niiexname])\n\n    # write cifti\n    fname = fstem + '.dtseries.nii'\n    os.system('wb_command -cifti-create-dense-timeseries ' + fname +\n              ' -volume ' + fnamev + ' ' + volatlas +\n              ' -left-metric ' + fnamel + ' -right-metric ' + fnamer)\n\n    # clean up\n    for f in tmpfiles:\n        os.remove(f)\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.save_nifti","title":"<code>save_nifti(data, filename, examplenii, mask, dtype=None)</code>","text":"<p>Write output to nifti</p> <p>Basic usage::</p> <pre><code>save_nifti(data, filename mask, dtype)\n</code></pre> <p>:param data: numpy array containing the data to write out :param filename: where to store it :param examplenii: nifti to copy the geometry and data type from :mask: nifti image containing a mask for the image :param dtype: data type for the output image (if different from the image)</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def save_nifti(data, filename, examplenii, mask, dtype=None):\n    '''\n    Write output to nifti\n\n    Basic usage::\n\n        save_nifti(data, filename mask, dtype)\n\n    :param data: numpy array containing the data to write out\n    :param filename: where to store it\n    :param examplenii: nifti to copy the geometry and data type from\n    :mask: nifti image containing a mask for the image\n    :param dtype: data type for the output image (if different from the image)\n    '''\n\n    # load mask\n    if isinstance(mask, str):\n        mask = load_nifti(mask, vol=True)\n        mask = mask != 0\n\n    # load example image\n    ex_img = nib.load(examplenii)\n    ex_img.shape\n    dim = ex_img.shape[0:3]\n    if len(data.shape) &lt; 2:\n        nvol = 1\n        data = data[:, np.newaxis]\n    else:\n        nvol = int(data.shape[1])\n\n    # write data\n    array_data = np.zeros((np.prod(dim), nvol))\n    array_data[mask.flatten(), :] = data\n    array_data = np.reshape(array_data, dim+(nvol,))\n    hdr = ex_img.header\n    if dtype is not None:\n        hdr.set_data_dtype(dtype)\n        array_data = array_data.astype(dtype)\n    array_img = nib.Nifti1Image(array_data, ex_img.affine, hdr)\n\n    nib.save(array_img, filename)\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.save_pd","title":"<code>save_pd(data, filename)</code>","text":"<p>Save a pandas dataframe to a csv file</p> <p>Basic usage::</p> <pre><code>save_pd(data, filename)\n</code></pre> <p>:param data: pandas dataframe containing the data to write out :param filename: where to store it</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def save_pd(data, filename):\n    \"\"\"\n    Save a pandas dataframe to a csv file\n\n    Basic usage::\n\n        save_pd(data, filename)\n\n    :param data: pandas dataframe containing the data to write out\n    :param filename: where to store it\n    \"\"\"\n    # based on pandas\n    data.to_csv(filename,\n                index=None,\n                header=None,\n                sep=' ',\n                na_rep='NaN')\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.sort_nicely","title":"<code>sort_nicely(l)</code>","text":"<p>Sort a list of strings in a natural way</p> <p>Basic usage::</p> <pre><code>sort_nicely(l)\n</code></pre> <p>:param l: list of strings to sort</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def sort_nicely(l):\n    \"\"\"\n    Sort a list of strings in a natural way\n\n    Basic usage::\n\n        sort_nicely(l)  \n\n    :param l: list of strings to sort\n    \"\"\"\n\n    return sorted(l, key=alphanum_key)\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.tryint","title":"<code>tryint(s)</code>","text":"<p>Try to convert a string to an integer</p> <p>Basic usage::</p> <pre><code>            tryint(s)\n</code></pre> <p>:param s: string to convert</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def tryint(s):\n    \"\"\"\n    Try to convert a string to an integer\n\n    Basic usage::\n\n                    tryint(s)\n\n    :param s: string to convert\n    \"\"\"\n\n    try:\n        return int(s)\n    except ValueError:\n        return s\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.fileio.vol2vec","title":"<code>vol2vec(dat, mask, verbose=False)</code>","text":"<p>Vectorise a 3d image</p> <p>Basic usage::</p> <pre><code>        vol2vec(dat, mask, verbose)\n</code></pre> <p>:param dat: numpy array containing the data to write out :param mask: nifti image containing a mask for the image :param verbose: verbose output</p> Source code in <code>pcntoolkit/dataio/fileio.py</code> <pre><code>def vol2vec(dat, mask, verbose=False):\n    \"\"\"\n    Vectorise a 3d image\n\n    Basic usage::\n\n                vol2vec(dat, mask, verbose)\n\n    :param dat: numpy array containing the data to write out\n    :param mask: nifti image containing a mask for the image\n    :param verbose: verbose output\n    \"\"\"\n    # vectorise a 3d image\n\n    if len(dat.shape) &lt; 4:\n        dim = dat.shape[0:3] + (1,)\n    else:\n        dim = dat.shape[0:3] + (dat.shape[3],)\n\n    # mask = create_mask(dat, mask=mask, verbose=verbose)\n    if mask is None:\n        mask = create_mask(dat, mask=mask, verbose=verbose)\n\n    # mask the image\n    maskid = np.where(mask.ravel())[0]\n    dat = np.reshape(dat, (np.prod(dim[0:3]), dim[3]))\n    dat = dat[maskid, :]\n\n    # convert to 1-d array if the file only contains one volume\n    if dim[3] == 1:\n        dat = dat.ravel()\n\n    return dat\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data","title":"<code>norm_data</code>","text":""},{"location":"api/#pcntoolkit.dataio.norm_data--norm_data-module","title":"norm_data module","text":"<p>This module provides functionalities for normalizing and converting dataset attributes into a pandas DataFrame. It is designed to handle datasets with attributes such as 'X' and 'scaled_X', facilitating data manipulation and analysis.</p> <p>print(df.head())</p>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData","title":"<code>NormData</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A class for handling normative modeling data, extending xarray.Dataset.</p> <p>NormData provides functionality for loading, preprocessing, and managing data for normative modeling. It supports various data formats and includes methods for data scaling, splitting, and visualization.</p> <p>Attributes:     X (xr.DataArray): Covariate data.     y (xr.DataArray): Response variable data.     scaled_X (xr.DataArray): Scaled version of covariate data.     scaled_y (xr.DataArray): Scaled version of response variable data.     batch_effects (xr.DataArray): Batch effect data.     Phi (xr.DataArray): Design matrix.     scaled_centiles (xr.DataArray): Scaled centile data (if applicable).     centiles (xr.DataArray): Unscaled centile data (if applicable).     zscores (xr.DataArray): Z-score data (if applicable).</p> <p>Note:     This class stores both original and scaled versions of X and y data.     While this approach offers convenience and transparency, it may     increase memory usage. Consider memory constraints when working with     large datasets.</p> <p>Example:     &gt;&gt;&gt; data = NormData.from_dataframe(\"my_data\", df, covariates,     ...                                batch_effects, response_vars)     &gt;&gt;&gt; data.scale_forward(inscalers, outscalers)     &gt;&gt;&gt; train_data, test_data = data.train_test_split([0.8, 0.2])</p> Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>class NormData(xr.Dataset):\n    \"\"\"\n    A class for handling normative modeling data, extending xarray.Dataset.\n\n    NormData provides functionality for loading, preprocessing, and managing\n    data for normative modeling. It supports various data formats and includes\n    methods for data scaling, splitting, and visualization.\n\n    Attributes:\n        X (xr.DataArray): Covariate data.\n        y (xr.DataArray): Response variable data.\n        scaled_X (xr.DataArray): Scaled version of covariate data.\n        scaled_y (xr.DataArray): Scaled version of response variable data.\n        batch_effects (xr.DataArray): Batch effect data.\n        Phi (xr.DataArray): Design matrix.\n        scaled_centiles (xr.DataArray): Scaled centile data (if applicable).\n        centiles (xr.DataArray): Unscaled centile data (if applicable).\n        zscores (xr.DataArray): Z-score data (if applicable).\n\n    Note:\n        This class stores both original and scaled versions of X and y data.\n        While this approach offers convenience and transparency, it may\n        increase memory usage. Consider memory constraints when working with\n        large datasets.\n\n    Example:\n        &gt;&gt;&gt; data = NormData.from_dataframe(\"my_data\", df, covariates,\n        ...                                batch_effects, response_vars)\n        &gt;&gt;&gt; data.scale_forward(inscalers, outscalers)\n        &gt;&gt;&gt; train_data, test_data = data.train_test_split([0.8, 0.2])\n    \"\"\"\n\n    __slots__ = (\n        \"X\",\n        \"y\",\n        \"scaled_X\",\n        \"scaled_y\",\n        \"Phi\",\n        \"scaled_centiles\",\n        \"centiles\",\n        \"zscores\",\n    )\n\n    def __init__(\n        self,\n        name: str,\n        data_vars: DataVars,\n        coords: Mapping[Any, Any],\n        attrs: Mapping[Any, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize a NormData object.\n\n        Parameters\n        ----------\n        name : str\n            The name of the dataset.\n        data_vars : DataVars\n            Data variables for the dataset.\n        coords : Mapping[Any, Any]\n            Coordinates for the dataset.\n        attrs : Mapping[Any, Any] | None, optional\n            Additional attributes for the dataset, by default None.\n        \"\"\"\n        if attrs is None:\n            attrs = {}\n        attrs[\"name\"] = name  # type: ignore\n        super().__init__(data_vars=data_vars, coords=coords, attrs=attrs)\n        self.create_batch_effects_maps()\n\n    @classmethod\n    def from_ndarrays(\n        cls,\n        name: str,\n        X: np.ndarray,\n        y: np.ndarray,\n        batch_effects: np.ndarray,\n        attrs: Mapping[str, Any] | None = None,\n    ) -&gt; NormData:\n        \"\"\"\n        Create a NormData object from numpy arrays.\n\n        Parameters\n        ----------\n        name : str\n            The name of the dataset.\n        X : np.ndarray\n            Covariate data.\n        y : np.ndarray\n            Response variable data.\n        batch_effects : np.ndarray\n            Batch effect data.\n        attrs : Mapping[str, Any] | None, optional\n            Additional attributes for the dataset, by default None.\n\n        Returns\n        -------\n        NormData\n            An instance of NormData.\n        \"\"\"\n        if X.ndim == 1:\n            X = X[:, None]\n        if y.ndim == 1:\n            y = y[:, None]\n        if batch_effects.ndim == 1:\n            batch_effects = batch_effects[:, None]\n        return cls(\n            name,\n            {\n                \"X\": ([\"datapoints\", \"covariates\"], X),\n                \"y\": ([\"datapoints\", \"response_vars\"], y),\n                \"batch_effects\": ([\"datapoints\", \"batch_effect_dims\"], batch_effects),\n            },\n            coords={\n                \"datapoints\": list(np.arange(X.shape[0])),\n                \"covariates\": [f\"covariate_{i}\" for i in np.arange(X.shape[1])],\n                \"response_vars\": [f\"response_var_{i}\" for i in np.arange(y.shape[1])],\n                \"batch_effect_dims\": [\n                    f\"batch_effect_{i}\" for i in range(batch_effects.shape[1])\n                ],\n            },\n            attrs=attrs,\n        )\n\n    def expand_basis(\n        self,\n        source_array_name: str,\n        basis_function: str,\n        basis_column: int = 0,\n        linear_component: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Expand the basis of a source array using a specified basis function.\n\n        Parameters\n        ----------\n        source_array_name : str\n            The name of the source array to expand.\n        basis_function : str\n            The basis function to use ('polynomial', 'bspline', 'linear', or 'none').\n        basis_column : int, optional\n            The column index to apply the basis function, by default 0.\n        linear_component : bool, optional\n            Whether to include a linear component, by default True.\n        **kwargs : Any\n            Additional arguments for basis functions.\n\n        Raises\n        ------\n        ValueError\n            If the source array does not exist or if required parameters are missing.\n        \"\"\"\n        all_arrays = []\n        all_dims = self.X.dims()\n        source_array: xr.DataArray = None  # type: ignore\n        if source_array_name == \"scaled_X\":\n            if self.scaled_X is None:\n                raise ValueError(\n                    \"scaled_X does not exist. Please scale the data first using the scale_forward method.\"\n                )\n            source_array = self.scaled_X\n        elif source_array_name == \"X\":\n            source_array = self.X\n\n        if basis_function == \"polynomial\":\n            expanded_basis = create_poly_basis(\n                source_array.data[:, basis_column], self.norm_conf.order\n            )\n            all_arrays.append(expanded_basis)\n            all_dims.extend([f\"poly_{i}\" for i in range(expanded_basis.shape[1])])\n        elif basis_function == \"bspline\":\n            bspline_basis = kwargs[\"bspline_basis\"]\n            if bspline_basis is None:\n                raise ValueError(\n                    \"bspline_basis is not defined. Please provide a bspline basis function.\"\n                )\n            expanded_basis = np.array(\n                [bspline_basis(c) for c in source_array.data[:, basis_column]]  # type: ignore\n            )\n            all_arrays.append(expanded_basis)\n            all_dims.extend([f\"bspline_{i}\" for i in range(expanded_basis.shape[1])])\n        elif basis_function == \"linear\" or linear_component:\n            all_arrays.append(source_array.data)\n            all_dims.extend([f\"linear_{i}\" for i in range(source_array.data.shape[1])])\n        elif basis_function in [\"none\"]:\n            # Do not expand the basis\n            pass\n\n    @classmethod\n    def from_fsl(cls, fsl_folder, config_params) -&gt; \"NormData\":  # type: ignore\n        \"\"\"\n        Load a normative dataset from a FSL file.\n\n        Parameters\n        ----------\n        fsl_folder : str\n            Path to the FSL folder.\n        config_params : dict\n            Configuration parameters for loading the dataset.\n\n        Returns\n        -------\n        NormData\n            An instance of NormData.\n        \"\"\"\n\n    @classmethod\n    def from_nifti(cls, nifti_folder, config_params) -&gt; \"NormData\":  # type: ignore\n        \"\"\"\n        Load a normative dataset from a Nifti file.\n\n        Parameters\n        ----------\n        nifti_folder : str\n            Path to the Nifti folder.\n        config_params : dict\n            Configuration parameters for loading the dataset.\n\n        Returns\n        -------\n        NormData\n            An instance of NormData.\n        \"\"\"\n\n    @classmethod\n    def from_bids(cls, bids_folder, config_params) -&gt; \"NormData\":  # type: ignore\n        \"\"\"\n        Load a normative dataset from a BIDS dataset.\n\n        Parameters\n        ----------\n        bids_folder : str\n            Path to the BIDS folder.\n        config_params : dict\n            Configuration parameters for loading the dataset.\n\n        Returns\n        -------\n        NormData\n            An instance of NormData.\n        \"\"\"\n\n    @classmethod\n    def from_xarray(cls, name: str, xarray_dataset: xr.Dataset) -&gt; NormData:\n        \"\"\"\n        Load a normative dataset from an xarray dataset.\n\n        Parameters\n        ----------\n        name : str\n            The name of the dataset.\n        xarray_dataset : xr.Dataset\n            The xarray dataset to load.\n\n        Returns\n        -------\n        NormData\n            An instance of NormData.\n        \"\"\"\n        return cls(\n            name,\n            xarray_dataset.data_vars,\n            xarray_dataset.coords,\n            xarray_dataset.attrs,\n        )\n\n    # pylint: disable=arguments-differ\n    @classmethod\n    def from_dataframe(  # type:ignore\n        cls,\n        name: str,\n        dataframe: pd.DataFrame,\n        covariates: List[str],\n        batch_effects: List[str],\n        response_vars: List[str],\n        attrs: Mapping[str, Any] | None = None,\n    ) -&gt; NormData:\n        return cls(\n            name,\n            {\n                \"X\": ([\"datapoints\", \"covariates\"], dataframe[covariates].to_numpy()),\n                \"y\": (\n                    [\"datapoints\", \"response_vars\"],\n                    dataframe[response_vars].to_numpy(),\n                ),\n                \"batch_effects\": (\n                    [\"datapoints\", \"batch_effect_dims\"],\n                    dataframe[batch_effects].to_numpy(),\n                ),\n            },\n            coords={\n                \"datapoints\": list(\n                    np.arange(dataframe[covariates].to_numpy().shape[0])\n                ),\n                \"response_vars\": response_vars,\n                \"covariates\": covariates,\n                \"batch_effect_dims\": batch_effects,\n            },\n            attrs=attrs,\n        )\n\n    # pylint: enable=arguments-differ\n\n    def create_synthetic_data(\n        self,\n        n_datapoints: int = 100,\n        range_dim: Union[int, str] = 0,\n        batch_effects_to_sample: Dict[str, List[Any]] | None = None,  # type: ignore\n    ) -&gt; NormData:\n        \"\"\"\n        Create a synthetic dataset with the same dimensions as the original dataset.\n\n        Parameters\n        ----------\n        n_datapoints : int, optional\n            The number of datapoints to create, by default 100.\n        range_dim : Union[int, str], optional\n            The covariate to use for the range of values, by default 0.\n        batch_effects_to_sample : Dict[str, List[Any]] | None, optional\n            The batch effects to sample, by default None.\n\n        Returns\n        -------\n        NormData\n            A synthetic NormData instance.\n        \"\"\"\n        ## Creates a synthetic dataset with the same dimensions as the original dataset\n\n        # The range_dim specifies for which covariate a range of values will be generated\n        if isinstance(range_dim, int):\n            range_dim = self.covariates[range_dim].to_numpy().item()\n\n        min_range = np.min(self.X.sel(covariates=range_dim))\n        max_range = np.max(self.X.sel(covariates=range_dim))\n        X = np.linspace(min_range, max_range, n_datapoints)\n\n        df = pd.DataFrame(X, columns=[range_dim])\n\n        # For all the other covariates:\n        for covariate in self.covariates.to_numpy():\n            if covariate != range_dim:\n                # Use the mean of the original dataset\n                df[covariate] = np.mean(self.X.sel(covariates=covariate))\n\n        batch_effects_to_sample = batch_effects_to_sample or {}\n\n        # For each batch_effect dimension that is not specified, sample from the first value in the batch effects map\n        for dim in self.batch_effect_dims.to_numpy():\n            if dim not in batch_effects_to_sample:\n                batch_effects_to_sample[dim] = [\n                    list(self.attrs[\"batch_effects_maps\"][dim].keys())[0]\n                ]\n\n        # # Assert that the batch effects to sample are in the batch effects maps\n        for dim, values in batch_effects_to_sample.items():\n            assert (\n                dim in self.attrs[\"batch_effects_maps\"]\n            ), f\"{dim} is not a known batch effect dimension\"\n            assert (\n                len(values) &gt; 0\n            ), f\"No values provided for batch effect dimension {dim}\"\n            for value in values:\n                assert (\n                    value in self.attrs[\"batch_effects_maps\"][dim]\n                ), f\"{value} is not a known value for batch effect dimension {dim}\"\n\n        for batch_effect_dim, values_to_sample in batch_effects_to_sample.items():\n            df[batch_effect_dim] = np.random.choice(values_to_sample, n_datapoints)\n\n        # For each response variable, sample from a normal distribution with the mean and std of the original dataset\n        for response_var in self.response_vars.to_numpy():\n            df[response_var] = np.random.normal(\n                self.y.sel(response_vars=response_var).mean(),\n                self.y.sel(response_vars=response_var).std(),\n                n_datapoints,\n            )\n\n        to_return = NormData.from_dataframe(\n            f\"{self.attrs['name']}_synthetic\",\n            df,\n            self.covariates.to_numpy(),\n            self.batch_effect_dims.to_numpy(),\n            self.response_vars.to_numpy(),\n        )\n\n        # set the batch effects maps\n        to_return.attrs[\"batch_effects_maps\"] = self.attrs[\"batch_effects_maps\"]\n\n        return to_return\n\n    def get_single_batch_effect(self) -&gt; Dict[str, List[str]]:\n        \"\"\"\n        Get a single batch effect for each dimension.\n\n        Returns\n        -------\n        Dict[str, List[str]]\n            A dictionary mapping each batch effect dimension to a list containing a single value.\n        \"\"\"\n        return {\n            k: [list(v.keys())[0]] for k, v in self.attrs[\"batch_effects_maps\"].items()\n        }\n\n    def concatenate_string_arrays(self, arrays: List[np.ndarray]) -&gt; np.ndarray:\n        \"\"\"\n        Concatenate arrays of strings.\n\n        Parameters\n        ----------\n        arrays : List[np.ndarray]\n            A list of numpy arrays containing strings.\n\n        Returns\n        -------\n        np.ndarray\n            A single concatenated numpy array of strings.\n        \"\"\"\n        return reduce(np.char.add, arrays)\n\n    def train_test_split(\n        self,\n        splits: Tuple[float, ...],\n        split_names: Tuple[str, ...] | None = None,  # type: ignore\n    ) -&gt; Tuple[NormData, ...]:\n        \"\"\"\n        Split the data into training and testing datasets.\n\n        Parameters\n        ----------\n        splits : Tuple[float, ...]\n            A tuple specifying the proportion of data for each split.\n        split_names : Tuple[str, ...] | None, optional\n            Names for the splits, by default None.\n\n        Returns\n        -------\n        Tuple[NormData, ...]\n            A tuple containing the training and testing NormData instances.\n        \"\"\"\n        batch_effects_stringified = self.concatenate_string_arrays(\n            *[\n                self.batch_effects[:, i].astype(str)\n                for i in range(self.batch_effects.shape[1])\n            ]\n        )\n        train_idx, test_idx = train_test_split(\n            np.arange(self.X.shape[0]),\n            test_size=splits[1],\n            random_state=42,\n            stratify=batch_effects_stringified,\n        )\n        split1 = self.isel(datapoints=train_idx)\n        split1.attrs = self.attrs.copy()\n        split2 = self.isel(datapoints=test_idx)\n        split2.attrs = self.attrs.copy()\n        if split_names is not None:\n            split1.attrs[\"name\"] = split_names[0]\n            split2.attrs[\"name\"] = split_names[1]\n        else:\n            split1.attrs[\"name\"] = f\"{self.attrs['name']}_train\"\n            split2.attrs[\"name\"] = f\"{self.attrs['name']}_test\"\n        return split1, split2\n\n    def kfold_split(self, k: int) -&gt; Generator[Tuple[NormData, NormData], Any, Any]:\n        \"\"\"\n        Perform k-fold splitting of the data.\n\n        Parameters\n        ----------\n        k : int\n            The number of folds.\n\n        Returns\n        -------\n        Generator[Tuple[NormData, NormData], Any, Any]\n            A generator yielding training and testing NormData instances for each fold.\n        \"\"\"\n        # Returns an iterator of (NormData, NormData) objects, split into k folds\n        stratified_kfold_split = StratifiedKFold(\n            n_splits=k, shuffle=True, random_state=42\n        )\n        batch_effects_stringified = self.concatenate_string_arrays(\n            *[\n                self.batch_effects[:, i].astype(str)\n                for i in range(self.batch_effects.shape[1])\n            ]\n        )\n        for train_idx, test_idx in stratified_kfold_split.split(\n            self.X, batch_effects_stringified\n        ):\n            split1 = self.isel(datapoints=train_idx)\n            split2 = self.isel(datapoints=test_idx)\n            yield split1, split2\n\n    def create_batch_effects_maps(self) -&gt; None:\n        \"\"\"\n        Create a mapping of batch effects to integer indices.\n        \"\"\"\n        # create a dictionary with for each column in the batch effects, a dict from value to int\n        batch_effects_maps = {}\n        for i, dim in enumerate(self.batch_effect_dims.to_numpy()):\n            batch_effects_maps[dim] = {\n                value: j for j, value in enumerate(np.unique(self.batch_effects[:, i]))\n            }\n        self.attrs[\"batch_effects_maps\"] = batch_effects_maps\n\n    def is_compatible_with(self, other: NormData) -&gt; bool:\n        \"\"\"\n        Check if the data is compatible with another dataset.\n\n        Parameters\n        ----------\n        other : NormData\n            Another NormData instance to compare with.\n\n        Returns\n        -------\n        bool\n            True if compatible, False otherwise.\n        \"\"\"\n        same_covariates: bool = np.all(self.covariates == other.covariates).astype(bool)\n        same_batch_effect_dims: bool = np.all(\n            self.batch_effect_dims == other.batch_effect_dims\n        ).astype(bool)\n        same_batch_effects_maps: bool = (\n            self.attrs[\"batch_effects_maps\"] == other.attrs[\"batch_effects_maps\"]\n        ).astype(bool)\n        return same_covariates and same_batch_effect_dims and same_batch_effects_maps\n\n    def scale_forward(\n        self, inscalers: Dict[str, Any], outscaler: Dict[str, Any]\n    ) -&gt; None:\n        \"\"\"\n        Scale the data forward using provided scalers.\n\n        Parameters\n        ----------\n        inscalers : Dict[str, Any]\n            Scalers for the covariate data.\n        outscaler : Dict[str, Any]\n            Scalers for the response variable data.\n        \"\"\"\n        # Scale X column-wise using the inscalers\n        self[\"scaled_X\"] = xr.DataArray(\n            np.zeros(self.X.shape),\n            coords=self.X.coords,\n            dims=self.X.dims,\n            attrs=self.X.attrs,\n        )\n        for covariate in self.covariates.to_numpy():\n            self.scaled_X.loc[:, covariate] = inscalers[covariate].transform(\n                self.X.sel(covariates=covariate).data\n            )\n\n        # Scale y column-wise using the outscalers\n        self[\"scaled_y\"] = xr.DataArray(\n            np.zeros(self.y.shape),\n            coords=self.y.coords,\n            dims=self.y.dims,\n            attrs=self.y.attrs,\n        )\n        for responsevar in self.response_vars.to_numpy():\n            self.scaled_y.loc[:, responsevar] = outscaler[responsevar].transform(\n                self.y.sel(response_vars=responsevar).data\n            )\n\n    def scale_backward(\n        self, inscalers: Dict[str, Any], outscalers: Dict[str, Any]\n    ) -&gt; None:\n        \"\"\"\n        Scale the data backward using provided scalers.\n\n        Parameters\n        ----------\n        inscalers : Dict[str, Any]\n            Scalers for the covariate data.\n        outscalers : Dict[str, Any]\n            Scalers for the response variable data.\n        \"\"\"\n        # Scale X column-wise using the inscalers\n        self[\"X\"] = xr.DataArray(\n            np.zeros(self.scaled_X.shape),\n            coords=self.scaled_X.coords,\n            dims=self.scaled_X.dims,\n            attrs=self.scaled_X.attrs,\n        )\n        for covariate in self.covariates.to_numpy():\n            self.X.loc[:, covariate] = inscalers[covariate].inverse_transform(\n                self.scaled_X.sel(covariates=covariate).data\n            )\n\n        # Scale y column-wise using the outscalers\n        self[\"y\"] = xr.DataArray(\n            np.zeros(self.scaled_y.shape),\n            coords=self.scaled_y.coords,\n            dims=self.scaled_y.dims,\n            attrs=self.scaled_y.attrs,\n        )\n        for responsevar in self.response_vars.to_numpy():\n            self.y.loc[:, responsevar] = outscalers[responsevar].inverse_transform(\n                self.scaled_y.sel(response_vars=responsevar).data\n            )\n\n        # Unscale the centiles, if they exist\n        if \"scaled_centiles\" in self.data_vars:\n            self[\"centiles\"] = xr.DataArray(\n                np.zeros(self.scaled_centiles.shape),\n                coords=self.scaled_centiles.coords,\n                dims=self.scaled_centiles.dims,\n                attrs=self.scaled_centiles.attrs,\n            )\n            for responsevar in self.response_vars.to_numpy():\n                self.centiles.loc[{\"response_vars\": responsevar}] = outscalers[\n                    responsevar\n                ].inverse_transform(\n                    self.scaled_centiles.sel(response_vars=responsevar).data\n                )\n\n    def select_batch_effects(self, batch_effects: Dict[str, List[str]]) -&gt; NormData:\n        \"\"\"\n        Select only the specified batch effects.\n\n        Parameters\n        ----------\n        batch_effects : Dict[str, List[str]]\n            A dictionary specifying which batch effects to select.\n\n        Returns\n        -------\n        NormData\n            A NormData instance with the selected batch effects.\n        \"\"\"\n        mask = np.zeros(self.batch_effects.shape[0], dtype=bool)\n        for key, values in batch_effects.items():\n            this_batch_effect = self.batch_effects.sel(batch_effect_dims=key)\n            for value in values:\n                mask = np.logical_or(mask, this_batch_effect == value)\n\n        to_return = self.where(mask).dropna(dim=\"datapoints\", how=\"all\")\n        if isinstance(to_return, xr.Dataset):\n            to_return = NormData.from_xarray(\n                f\"{self.attrs['name']}_selected\", to_return\n            )\n        to_return.attrs[\"batch_effects_maps\"] = self.attrs[\"batch_effects_maps\"].copy()\n        return to_return\n\n    def to_dataframe(self, dim_order: Sequence[Hashable] | None = None) -&gt; pd.DataFrame:\n        \"\"\"\n        Convert the NormData instance to a pandas DataFrame.\n\n        Parameters\n        ----------\n        dim_order : Sequence[Hashable] | None, optional\n            The order of dimensions for the DataFrame, by default None.\n\n        Returns\n        -------\n        pd.DataFrame\n            A DataFrame representation of the NormData instance.\n        \"\"\"\n        acc = []\n        x_columns = [col for col in [\"X\", \"scaled_X\"] if hasattr(self, col)]\n        y_columns = [col for col in [\"y\", \"zscores\", \"scaled_y\"] if hasattr(self, col)]\n        acc.append(\n            xr.Dataset.to_dataframe(self[x_columns], dim_order)\n            .reset_index(drop=False)\n            .pivot(index=\"datapoints\", columns=\"covariates\", values=x_columns)\n        )\n        acc.append(\n            xr.Dataset.to_dataframe(self[y_columns], dim_order)\n            .reset_index(drop=False)\n            .pivot(index=\"datapoints\", columns=\"response_vars\", values=y_columns)\n        )\n        be = (\n            xr.DataArray.to_dataframe(self.batch_effects, dim_order)\n            .reset_index(drop=False)\n            .pivot(\n                index=\"datapoints\", columns=\"batch_effect_dims\", values=\"batch_effects\"\n            )\n        )\n        be.columns = [(\"batch_effects\", col) for col in be.columns]\n        acc.append(be)\n        if hasattr(self, \"Phi\"):\n            phi = (\n                xr.DataArray.to_dataframe(self.Phi, dim_order)\n                .reset_index(drop=False)\n                .pivot(index=\"datapoints\", columns=\"basis_functions\", values=\"Phi\")\n            )\n            phi.columns = [(\"Phi\", col) for col in phi.columns]\n            acc.append(phi)\n        if hasattr(self, \"centiles\"):\n            centiles = (\n                xr.DataArray.to_dataframe(self.centiles, dim_order)\n                .reset_index(drop=False)\n                .pivot(\n                    index=\"datapoints\",\n                    columns=[\"response_vars\", \"cdf\"],\n                    values=\"centiles\",\n                )\n            )\n            centiles.columns = [(\"centiles\", col) for col in centiles.columns]\n            acc.append(centiles)\n        return pd.concat(acc, axis=1)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        Get the name of the dataset.\n\n        Returns\n        -------\n        str\n            The name of the dataset.\n        \"\"\"\n        return self.attrs[\"name\"]\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.name","title":"<code>name: str</code>  <code>property</code>","text":"<p>Get the name of the dataset.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the dataset.</p>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.__init__","title":"<code>__init__(name: str, data_vars: DataVars, coords: Mapping[Any, Any], attrs: Mapping[Any, Any] | None = None) -&gt; None</code>","text":"<p>Initialize a NormData object.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the dataset.</p> required <code>data_vars</code> <code>DataVars</code> <p>Data variables for the dataset.</p> required <code>coords</code> <code>Mapping[Any, Any]</code> <p>Coordinates for the dataset.</p> required <code>attrs</code> <code>Mapping[Any, Any] | None</code> <p>Additional attributes for the dataset, by default None.</p> <code>None</code> Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    data_vars: DataVars,\n    coords: Mapping[Any, Any],\n    attrs: Mapping[Any, Any] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize a NormData object.\n\n    Parameters\n    ----------\n    name : str\n        The name of the dataset.\n    data_vars : DataVars\n        Data variables for the dataset.\n    coords : Mapping[Any, Any]\n        Coordinates for the dataset.\n    attrs : Mapping[Any, Any] | None, optional\n        Additional attributes for the dataset, by default None.\n    \"\"\"\n    if attrs is None:\n        attrs = {}\n    attrs[\"name\"] = name  # type: ignore\n    super().__init__(data_vars=data_vars, coords=coords, attrs=attrs)\n    self.create_batch_effects_maps()\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.concatenate_string_arrays","title":"<code>concatenate_string_arrays(arrays: List[np.ndarray]) -&gt; np.ndarray</code>","text":"<p>Concatenate arrays of strings.</p> <p>Parameters:</p> Name Type Description Default <code>arrays</code> <code>List[ndarray]</code> <p>A list of numpy arrays containing strings.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A single concatenated numpy array of strings.</p> Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>def concatenate_string_arrays(self, arrays: List[np.ndarray]) -&gt; np.ndarray:\n    \"\"\"\n    Concatenate arrays of strings.\n\n    Parameters\n    ----------\n    arrays : List[np.ndarray]\n        A list of numpy arrays containing strings.\n\n    Returns\n    -------\n    np.ndarray\n        A single concatenated numpy array of strings.\n    \"\"\"\n    return reduce(np.char.add, arrays)\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.create_batch_effects_maps","title":"<code>create_batch_effects_maps() -&gt; None</code>","text":"<p>Create a mapping of batch effects to integer indices.</p> Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>def create_batch_effects_maps(self) -&gt; None:\n    \"\"\"\n    Create a mapping of batch effects to integer indices.\n    \"\"\"\n    # create a dictionary with for each column in the batch effects, a dict from value to int\n    batch_effects_maps = {}\n    for i, dim in enumerate(self.batch_effect_dims.to_numpy()):\n        batch_effects_maps[dim] = {\n            value: j for j, value in enumerate(np.unique(self.batch_effects[:, i]))\n        }\n    self.attrs[\"batch_effects_maps\"] = batch_effects_maps\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.create_synthetic_data","title":"<code>create_synthetic_data(n_datapoints: int = 100, range_dim: Union[int, str] = 0, batch_effects_to_sample: Dict[str, List[Any]] | None = None) -&gt; NormData</code>","text":"<p>Create a synthetic dataset with the same dimensions as the original dataset.</p> <p>Parameters:</p> Name Type Description Default <code>n_datapoints</code> <code>int</code> <p>The number of datapoints to create, by default 100.</p> <code>100</code> <code>range_dim</code> <code>Union[int, str]</code> <p>The covariate to use for the range of values, by default 0.</p> <code>0</code> <code>batch_effects_to_sample</code> <code>Dict[str, List[Any]] | None</code> <p>The batch effects to sample, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>NormData</code> <p>A synthetic NormData instance.</p> Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>def create_synthetic_data(\n    self,\n    n_datapoints: int = 100,\n    range_dim: Union[int, str] = 0,\n    batch_effects_to_sample: Dict[str, List[Any]] | None = None,  # type: ignore\n) -&gt; NormData:\n    \"\"\"\n    Create a synthetic dataset with the same dimensions as the original dataset.\n\n    Parameters\n    ----------\n    n_datapoints : int, optional\n        The number of datapoints to create, by default 100.\n    range_dim : Union[int, str], optional\n        The covariate to use for the range of values, by default 0.\n    batch_effects_to_sample : Dict[str, List[Any]] | None, optional\n        The batch effects to sample, by default None.\n\n    Returns\n    -------\n    NormData\n        A synthetic NormData instance.\n    \"\"\"\n    ## Creates a synthetic dataset with the same dimensions as the original dataset\n\n    # The range_dim specifies for which covariate a range of values will be generated\n    if isinstance(range_dim, int):\n        range_dim = self.covariates[range_dim].to_numpy().item()\n\n    min_range = np.min(self.X.sel(covariates=range_dim))\n    max_range = np.max(self.X.sel(covariates=range_dim))\n    X = np.linspace(min_range, max_range, n_datapoints)\n\n    df = pd.DataFrame(X, columns=[range_dim])\n\n    # For all the other covariates:\n    for covariate in self.covariates.to_numpy():\n        if covariate != range_dim:\n            # Use the mean of the original dataset\n            df[covariate] = np.mean(self.X.sel(covariates=covariate))\n\n    batch_effects_to_sample = batch_effects_to_sample or {}\n\n    # For each batch_effect dimension that is not specified, sample from the first value in the batch effects map\n    for dim in self.batch_effect_dims.to_numpy():\n        if dim not in batch_effects_to_sample:\n            batch_effects_to_sample[dim] = [\n                list(self.attrs[\"batch_effects_maps\"][dim].keys())[0]\n            ]\n\n    # # Assert that the batch effects to sample are in the batch effects maps\n    for dim, values in batch_effects_to_sample.items():\n        assert (\n            dim in self.attrs[\"batch_effects_maps\"]\n        ), f\"{dim} is not a known batch effect dimension\"\n        assert (\n            len(values) &gt; 0\n        ), f\"No values provided for batch effect dimension {dim}\"\n        for value in values:\n            assert (\n                value in self.attrs[\"batch_effects_maps\"][dim]\n            ), f\"{value} is not a known value for batch effect dimension {dim}\"\n\n    for batch_effect_dim, values_to_sample in batch_effects_to_sample.items():\n        df[batch_effect_dim] = np.random.choice(values_to_sample, n_datapoints)\n\n    # For each response variable, sample from a normal distribution with the mean and std of the original dataset\n    for response_var in self.response_vars.to_numpy():\n        df[response_var] = np.random.normal(\n            self.y.sel(response_vars=response_var).mean(),\n            self.y.sel(response_vars=response_var).std(),\n            n_datapoints,\n        )\n\n    to_return = NormData.from_dataframe(\n        f\"{self.attrs['name']}_synthetic\",\n        df,\n        self.covariates.to_numpy(),\n        self.batch_effect_dims.to_numpy(),\n        self.response_vars.to_numpy(),\n    )\n\n    # set the batch effects maps\n    to_return.attrs[\"batch_effects_maps\"] = self.attrs[\"batch_effects_maps\"]\n\n    return to_return\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.expand_basis","title":"<code>expand_basis(source_array_name: str, basis_function: str, basis_column: int = 0, linear_component: bool = True, **kwargs: Any) -&gt; None</code>","text":"<p>Expand the basis of a source array using a specified basis function.</p> <p>Parameters:</p> Name Type Description Default <code>source_array_name</code> <code>str</code> <p>The name of the source array to expand.</p> required <code>basis_function</code> <code>str</code> <p>The basis function to use ('polynomial', 'bspline', 'linear', or 'none').</p> required <code>basis_column</code> <code>int</code> <p>The column index to apply the basis function, by default 0.</p> <code>0</code> <code>linear_component</code> <code>bool</code> <p>Whether to include a linear component, by default True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for basis functions.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the source array does not exist or if required parameters are missing.</p> Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>def expand_basis(\n    self,\n    source_array_name: str,\n    basis_function: str,\n    basis_column: int = 0,\n    linear_component: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Expand the basis of a source array using a specified basis function.\n\n    Parameters\n    ----------\n    source_array_name : str\n        The name of the source array to expand.\n    basis_function : str\n        The basis function to use ('polynomial', 'bspline', 'linear', or 'none').\n    basis_column : int, optional\n        The column index to apply the basis function, by default 0.\n    linear_component : bool, optional\n        Whether to include a linear component, by default True.\n    **kwargs : Any\n        Additional arguments for basis functions.\n\n    Raises\n    ------\n    ValueError\n        If the source array does not exist or if required parameters are missing.\n    \"\"\"\n    all_arrays = []\n    all_dims = self.X.dims()\n    source_array: xr.DataArray = None  # type: ignore\n    if source_array_name == \"scaled_X\":\n        if self.scaled_X is None:\n            raise ValueError(\n                \"scaled_X does not exist. Please scale the data first using the scale_forward method.\"\n            )\n        source_array = self.scaled_X\n    elif source_array_name == \"X\":\n        source_array = self.X\n\n    if basis_function == \"polynomial\":\n        expanded_basis = create_poly_basis(\n            source_array.data[:, basis_column], self.norm_conf.order\n        )\n        all_arrays.append(expanded_basis)\n        all_dims.extend([f\"poly_{i}\" for i in range(expanded_basis.shape[1])])\n    elif basis_function == \"bspline\":\n        bspline_basis = kwargs[\"bspline_basis\"]\n        if bspline_basis is None:\n            raise ValueError(\n                \"bspline_basis is not defined. Please provide a bspline basis function.\"\n            )\n        expanded_basis = np.array(\n            [bspline_basis(c) for c in source_array.data[:, basis_column]]  # type: ignore\n        )\n        all_arrays.append(expanded_basis)\n        all_dims.extend([f\"bspline_{i}\" for i in range(expanded_basis.shape[1])])\n    elif basis_function == \"linear\" or linear_component:\n        all_arrays.append(source_array.data)\n        all_dims.extend([f\"linear_{i}\" for i in range(source_array.data.shape[1])])\n    elif basis_function in [\"none\"]:\n        # Do not expand the basis\n        pass\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.from_bids","title":"<code>from_bids(bids_folder, config_params) -&gt; 'NormData'</code>  <code>classmethod</code>","text":"<p>Load a normative dataset from a BIDS dataset.</p> <p>Parameters:</p> Name Type Description Default <code>bids_folder</code> <code>str</code> <p>Path to the BIDS folder.</p> required <code>config_params</code> <code>dict</code> <p>Configuration parameters for loading the dataset.</p> required <p>Returns:</p> Type Description <code>NormData</code> <p>An instance of NormData.</p> Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>@classmethod\ndef from_bids(cls, bids_folder, config_params) -&gt; \"NormData\":  # type: ignore\n    \"\"\"\n    Load a normative dataset from a BIDS dataset.\n\n    Parameters\n    ----------\n    bids_folder : str\n        Path to the BIDS folder.\n    config_params : dict\n        Configuration parameters for loading the dataset.\n\n    Returns\n    -------\n    NormData\n        An instance of NormData.\n    \"\"\"\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.from_fsl","title":"<code>from_fsl(fsl_folder, config_params) -&gt; 'NormData'</code>  <code>classmethod</code>","text":"<p>Load a normative dataset from a FSL file.</p> <p>Parameters:</p> Name Type Description Default <code>fsl_folder</code> <code>str</code> <p>Path to the FSL folder.</p> required <code>config_params</code> <code>dict</code> <p>Configuration parameters for loading the dataset.</p> required <p>Returns:</p> Type Description <code>NormData</code> <p>An instance of NormData.</p> Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>@classmethod\ndef from_fsl(cls, fsl_folder, config_params) -&gt; \"NormData\":  # type: ignore\n    \"\"\"\n    Load a normative dataset from a FSL file.\n\n    Parameters\n    ----------\n    fsl_folder : str\n        Path to the FSL folder.\n    config_params : dict\n        Configuration parameters for loading the dataset.\n\n    Returns\n    -------\n    NormData\n        An instance of NormData.\n    \"\"\"\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.from_ndarrays","title":"<code>from_ndarrays(name: str, X: np.ndarray, y: np.ndarray, batch_effects: np.ndarray, attrs: Mapping[str, Any] | None = None) -&gt; NormData</code>  <code>classmethod</code>","text":"<p>Create a NormData object from numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the dataset.</p> required <code>X</code> <code>ndarray</code> <p>Covariate data.</p> required <code>y</code> <code>ndarray</code> <p>Response variable data.</p> required <code>batch_effects</code> <code>ndarray</code> <p>Batch effect data.</p> required <code>attrs</code> <code>Mapping[str, Any] | None</code> <p>Additional attributes for the dataset, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>NormData</code> <p>An instance of NormData.</p> Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>@classmethod\ndef from_ndarrays(\n    cls,\n    name: str,\n    X: np.ndarray,\n    y: np.ndarray,\n    batch_effects: np.ndarray,\n    attrs: Mapping[str, Any] | None = None,\n) -&gt; NormData:\n    \"\"\"\n    Create a NormData object from numpy arrays.\n\n    Parameters\n    ----------\n    name : str\n        The name of the dataset.\n    X : np.ndarray\n        Covariate data.\n    y : np.ndarray\n        Response variable data.\n    batch_effects : np.ndarray\n        Batch effect data.\n    attrs : Mapping[str, Any] | None, optional\n        Additional attributes for the dataset, by default None.\n\n    Returns\n    -------\n    NormData\n        An instance of NormData.\n    \"\"\"\n    if X.ndim == 1:\n        X = X[:, None]\n    if y.ndim == 1:\n        y = y[:, None]\n    if batch_effects.ndim == 1:\n        batch_effects = batch_effects[:, None]\n    return cls(\n        name,\n        {\n            \"X\": ([\"datapoints\", \"covariates\"], X),\n            \"y\": ([\"datapoints\", \"response_vars\"], y),\n            \"batch_effects\": ([\"datapoints\", \"batch_effect_dims\"], batch_effects),\n        },\n        coords={\n            \"datapoints\": list(np.arange(X.shape[0])),\n            \"covariates\": [f\"covariate_{i}\" for i in np.arange(X.shape[1])],\n            \"response_vars\": [f\"response_var_{i}\" for i in np.arange(y.shape[1])],\n            \"batch_effect_dims\": [\n                f\"batch_effect_{i}\" for i in range(batch_effects.shape[1])\n            ],\n        },\n        attrs=attrs,\n    )\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.from_nifti","title":"<code>from_nifti(nifti_folder, config_params) -&gt; 'NormData'</code>  <code>classmethod</code>","text":"<p>Load a normative dataset from a Nifti file.</p> <p>Parameters:</p> Name Type Description Default <code>nifti_folder</code> <code>str</code> <p>Path to the Nifti folder.</p> required <code>config_params</code> <code>dict</code> <p>Configuration parameters for loading the dataset.</p> required <p>Returns:</p> Type Description <code>NormData</code> <p>An instance of NormData.</p> Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>@classmethod\ndef from_nifti(cls, nifti_folder, config_params) -&gt; \"NormData\":  # type: ignore\n    \"\"\"\n    Load a normative dataset from a Nifti file.\n\n    Parameters\n    ----------\n    nifti_folder : str\n        Path to the Nifti folder.\n    config_params : dict\n        Configuration parameters for loading the dataset.\n\n    Returns\n    -------\n    NormData\n        An instance of NormData.\n    \"\"\"\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.from_xarray","title":"<code>from_xarray(name: str, xarray_dataset: xr.Dataset) -&gt; NormData</code>  <code>classmethod</code>","text":"<p>Load a normative dataset from an xarray dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the dataset.</p> required <code>xarray_dataset</code> <code>Dataset</code> <p>The xarray dataset to load.</p> required <p>Returns:</p> Type Description <code>NormData</code> <p>An instance of NormData.</p> Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>@classmethod\ndef from_xarray(cls, name: str, xarray_dataset: xr.Dataset) -&gt; NormData:\n    \"\"\"\n    Load a normative dataset from an xarray dataset.\n\n    Parameters\n    ----------\n    name : str\n        The name of the dataset.\n    xarray_dataset : xr.Dataset\n        The xarray dataset to load.\n\n    Returns\n    -------\n    NormData\n        An instance of NormData.\n    \"\"\"\n    return cls(\n        name,\n        xarray_dataset.data_vars,\n        xarray_dataset.coords,\n        xarray_dataset.attrs,\n    )\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.get_single_batch_effect","title":"<code>get_single_batch_effect() -&gt; Dict[str, List[str]]</code>","text":"<p>Get a single batch effect for each dimension.</p> <p>Returns:</p> Type Description <code>Dict[str, List[str]]</code> <p>A dictionary mapping each batch effect dimension to a list containing a single value.</p> Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>def get_single_batch_effect(self) -&gt; Dict[str, List[str]]:\n    \"\"\"\n    Get a single batch effect for each dimension.\n\n    Returns\n    -------\n    Dict[str, List[str]]\n        A dictionary mapping each batch effect dimension to a list containing a single value.\n    \"\"\"\n    return {\n        k: [list(v.keys())[0]] for k, v in self.attrs[\"batch_effects_maps\"].items()\n    }\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.is_compatible_with","title":"<code>is_compatible_with(other: NormData) -&gt; bool</code>","text":"<p>Check if the data is compatible with another dataset.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>NormData</code> <p>Another NormData instance to compare with.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if compatible, False otherwise.</p> Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>def is_compatible_with(self, other: NormData) -&gt; bool:\n    \"\"\"\n    Check if the data is compatible with another dataset.\n\n    Parameters\n    ----------\n    other : NormData\n        Another NormData instance to compare with.\n\n    Returns\n    -------\n    bool\n        True if compatible, False otherwise.\n    \"\"\"\n    same_covariates: bool = np.all(self.covariates == other.covariates).astype(bool)\n    same_batch_effect_dims: bool = np.all(\n        self.batch_effect_dims == other.batch_effect_dims\n    ).astype(bool)\n    same_batch_effects_maps: bool = (\n        self.attrs[\"batch_effects_maps\"] == other.attrs[\"batch_effects_maps\"]\n    ).astype(bool)\n    return same_covariates and same_batch_effect_dims and same_batch_effects_maps\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.kfold_split","title":"<code>kfold_split(k: int) -&gt; Generator[Tuple[NormData, NormData], Any, Any]</code>","text":"<p>Perform k-fold splitting of the data.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The number of folds.</p> required <p>Returns:</p> Type Description <code>Generator[Tuple[NormData, NormData], Any, Any]</code> <p>A generator yielding training and testing NormData instances for each fold.</p> Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>def kfold_split(self, k: int) -&gt; Generator[Tuple[NormData, NormData], Any, Any]:\n    \"\"\"\n    Perform k-fold splitting of the data.\n\n    Parameters\n    ----------\n    k : int\n        The number of folds.\n\n    Returns\n    -------\n    Generator[Tuple[NormData, NormData], Any, Any]\n        A generator yielding training and testing NormData instances for each fold.\n    \"\"\"\n    # Returns an iterator of (NormData, NormData) objects, split into k folds\n    stratified_kfold_split = StratifiedKFold(\n        n_splits=k, shuffle=True, random_state=42\n    )\n    batch_effects_stringified = self.concatenate_string_arrays(\n        *[\n            self.batch_effects[:, i].astype(str)\n            for i in range(self.batch_effects.shape[1])\n        ]\n    )\n    for train_idx, test_idx in stratified_kfold_split.split(\n        self.X, batch_effects_stringified\n    ):\n        split1 = self.isel(datapoints=train_idx)\n        split2 = self.isel(datapoints=test_idx)\n        yield split1, split2\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.scale_backward","title":"<code>scale_backward(inscalers: Dict[str, Any], outscalers: Dict[str, Any]) -&gt; None</code>","text":"<p>Scale the data backward using provided scalers.</p> <p>Parameters:</p> Name Type Description Default <code>inscalers</code> <code>Dict[str, Any]</code> <p>Scalers for the covariate data.</p> required <code>outscalers</code> <code>Dict[str, Any]</code> <p>Scalers for the response variable data.</p> required Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>def scale_backward(\n    self, inscalers: Dict[str, Any], outscalers: Dict[str, Any]\n) -&gt; None:\n    \"\"\"\n    Scale the data backward using provided scalers.\n\n    Parameters\n    ----------\n    inscalers : Dict[str, Any]\n        Scalers for the covariate data.\n    outscalers : Dict[str, Any]\n        Scalers for the response variable data.\n    \"\"\"\n    # Scale X column-wise using the inscalers\n    self[\"X\"] = xr.DataArray(\n        np.zeros(self.scaled_X.shape),\n        coords=self.scaled_X.coords,\n        dims=self.scaled_X.dims,\n        attrs=self.scaled_X.attrs,\n    )\n    for covariate in self.covariates.to_numpy():\n        self.X.loc[:, covariate] = inscalers[covariate].inverse_transform(\n            self.scaled_X.sel(covariates=covariate).data\n        )\n\n    # Scale y column-wise using the outscalers\n    self[\"y\"] = xr.DataArray(\n        np.zeros(self.scaled_y.shape),\n        coords=self.scaled_y.coords,\n        dims=self.scaled_y.dims,\n        attrs=self.scaled_y.attrs,\n    )\n    for responsevar in self.response_vars.to_numpy():\n        self.y.loc[:, responsevar] = outscalers[responsevar].inverse_transform(\n            self.scaled_y.sel(response_vars=responsevar).data\n        )\n\n    # Unscale the centiles, if they exist\n    if \"scaled_centiles\" in self.data_vars:\n        self[\"centiles\"] = xr.DataArray(\n            np.zeros(self.scaled_centiles.shape),\n            coords=self.scaled_centiles.coords,\n            dims=self.scaled_centiles.dims,\n            attrs=self.scaled_centiles.attrs,\n        )\n        for responsevar in self.response_vars.to_numpy():\n            self.centiles.loc[{\"response_vars\": responsevar}] = outscalers[\n                responsevar\n            ].inverse_transform(\n                self.scaled_centiles.sel(response_vars=responsevar).data\n            )\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.scale_forward","title":"<code>scale_forward(inscalers: Dict[str, Any], outscaler: Dict[str, Any]) -&gt; None</code>","text":"<p>Scale the data forward using provided scalers.</p> <p>Parameters:</p> Name Type Description Default <code>inscalers</code> <code>Dict[str, Any]</code> <p>Scalers for the covariate data.</p> required <code>outscaler</code> <code>Dict[str, Any]</code> <p>Scalers for the response variable data.</p> required Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>def scale_forward(\n    self, inscalers: Dict[str, Any], outscaler: Dict[str, Any]\n) -&gt; None:\n    \"\"\"\n    Scale the data forward using provided scalers.\n\n    Parameters\n    ----------\n    inscalers : Dict[str, Any]\n        Scalers for the covariate data.\n    outscaler : Dict[str, Any]\n        Scalers for the response variable data.\n    \"\"\"\n    # Scale X column-wise using the inscalers\n    self[\"scaled_X\"] = xr.DataArray(\n        np.zeros(self.X.shape),\n        coords=self.X.coords,\n        dims=self.X.dims,\n        attrs=self.X.attrs,\n    )\n    for covariate in self.covariates.to_numpy():\n        self.scaled_X.loc[:, covariate] = inscalers[covariate].transform(\n            self.X.sel(covariates=covariate).data\n        )\n\n    # Scale y column-wise using the outscalers\n    self[\"scaled_y\"] = xr.DataArray(\n        np.zeros(self.y.shape),\n        coords=self.y.coords,\n        dims=self.y.dims,\n        attrs=self.y.attrs,\n    )\n    for responsevar in self.response_vars.to_numpy():\n        self.scaled_y.loc[:, responsevar] = outscaler[responsevar].transform(\n            self.y.sel(response_vars=responsevar).data\n        )\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.select_batch_effects","title":"<code>select_batch_effects(batch_effects: Dict[str, List[str]]) -&gt; NormData</code>","text":"<p>Select only the specified batch effects.</p> <p>Parameters:</p> Name Type Description Default <code>batch_effects</code> <code>Dict[str, List[str]]</code> <p>A dictionary specifying which batch effects to select.</p> required <p>Returns:</p> Type Description <code>NormData</code> <p>A NormData instance with the selected batch effects.</p> Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>def select_batch_effects(self, batch_effects: Dict[str, List[str]]) -&gt; NormData:\n    \"\"\"\n    Select only the specified batch effects.\n\n    Parameters\n    ----------\n    batch_effects : Dict[str, List[str]]\n        A dictionary specifying which batch effects to select.\n\n    Returns\n    -------\n    NormData\n        A NormData instance with the selected batch effects.\n    \"\"\"\n    mask = np.zeros(self.batch_effects.shape[0], dtype=bool)\n    for key, values in batch_effects.items():\n        this_batch_effect = self.batch_effects.sel(batch_effect_dims=key)\n        for value in values:\n            mask = np.logical_or(mask, this_batch_effect == value)\n\n    to_return = self.where(mask).dropna(dim=\"datapoints\", how=\"all\")\n    if isinstance(to_return, xr.Dataset):\n        to_return = NormData.from_xarray(\n            f\"{self.attrs['name']}_selected\", to_return\n        )\n    to_return.attrs[\"batch_effects_maps\"] = self.attrs[\"batch_effects_maps\"].copy()\n    return to_return\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.to_dataframe","title":"<code>to_dataframe(dim_order: Sequence[Hashable] | None = None) -&gt; pd.DataFrame</code>","text":"<p>Convert the NormData instance to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>dim_order</code> <code>Sequence[Hashable] | None</code> <p>The order of dimensions for the DataFrame, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame representation of the NormData instance.</p> Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>def to_dataframe(self, dim_order: Sequence[Hashable] | None = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert the NormData instance to a pandas DataFrame.\n\n    Parameters\n    ----------\n    dim_order : Sequence[Hashable] | None, optional\n        The order of dimensions for the DataFrame, by default None.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame representation of the NormData instance.\n    \"\"\"\n    acc = []\n    x_columns = [col for col in [\"X\", \"scaled_X\"] if hasattr(self, col)]\n    y_columns = [col for col in [\"y\", \"zscores\", \"scaled_y\"] if hasattr(self, col)]\n    acc.append(\n        xr.Dataset.to_dataframe(self[x_columns], dim_order)\n        .reset_index(drop=False)\n        .pivot(index=\"datapoints\", columns=\"covariates\", values=x_columns)\n    )\n    acc.append(\n        xr.Dataset.to_dataframe(self[y_columns], dim_order)\n        .reset_index(drop=False)\n        .pivot(index=\"datapoints\", columns=\"response_vars\", values=y_columns)\n    )\n    be = (\n        xr.DataArray.to_dataframe(self.batch_effects, dim_order)\n        .reset_index(drop=False)\n        .pivot(\n            index=\"datapoints\", columns=\"batch_effect_dims\", values=\"batch_effects\"\n        )\n    )\n    be.columns = [(\"batch_effects\", col) for col in be.columns]\n    acc.append(be)\n    if hasattr(self, \"Phi\"):\n        phi = (\n            xr.DataArray.to_dataframe(self.Phi, dim_order)\n            .reset_index(drop=False)\n            .pivot(index=\"datapoints\", columns=\"basis_functions\", values=\"Phi\")\n        )\n        phi.columns = [(\"Phi\", col) for col in phi.columns]\n        acc.append(phi)\n    if hasattr(self, \"centiles\"):\n        centiles = (\n            xr.DataArray.to_dataframe(self.centiles, dim_order)\n            .reset_index(drop=False)\n            .pivot(\n                index=\"datapoints\",\n                columns=[\"response_vars\", \"cdf\"],\n                values=\"centiles\",\n            )\n        )\n        centiles.columns = [(\"centiles\", col) for col in centiles.columns]\n        acc.append(centiles)\n    return pd.concat(acc, axis=1)\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.norm_data.NormData.train_test_split","title":"<code>train_test_split(splits: Tuple[float, ...], split_names: Tuple[str, ...] | None = None) -&gt; Tuple[NormData, ...]</code>","text":"<p>Split the data into training and testing datasets.</p> <p>Parameters:</p> Name Type Description Default <code>splits</code> <code>Tuple[float, ...]</code> <p>A tuple specifying the proportion of data for each split.</p> required <code>split_names</code> <code>Tuple[str, ...] | None</code> <p>Names for the splits, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[NormData, ...]</code> <p>A tuple containing the training and testing NormData instances.</p> Source code in <code>pcntoolkit/dataio/norm_data.py</code> <pre><code>def train_test_split(\n    self,\n    splits: Tuple[float, ...],\n    split_names: Tuple[str, ...] | None = None,  # type: ignore\n) -&gt; Tuple[NormData, ...]:\n    \"\"\"\n    Split the data into training and testing datasets.\n\n    Parameters\n    ----------\n    splits : Tuple[float, ...]\n        A tuple specifying the proportion of data for each split.\n    split_names : Tuple[str, ...] | None, optional\n        Names for the splits, by default None.\n\n    Returns\n    -------\n    Tuple[NormData, ...]\n        A tuple containing the training and testing NormData instances.\n    \"\"\"\n    batch_effects_stringified = self.concatenate_string_arrays(\n        *[\n            self.batch_effects[:, i].astype(str)\n            for i in range(self.batch_effects.shape[1])\n        ]\n    )\n    train_idx, test_idx = train_test_split(\n        np.arange(self.X.shape[0]),\n        test_size=splits[1],\n        random_state=42,\n        stratify=batch_effects_stringified,\n    )\n    split1 = self.isel(datapoints=train_idx)\n    split1.attrs = self.attrs.copy()\n    split2 = self.isel(datapoints=test_idx)\n    split2.attrs = self.attrs.copy()\n    if split_names is not None:\n        split1.attrs[\"name\"] = split_names[0]\n        split2.attrs[\"name\"] = split_names[1]\n    else:\n        split1.attrs[\"name\"] = f\"{self.attrs['name']}_train\"\n        split2.attrs[\"name\"] = f\"{self.attrs['name']}_test\"\n    return split1, split2\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.scaler","title":"<code>scaler</code>","text":"<p>Data scaling and normalization module for PCNToolkit.</p> <p>This module provides various scaling implementations for data preprocessing, including standardization, min-max scaling, robust scaling, and identity scaling. All scalers implement a common interface defined by the abstract base class Scaler.</p> <p>The module supports the following scaling operations:     - Standardization (zero mean, unit variance)     - Min-max scaling (to [0,1] range)     - Robust min-max scaling (using percentiles)     - Identity scaling (no transformation)</p> <p>Each scaler supports:     - Fitting to training data     - Transforming new data     - Inverse transforming scaled data     - Serialization to/from dictionaries     - Optional outlier adjustment</p> Available Classes <p>Scaler : ABC     Abstract base class defining the scaler interface StandardScaler     Standardizes features to zero mean and unit variance MinMaxScaler     Scales features to a fixed range [0, 1] RobustMinMaxScaler     Scales features using robust statistics based on percentiles IdentityScaler     Passes data through unchanged</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pcntoolkit.dataio.scaler import StandardScaler\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; scaler = StandardScaler()\n&gt;&gt;&gt; X_scaled = scaler.fit_transform(X)\n</code></pre> Notes <p>All scalers support both fitting to the entire dataset and transforming specific indices of features, allowing for flexible scaling strategies. The scalers can be serialized to dictionaries for saving/loading trained parameters.</p> See Also <p>pcntoolkit.normative_model : Uses scalers for data preprocessing pcntoolkit.dataio.basis_expansions : Complementary data transformations</p>"},{"location":"api/#pcntoolkit.dataio.scaler.IdentityScaler","title":"<code>IdentityScaler</code>","text":"<p>               Bases: <code>Scaler</code></p> <p>A scaler that returns the input unchanged.</p> <p>This scaler is useful as a placeholder when no scaling is desired but a scaler object is required by the API.</p> <p>Parameters:</p> Name Type Description Default <code>adjust_outliers</code> <code>bool</code> <p>Has no effect for this scaler, included for API compatibility</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pcntoolkit.dataio.scaler import IdentityScaler\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; scaler = IdentityScaler()\n&gt;&gt;&gt; X_scaled = scaler.fit_transform(X)\n&gt;&gt;&gt; np.array_equal(X, X_scaled)\nTrue\n</code></pre> Source code in <code>pcntoolkit/dataio/scaler.py</code> <pre><code>class IdentityScaler(Scaler):\n    \"\"\"A scaler that returns the input unchanged.\n\n    This scaler is useful as a placeholder when no scaling is desired\n    but a scaler object is required by the API.\n\n    Parameters\n    ----------\n    adjust_outliers : bool, optional\n        Has no effect for this scaler, included for API compatibility\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from pcntoolkit.dataio.scaler import IdentityScaler\n    &gt;&gt;&gt; X = np.array([[1, 2], [3, 4]])\n    &gt;&gt;&gt; scaler = IdentityScaler()\n    &gt;&gt;&gt; X_scaled = scaler.fit_transform(X)\n    &gt;&gt;&gt; np.array_equal(X, X_scaled)\n    True\n    \"\"\"\n\n    def fit(self, X: NDArray) -&gt; None:\n        pass\n\n    def transform(self, X: NDArray, index: Optional[NDArray] = None) -&gt; NDArray:\n        return X.copy()\n\n    def inverse_transform(self, X: NDArray, index: Optional[NDArray] = None) -&gt; NDArray:\n        return X.copy()\n\n    def to_dict(self) -&gt; Dict[str, Union[bool, str, float, List[float]]]:\n        return {\"scaler_type\": \"id\", \"adjust_outliers\": self.adjust_outliers}\n\n    @classmethod\n    def from_dict(\n        cls, my_dict: Dict[str, Union[bool, str, float, List[float]]]\n    ) -&gt; \"IdentityScaler\":\n        return cls(adjust_outliers=bool(my_dict[\"adjust_outliers\"]))\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.scaler.MinMaxScaler","title":"<code>MinMaxScaler</code>","text":"<p>               Bases: <code>Scaler</code></p> <p>Scale features to a fixed range [0, 1].</p> <p>Transforms features by scaling each feature to a given range (default [0, 1]): X_scaled = (X - X_min) / (X_max - X_min)</p> <p>Parameters:</p> Name Type Description Default <code>adjust_outliers</code> <code>bool</code> <p>Whether to clip transformed values to [0, 1], by default True</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>min</code> <code>Optional[NDArray]</code> <p>Minimum value for each feature from training data</p> <code>max</code> <code>Optional[NDArray]</code> <p>Maximum value for each feature from training data</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pcntoolkit.dataio.scaler import MinMaxScaler\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; scaler = MinMaxScaler()\n&gt;&gt;&gt; X_scaled = scaler.fit_transform(X)\n&gt;&gt;&gt; print(X_scaled.min(axis=0))  # [0, 0]\n&gt;&gt;&gt; print(X_scaled.max(axis=0))  # [1, 1]\n</code></pre> Source code in <code>pcntoolkit/dataio/scaler.py</code> <pre><code>class MinMaxScaler(Scaler):\n    \"\"\"Scale features to a fixed range [0, 1].\n\n    Transforms features by scaling each feature to a given range (default [0, 1]):\n    X_scaled = (X - X_min) / (X_max - X_min)\n\n    Parameters\n    ----------\n    adjust_outliers : bool, optional\n        Whether to clip transformed values to [0, 1], by default True\n\n    Attributes\n    ----------\n    min : Optional[NDArray]\n        Minimum value for each feature from training data\n    max : Optional[NDArray]\n        Maximum value for each feature from training data\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from pcntoolkit.dataio.scaler import MinMaxScaler\n    &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n    &gt;&gt;&gt; scaler = MinMaxScaler()\n    &gt;&gt;&gt; X_scaled = scaler.fit_transform(X)\n    &gt;&gt;&gt; print(X_scaled.min(axis=0))  # [0, 0]\n    &gt;&gt;&gt; print(X_scaled.max(axis=0))  # [1, 1]\n    \"\"\"\n\n    def __init__(self, adjust_outliers: bool = True) -&gt; None:\n        super().__init__(adjust_outliers)\n        self.min: Optional[NDArray] = None\n        self.max: Optional[NDArray] = None\n\n    def fit(self, X: NDArray) -&gt; None:\n        self.min = np.min(X, axis=0)\n        self.max = np.max(X, axis=0)\n\n    def transform(self, X: NDArray, index: Optional[NDArray] = None) -&gt; NDArray:\n        if self.min is None or self.max is None:\n            raise ValueError(\"Scaler must be fitted before transform\")\n        if index is None:\n            X_scaled = (X - self.min) / (self.max - self.min)\n        else:\n            X_scaled = (X - self.min[index]) / (self.max[index] - self.min[index])\n\n        if self.adjust_outliers:\n            X_scaled = np.clip(X_scaled, 0, 1)\n        return X_scaled\n\n    def inverse_transform(self, X: NDArray, index: Optional[NDArray] = None) -&gt; NDArray:\n        if self.min is None or self.max is None:\n            raise ValueError(\"Scaler must be fitted before inverse_transform\")\n        if index is None:\n            return X * (self.max - self.min) + self.min\n        return X * (self.max[index] - self.min[index]) + self.min[index]\n\n    def to_dict(self) -&gt; Dict[str, Union[bool, str, float, List[float]]]:\n        if self.min is None or self.max is None:\n            raise ValueError(\"Scaler must be fitted before serialization\")\n        return {\n            \"scaler_type\": \"minmax\",\n            \"adjust_outliers\": self.adjust_outliers,\n            \"min\": self.min.tolist(),\n            \"max\": self.max.tolist(),\n        }\n\n    @classmethod\n    def from_dict(\n        cls, my_dict: Dict[str, Union[bool, str, float, List[float]]]\n    ) -&gt; \"MinMaxScaler\":\n        instance = cls(adjust_outliers=bool(my_dict[\"adjust_outliers\"]))\n        instance.min = np.array(my_dict[\"min\"])\n        instance.max = np.array(my_dict[\"max\"])\n        return instance\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.scaler.RobustMinMaxScaler","title":"<code>RobustMinMaxScaler</code>","text":"<p>               Bases: <code>MinMaxScaler</code></p> <p>Scale features using robust statistics based on percentiles.</p> <p>Similar to MinMaxScaler but uses percentile-based statistics to be robust to outliers.</p> <p>Parameters:</p> Name Type Description Default <code>adjust_outliers</code> <code>bool</code> <p>Whether to clip transformed values to [0, 1], by default True</p> <code>True</code> <code>tail</code> <code>float</code> <p>The percentile to use for computing robust min/max, by default 0.05 (5th and 95th percentiles)</p> <code>0.05</code> <p>Attributes:</p> Name Type Description <code>min</code> <code>Optional[NDArray]</code> <p>Robust minimum for each feature from training data</p> <code>max</code> <code>Optional[NDArray]</code> <p>Robust maximum for each feature from training data</p> <code>tail</code> <code>float</code> <p>The percentile value used for robust statistics</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pcntoolkit.dataio.scaler import RobustMinMaxScaler\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [100, 6]])  # with outlier\n&gt;&gt;&gt; scaler = RobustMinMaxScaler(tail=0.1)\n&gt;&gt;&gt; X_scaled = scaler.fit_transform(X)\n</code></pre> Source code in <code>pcntoolkit/dataio/scaler.py</code> <pre><code>class RobustMinMaxScaler(MinMaxScaler):\n    \"\"\"Scale features using robust statistics based on percentiles.\n\n    Similar to MinMaxScaler but uses percentile-based statistics to be\n    robust to outliers.\n\n    Parameters\n    ----------\n    adjust_outliers : bool, optional\n        Whether to clip transformed values to [0, 1], by default True\n    tail : float, optional\n        The percentile to use for computing robust min/max, by default 0.05\n        (5th and 95th percentiles)\n\n    Attributes\n    ----------\n    min : Optional[NDArray]\n        Robust minimum for each feature from training data\n    max : Optional[NDArray]\n        Robust maximum for each feature from training data\n    tail : float\n        The percentile value used for robust statistics\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from pcntoolkit.dataio.scaler import RobustMinMaxScaler\n    &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [100, 6]])  # with outlier\n    &gt;&gt;&gt; scaler = RobustMinMaxScaler(tail=0.1)\n    &gt;&gt;&gt; X_scaled = scaler.fit_transform(X)\n    \"\"\"\n\n    def __init__(self, adjust_outliers: bool = True, tail: float = 0.05) -&gt; None:\n        super().__init__(adjust_outliers)\n        self.tail = tail\n\n    def fit(self, X: NDArray) -&gt; None:\n        reshape = len(X.shape) == 1\n        if reshape:\n            X = X.reshape(-1, 1)\n\n        self.min = np.zeros(X.shape[1])\n        self.max = np.zeros(X.shape[1])\n\n        for i in range(X.shape[1]):\n            sorted_vals = np.sort(X[:, i])\n            lower_idx = int(np.round(X.shape[0] * self.tail))\n            upper_idx = -int(np.round(X.shape[0] * self.tail))\n            self.min[i] = np.median(sorted_vals[0:lower_idx])\n            self.max[i] = np.median(sorted_vals[upper_idx:])\n\n    def to_dict(self) -&gt; Dict[str, Union[bool, str, float, List[float]]]:\n        if self.min is None or self.max is None:\n            raise ValueError(\"Scaler must be fitted before serialization\")\n        return {\n            \"scaler_type\": \"robminmax\",\n            \"adjust_outliers\": self.adjust_outliers,\n            \"tail\": self.tail,\n            \"min\": self.min.tolist(),\n            \"max\": self.max.tolist(),\n        }\n\n    @classmethod\n    def from_dict(\n        cls, my_dict: Dict[str, Union[bool, str, float, List[float]]]\n    ) -&gt; \"RobustMinMaxScaler\":\n        instance = cls(\n            adjust_outliers=bool(my_dict[\"adjust_outliers\"]),\n            tail=float(my_dict[\"tail\"]),  # type: ignore\n        )\n        instance.min = np.array(my_dict[\"min\"])\n        instance.max = np.array(my_dict[\"max\"])\n        return instance\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.scaler.Scaler","title":"<code>Scaler</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for data scaling operations.</p> <p>This class defines the interface for all scaling operations in PCNToolkit. Concrete implementations must implement fit, transform, inverse_transform, and to_dict methods.</p> <p>Parameters:</p> Name Type Description Default <code>adjust_outliers</code> <code>bool</code> <p>Whether to clip values to valid ranges, by default True</p> <code>True</code> Notes <p>All scaling operations support both fitting to the entire dataset and transforming specific indices of features, allowing for flexible scaling strategies.</p> Source code in <code>pcntoolkit/dataio/scaler.py</code> <pre><code>class Scaler(ABC):\n    \"\"\"Abstract base class for data scaling operations.\n\n    This class defines the interface for all scaling operations in PCNToolkit.\n    Concrete implementations must implement fit, transform, inverse_transform,\n    and to_dict methods.\n\n    Parameters\n    ----------\n    adjust_outliers : bool, optional\n        Whether to clip values to valid ranges, by default True\n\n    Notes\n    -----\n    All scaling operations support both fitting to the entire dataset and\n    transforming specific indices of features, allowing for flexible scaling\n    strategies.\n    \"\"\"\n\n    def __init__(self, adjust_outliers: bool = True) -&gt; None:\n        self.adjust_outliers = adjust_outliers\n\n    @abstractmethod\n    def fit(self, X: NDArray) -&gt; None:\n        \"\"\"Compute the parameters needed for scaling.\n\n        Parameters\n        ----------\n        X : NDArray\n            Training data to fit the scaler on, shape (n_samples, n_features)\n        \"\"\"\n\n    @abstractmethod\n    def transform(self, X: NDArray, index: Optional[NDArray] = None) -&gt; NDArray:\n        \"\"\"Transform the data using the fitted scaler.\n\n        Parameters\n        ----------\n        X : NDArray\n            Data to transform, shape (n_samples, n_features)\n        index : Optional[NDArray], optional\n            Indices of features to transform, by default None (transform all)\n\n        Returns\n        -------\n        NDArray\n            Transformed data\n\n        Raises\n        ------\n        ValueError\n            If the scaler has not been fitted\n        \"\"\"\n\n    @abstractmethod\n    def inverse_transform(self, X: NDArray, index: Optional[NDArray] = None) -&gt; NDArray:\n        \"\"\"Inverse transform scaled data back to original scale.\n\n        Parameters\n        ----------\n        X : NDArray\n            Data to inverse transform, shape (n_samples, n_features)\n        index : Optional[NDArray], optional\n            Indices of features to inverse transform, by default None (transform all)\n\n        Returns\n        -------\n        NDArray\n            Inverse transformed data\n\n        Raises\n        ------\n        ValueError\n            If the scaler has not been fitted\n        \"\"\"\n\n    def fit_transform(self, X: NDArray) -&gt; NDArray:\n        \"\"\"Fit the scaler and transform the data in one step.\n\n        Parameters\n        ----------\n        X : NDArray\n            Data to fit and transform, shape (n_samples, n_features)\n\n        Returns\n        -------\n        NDArray\n            Transformed data\n        \"\"\"\n        self.fit(X)\n        return self.transform(X)\n\n    @abstractmethod\n    def to_dict(self) -&gt; Dict[str, Union[bool, str, float, List[float]]]:\n        \"\"\"Convert scaler instance to dictionary.\"\"\"\n\n    @classmethod\n    def from_dict(\n        cls, my_dict: Dict[str, Union[bool, str, float, List[float]]]\n    ) -&gt; \"Scaler\":\n        \"\"\"Create a scaler instance from a dictionary.\n\n        Parameters\n        ----------\n        my_dict : Dict[str, Union[bool, str, float, List[float]]]\n            Dictionary containing scaler parameters. Must include 'scaler_type' key.\n\n        Returns\n        -------\n        Scaler\n            Instance of the appropriate scaler subclass\n\n        Raises\n        ------\n        ValueError\n            If scaler_type is missing or invalid\n        \"\"\"\n        if \"scaler_type\" not in my_dict:\n            raise ValueError(\"Dictionary must contain 'scaler_type' key\")\n\n        scaler_type: str = my_dict[\"scaler_type\"]  # type: ignore\n        scalers: Dict[str, Type[Scaler]] = {\n            \"standardize\": StandardScaler,\n            \"minmax\": MinMaxScaler,\n            \"robminmax\": RobustMinMaxScaler,\n            \"id\": IdentityScaler,\n            \"none\": IdentityScaler,\n        }\n\n        if scaler_type not in scalers:\n            raise ValueError(f\"Undefined scaler type: {scaler_type}\")\n\n        return scalers[scaler_type].from_dict(my_dict)\n\n    @staticmethod\n    def from_string(scaler_type: str, **kwargs: Any) -&gt; \"Scaler\":\n        \"\"\"Create a scaler instance from a string identifier.\n\n        Parameters\n        ----------\n        scaler_type : str\n            The type of scaling to apply. Options are:\n            - \"standardize\": zero mean, unit variance\n            - \"minmax\": scale to range [0,1]\n            - \"robminmax\": robust minmax scaling using percentiles\n            - \"id\" or \"none\": no scaling\n        **kwargs : dict\n            Additional arguments to pass to the scaler constructor\n\n        Returns\n        -------\n        Scaler\n            Instance of the appropriate scaler class\n        \"\"\"\n        scalers: Dict[str, Type[Scaler]] = {\n            \"standardize\": StandardScaler,\n            \"minmax\": MinMaxScaler,\n            \"robminmax\": RobustMinMaxScaler,\n            \"id\": IdentityScaler,\n            \"none\": IdentityScaler,\n        }\n\n        if scaler_type not in scalers:\n            raise ValueError(f\"Undefined scaler type: {scaler_type}\")\n\n        return scalers[scaler_type](**kwargs)\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.scaler.Scaler.fit","title":"<code>fit(X: NDArray) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Compute the parameters needed for scaling.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>NDArray</code> <p>Training data to fit the scaler on, shape (n_samples, n_features)</p> required Source code in <code>pcntoolkit/dataio/scaler.py</code> <pre><code>@abstractmethod\ndef fit(self, X: NDArray) -&gt; None:\n    \"\"\"Compute the parameters needed for scaling.\n\n    Parameters\n    ----------\n    X : NDArray\n        Training data to fit the scaler on, shape (n_samples, n_features)\n    \"\"\"\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.scaler.Scaler.fit_transform","title":"<code>fit_transform(X: NDArray) -&gt; NDArray</code>","text":"<p>Fit the scaler and transform the data in one step.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>NDArray</code> <p>Data to fit and transform, shape (n_samples, n_features)</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>Transformed data</p> Source code in <code>pcntoolkit/dataio/scaler.py</code> <pre><code>def fit_transform(self, X: NDArray) -&gt; NDArray:\n    \"\"\"Fit the scaler and transform the data in one step.\n\n    Parameters\n    ----------\n    X : NDArray\n        Data to fit and transform, shape (n_samples, n_features)\n\n    Returns\n    -------\n    NDArray\n        Transformed data\n    \"\"\"\n    self.fit(X)\n    return self.transform(X)\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.scaler.Scaler.from_dict","title":"<code>from_dict(my_dict: Dict[str, Union[bool, str, float, List[float]]]) -&gt; Scaler</code>  <code>classmethod</code>","text":"<p>Create a scaler instance from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>my_dict</code> <code>Dict[str, Union[bool, str, float, List[float]]]</code> <p>Dictionary containing scaler parameters. Must include 'scaler_type' key.</p> required <p>Returns:</p> Type Description <code>Scaler</code> <p>Instance of the appropriate scaler subclass</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If scaler_type is missing or invalid</p> Source code in <code>pcntoolkit/dataio/scaler.py</code> <pre><code>@classmethod\ndef from_dict(\n    cls, my_dict: Dict[str, Union[bool, str, float, List[float]]]\n) -&gt; \"Scaler\":\n    \"\"\"Create a scaler instance from a dictionary.\n\n    Parameters\n    ----------\n    my_dict : Dict[str, Union[bool, str, float, List[float]]]\n        Dictionary containing scaler parameters. Must include 'scaler_type' key.\n\n    Returns\n    -------\n    Scaler\n        Instance of the appropriate scaler subclass\n\n    Raises\n    ------\n    ValueError\n        If scaler_type is missing or invalid\n    \"\"\"\n    if \"scaler_type\" not in my_dict:\n        raise ValueError(\"Dictionary must contain 'scaler_type' key\")\n\n    scaler_type: str = my_dict[\"scaler_type\"]  # type: ignore\n    scalers: Dict[str, Type[Scaler]] = {\n        \"standardize\": StandardScaler,\n        \"minmax\": MinMaxScaler,\n        \"robminmax\": RobustMinMaxScaler,\n        \"id\": IdentityScaler,\n        \"none\": IdentityScaler,\n    }\n\n    if scaler_type not in scalers:\n        raise ValueError(f\"Undefined scaler type: {scaler_type}\")\n\n    return scalers[scaler_type].from_dict(my_dict)\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.scaler.Scaler.from_string","title":"<code>from_string(scaler_type: str, **kwargs: Any) -&gt; Scaler</code>  <code>staticmethod</code>","text":"<p>Create a scaler instance from a string identifier.</p> <p>Parameters:</p> Name Type Description Default <code>scaler_type</code> <code>str</code> <p>The type of scaling to apply. Options are: - \"standardize\": zero mean, unit variance - \"minmax\": scale to range [0,1] - \"robminmax\": robust minmax scaling using percentiles - \"id\" or \"none\": no scaling</p> required <code>**kwargs</code> <code>dict</code> <p>Additional arguments to pass to the scaler constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>Scaler</code> <p>Instance of the appropriate scaler class</p> Source code in <code>pcntoolkit/dataio/scaler.py</code> <pre><code>@staticmethod\ndef from_string(scaler_type: str, **kwargs: Any) -&gt; \"Scaler\":\n    \"\"\"Create a scaler instance from a string identifier.\n\n    Parameters\n    ----------\n    scaler_type : str\n        The type of scaling to apply. Options are:\n        - \"standardize\": zero mean, unit variance\n        - \"minmax\": scale to range [0,1]\n        - \"robminmax\": robust minmax scaling using percentiles\n        - \"id\" or \"none\": no scaling\n    **kwargs : dict\n        Additional arguments to pass to the scaler constructor\n\n    Returns\n    -------\n    Scaler\n        Instance of the appropriate scaler class\n    \"\"\"\n    scalers: Dict[str, Type[Scaler]] = {\n        \"standardize\": StandardScaler,\n        \"minmax\": MinMaxScaler,\n        \"robminmax\": RobustMinMaxScaler,\n        \"id\": IdentityScaler,\n        \"none\": IdentityScaler,\n    }\n\n    if scaler_type not in scalers:\n        raise ValueError(f\"Undefined scaler type: {scaler_type}\")\n\n    return scalers[scaler_type](**kwargs)\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.scaler.Scaler.inverse_transform","title":"<code>inverse_transform(X: NDArray, index: Optional[NDArray] = None) -&gt; NDArray</code>  <code>abstractmethod</code>","text":"<p>Inverse transform scaled data back to original scale.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>NDArray</code> <p>Data to inverse transform, shape (n_samples, n_features)</p> required <code>index</code> <code>Optional[NDArray]</code> <p>Indices of features to inverse transform, by default None (transform all)</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray</code> <p>Inverse transformed data</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the scaler has not been fitted</p> Source code in <code>pcntoolkit/dataio/scaler.py</code> <pre><code>@abstractmethod\ndef inverse_transform(self, X: NDArray, index: Optional[NDArray] = None) -&gt; NDArray:\n    \"\"\"Inverse transform scaled data back to original scale.\n\n    Parameters\n    ----------\n    X : NDArray\n        Data to inverse transform, shape (n_samples, n_features)\n    index : Optional[NDArray], optional\n        Indices of features to inverse transform, by default None (transform all)\n\n    Returns\n    -------\n    NDArray\n        Inverse transformed data\n\n    Raises\n    ------\n    ValueError\n        If the scaler has not been fitted\n    \"\"\"\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.scaler.Scaler.to_dict","title":"<code>to_dict() -&gt; Dict[str, Union[bool, str, float, List[float]]]</code>  <code>abstractmethod</code>","text":"<p>Convert scaler instance to dictionary.</p> Source code in <code>pcntoolkit/dataio/scaler.py</code> <pre><code>@abstractmethod\ndef to_dict(self) -&gt; Dict[str, Union[bool, str, float, List[float]]]:\n    \"\"\"Convert scaler instance to dictionary.\"\"\"\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.scaler.Scaler.transform","title":"<code>transform(X: NDArray, index: Optional[NDArray] = None) -&gt; NDArray</code>  <code>abstractmethod</code>","text":"<p>Transform the data using the fitted scaler.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>NDArray</code> <p>Data to transform, shape (n_samples, n_features)</p> required <code>index</code> <code>Optional[NDArray]</code> <p>Indices of features to transform, by default None (transform all)</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray</code> <p>Transformed data</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the scaler has not been fitted</p> Source code in <code>pcntoolkit/dataio/scaler.py</code> <pre><code>@abstractmethod\ndef transform(self, X: NDArray, index: Optional[NDArray] = None) -&gt; NDArray:\n    \"\"\"Transform the data using the fitted scaler.\n\n    Parameters\n    ----------\n    X : NDArray\n        Data to transform, shape (n_samples, n_features)\n    index : Optional[NDArray], optional\n        Indices of features to transform, by default None (transform all)\n\n    Returns\n    -------\n    NDArray\n        Transformed data\n\n    Raises\n    ------\n    ValueError\n        If the scaler has not been fitted\n    \"\"\"\n</code></pre>"},{"location":"api/#pcntoolkit.dataio.scaler.StandardScaler","title":"<code>StandardScaler</code>","text":"<p>               Bases: <code>Scaler</code></p> <p>Standardize features by removing the mean and scaling to unit variance.</p> <p>This scaler transforms the data to have zero mean and unit variance: z = (x - \u03bc) / \u03c3</p> <p>Parameters:</p> Name Type Description Default <code>adjust_outliers</code> <code>bool</code> <p>Whether to clip extreme values, by default True</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>m</code> <code>Optional[NDArray]</code> <p>Mean of the training data</p> <code>s</code> <code>Optional[NDArray]</code> <p>Standard deviation of the training data</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pcntoolkit.dataio.scaler import StandardScaler\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; scaler = StandardScaler()\n&gt;&gt;&gt; X_scaled = scaler.fit_transform(X)\n&gt;&gt;&gt; print(X_scaled.mean(axis=0))  # approximately [0, 0]\n&gt;&gt;&gt; print(X_scaled.std(axis=0))   # approximately [1, 1]\n</code></pre> Source code in <code>pcntoolkit/dataio/scaler.py</code> <pre><code>class StandardScaler(Scaler):\n    \"\"\"Standardize features by removing the mean and scaling to unit variance.\n\n    This scaler transforms the data to have zero mean and unit variance:\n    z = (x - \u03bc) / \u03c3\n\n    Parameters\n    ----------\n    adjust_outliers : bool, optional\n        Whether to clip extreme values, by default True\n\n    Attributes\n    ----------\n    m : Optional[NDArray]\n        Mean of the training data\n    s : Optional[NDArray]\n        Standard deviation of the training data\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from pcntoolkit.dataio.scaler import StandardScaler\n    &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n    &gt;&gt;&gt; scaler = StandardScaler()\n    &gt;&gt;&gt; X_scaled = scaler.fit_transform(X)\n    &gt;&gt;&gt; print(X_scaled.mean(axis=0))  # approximately [0, 0]\n    &gt;&gt;&gt; print(X_scaled.std(axis=0))   # approximately [1, 1]\n    \"\"\"\n\n    def __init__(self, adjust_outliers: bool = True) -&gt; None:\n        super().__init__(adjust_outliers)\n        self.m: Optional[NDArray] = None\n        self.s: Optional[NDArray] = None\n\n    def fit(self, X: NDArray) -&gt; None:\n        self.m = np.mean(X, axis=0)\n        self.s = np.std(X, axis=0)\n\n    def transform(self, X: NDArray, index: Optional[NDArray] = None) -&gt; NDArray:\n        if self.m is None or self.s is None:\n            raise ValueError(\"Scaler must be fitted before transform\")\n        if index is None:\n            return (X - self.m) / self.s\n        return (X - self.m[index]) / self.s[index]\n\n    def inverse_transform(self, X: NDArray, index: Optional[NDArray] = None) -&gt; NDArray:\n        if self.m is None or self.s is None:\n            raise ValueError(\"Scaler must be fitted before inverse_transform\")\n        if index is None:\n            return X * self.s + self.m\n        return X * self.s[index] + self.m[index]\n\n    def to_dict(self) -&gt; Dict[str, Union[bool, str, float, List[float]]]:\n        if self.m is None or self.s is None:\n            raise ValueError(\"Scaler must be fitted before serialization\")\n        return {\n            \"scaler_type\": \"standardize\",\n            \"adjust_outliers\": self.adjust_outliers,\n            \"m\": self.m.tolist(),\n            \"s\": self.s.tolist(),\n        }\n\n    @classmethod\n    def from_dict(\n        cls, my_dict: Dict[str, Union[bool, str, float, List[float]]]\n    ) -&gt; \"StandardScaler\":\n        instance = cls(adjust_outliers=bool(my_dict[\"adjust_outliers\"]))\n        instance.m = np.array(my_dict[\"m\"])\n        instance.s = np.array(my_dict[\"s\"])\n        return instance\n</code></pre>"},{"location":"api/#pcntoolkit.normative","title":"<code>normative</code>","text":"<p>Module providing entry points for fitting and predicting with normative models.</p>"},{"location":"api/#pcntoolkit.normative.estimate","title":"<code>estimate(conf_dict: dict) -&gt; None</code>","text":"<p>Legacy function signature. Calls fit_predict.</p> <p>Args:     conf_dict (dict): Dictionary containing configuration options</p> Source code in <code>pcntoolkit/normative.py</code> <pre><code>def estimate(conf_dict: dict) -&gt; None:\n    \"\"\"Legacy function signature. Calls fit_predict.\n\n    Args:\n        conf_dict (dict): Dictionary containing configuration options\n    \"\"\"\n    fit_predict(conf_dict)\n</code></pre>"},{"location":"api/#pcntoolkit.normative.fit","title":"<code>fit(conf_dict: dict) -&gt; None</code>","text":"<p>Fit a normative model.</p> <p>:param conf_dict: Dictionary containing configuration options</p> Source code in <code>pcntoolkit/normative.py</code> <pre><code>def fit(conf_dict: dict) -&gt; None:\n    \"\"\"\n    Fit a normative model.\n\n    :param conf_dict: Dictionary containing configuration options\n    \"\"\"\n\n    fit_data = load_data(conf_dict)\n    normative_model: NormBase = create_normative_model_from_args(conf_dict)\n    normative_model.fit(fit_data)\n    if normative_model.norm_conf.savemodel:\n        normative_model.save()\n</code></pre>"},{"location":"api/#pcntoolkit.normative.fit_predict","title":"<code>fit_predict(conf_dict: dict) -&gt; None</code>","text":"<p>Fit a normative model and predict response variables.</p> <p>:param conf_dict: Dictionary containing configuration options</p> Source code in <code>pcntoolkit/normative.py</code> <pre><code>def fit_predict(conf_dict: dict) -&gt; None:\n    \"\"\"\n    Fit a normative model and predict response variables.\n\n    :param conf_dict: Dictionary containing configuration options\n    \"\"\"\n\n    fit_data = load_data(conf_dict)\n    predict_data = load_test_data(conf_dict)\n\n    assert fit_data.is_compatible_with(\n        predict_data\n    ), \"Fit and predict data are not compatible.\"\n\n    normative_model: NormBase = create_normative_model_from_args(conf_dict)\n    normative_model.fit_predict(fit_data, predict_data)\n    if normative_model.norm_conf.savemodel:\n        normative_model.save()\n</code></pre>"},{"location":"api/#pcntoolkit.normative.get_argparser","title":"<code>get_argparser() -&gt; argparse.ArgumentParser</code>","text":"<p>Get an argument parser for the normative modeling functions.</p> <p>Returns:     argparse.ArgumentParser: The argument parser</p> Source code in <code>pcntoolkit/normative.py</code> <pre><code>def get_argparser() -&gt; argparse.ArgumentParser:\n    \"\"\"Get an argument parser for the normative modeling functions.\n\n    Returns:\n        argparse.ArgumentParser: The argument parser\n    \"\"\"\n    #  parse arguments\n    parser = argparse.ArgumentParser(description=\"Normative Modeling\")\n    parser.add_argument(\"responses\")\n    parser.add_argument(\n        \"-f\", \"--func\", help=\"Function to call\", dest=\"func\", default=\"estimate\"\n    )\n    parser.add_argument(\n        \"-m\", \"--maskfile\", help=\"mask file\", dest=\"maskfile\", default=None\n    )\n    parser.add_argument(\n        \"-c\", \"--covariates\", help=\"covariates file\", dest=\"covfile\", default=None\n    )\n    parser.add_argument(\n        \"-k\", \"--cvfolds\", help=\"cross-validation folds\", dest=\"cvfolds\", default=None\n    )\n    parser.add_argument(\n        \"-t\", \"--testcov\", help=\"covariates (test data)\", dest=\"testcov\", default=None\n    )\n    parser.add_argument(\n        \"-r\", \"--testresp\", help=\"responses (test data)\", dest=\"testresp\", default=None\n    )\n    parser.add_argument(\"-a\", \"--alg\", help=\"algorithm\", dest=\"alg\", default=\"gpr\")\n    return parser\n</code></pre>"},{"location":"api/#pcntoolkit.normative.get_conf_dict_from_args","title":"<code>get_conf_dict_from_args() -&gt; dict[str, str | int | float | bool]</code>","text":"<p>Parse the arguments and return a dictionary with the configuration options.</p> <p>Raises:     ValueError: Raised if an argument is specified twice.</p> <p>Returns:     dict[str, str | int | float | bool]: A dictionary with the configuration option, parsed to the correct type.</p> Source code in <code>pcntoolkit/normative.py</code> <pre><code>def get_conf_dict_from_args() -&gt; dict[str, str | int | float | bool]:\n    \"\"\"Parse the arguments and return a dictionary with the configuration options.\n\n    Raises:\n        ValueError: Raised if an argument is specified twice.\n\n    Returns:\n        dict[str, str | int | float | bool]: A dictionary with the configuration option, parsed to the correct type.\n    \"\"\"\n    parser = get_argparser()\n    known, unknown = parser.parse_known_args()\n\n    conf_dict = {}\n\n    for arg in vars(known):\n        conf_dict[arg] = getattr(known, arg)\n\n    for arg in unknown:\n        if \"=\" in arg:\n            key, value = arg.split(\"=\")\n            if key in conf_dict:\n                raise ValueError(f\"Argument {key} is specified twice.\")\n            conf_dict[key] = value\n\n    for k, v in conf_dict.items():\n        if v:\n            try:\n                conf_dict[k] = int(v)\n            except ValueError:\n                try:\n                    conf_dict[k] = float(v)\n                except ValueError:\n                    try:\n                        if v.lower() == \"true\":\n                            conf_dict[k] = True\n                        elif v.lower() == \"false\":\n                            conf_dict[k] = False\n                    except AttributeError:\n                        pass\n    return conf_dict\n</code></pre>"},{"location":"api/#pcntoolkit.normative.load_data","title":"<code>load_data(conf_dict: dict) -&gt; NormData</code>","text":"<p>Load the data from the configuration dictionary.</p> <p>Returns:     NormData: NormData object containing the data</p> Source code in <code>pcntoolkit/normative.py</code> <pre><code>def load_data(conf_dict: dict) -&gt; NormData:\n    \"\"\"Load the data from the configuration dictionary.\n\n    Returns:\n        NormData: NormData object containing the data\n    \"\"\"\n    respfile = conf_dict.pop(\"responses\")\n    covfile = conf_dict.pop(\"covfile\")\n    maskfile = conf_dict.pop(\"maskfile\", None)\n\n    X = fileio.load(covfile)\n    Y, _ = load_response_vars(respfile, maskfile=maskfile)\n    batch_effects = conf_dict.pop(\"trbefile\", None)\n    if batch_effects:\n        batch_effects = fileio.load(batch_effects)\n    else:\n        # If no batch effects are specified, create a zero array\n        batch_effects = np.zeros((Y.shape[0], 1))\n    data = NormData.from_ndarrays(\"fit_data\", X, Y, batch_effects)\n    return data\n</code></pre>"},{"location":"api/#pcntoolkit.normative.load_response_vars","title":"<code>load_response_vars(datafile: str, maskfile: str | None = None, vol: bool = True) -&gt; tuple[np.ndarray, np.ndarray | None]</code>","text":"<p>Load response variables from file. This will load the data and mask it if necessary. If the data is in ascii format it will be converted into a numpy array. If the data is in neuroimaging format it will be reshaped into a 2D array (subjects x variables) and a mask will be created if necessary.</p> <p>:param datafile: File containing the response variables :param maskfile: Mask file (nifti only) :param vol: If True, load the data as a 4D volume (nifti only) :returns Y: Response variables :returns volmask: Mask file (nifti only)</p> Source code in <code>pcntoolkit/normative.py</code> <pre><code>def load_response_vars(\n    datafile: str, maskfile: str | None = None, vol: bool = True\n) -&gt; tuple[np.ndarray, np.ndarray | None]:\n    \"\"\"\n    Load response variables from file. This will load the data and mask it if\n    necessary. If the data is in ascii format it will be converted into a numpy\n    array. If the data is in neuroimaging format it will be reshaped into a\n    2D array (subjects x variables) and a mask will be created if necessary.\n\n    :param datafile: File containing the response variables\n    :param maskfile: Mask file (nifti only)\n    :param vol: If True, load the data as a 4D volume (nifti only)\n    :returns Y: Response variables\n    :returns volmask: Mask file (nifti only)\n    \"\"\"\n\n    if fileio.file_type(datafile) == \"nifti\":\n        dat = fileio.load_nifti(datafile, vol=vol)\n        volmask = fileio.create_mask(dat, mask=maskfile)\n        Y = fileio.vol2vec(dat, volmask).T\n    else:\n        Y = fileio.load(datafile)\n        volmask = None\n        if fileio.file_type(datafile) == \"cifti\":\n            Y = Y.T\n\n    return Y, volmask\n</code></pre>"},{"location":"api/#pcntoolkit.normative.load_test_data","title":"<code>load_test_data(conf_dict: dict) -&gt; NormData</code>","text":"<p>Load the test data from the file specified in the configuration dictionary.</p> <p>Args:     conf_dict (dict): dictionary containing the configuration options</p> <p>Returns:     NormData: NormData object containing the test data</p> Source code in <code>pcntoolkit/normative.py</code> <pre><code>def load_test_data(conf_dict: dict) -&gt; NormData:\n    \"\"\"Load the test data from the file specified in the configuration dictionary.\n\n    Args:\n        conf_dict (dict): dictionary containing the configuration options\n\n    Returns:\n        NormData: NormData object containing the test data\n    \"\"\"\n    respfile = conf_dict.pop(\"testresp\")\n    covfile = conf_dict.pop(\"testcov\")\n    maskfile = conf_dict.pop(\"maskfile\", None)\n\n    X = fileio.load(covfile)\n    Y, _ = load_response_vars(respfile, maskfile=maskfile)\n    batch_effects = conf_dict.pop(\"tsbefile\", None)\n    if batch_effects:\n        batch_effects = fileio.load(batch_effects)\n    else:\n        batch_effects = np.zeros((Y.shape[0], 1))\n\n    data = NormData.from_ndarrays(\"predict_data\", X, Y, batch_effects)\n\n    return data\n</code></pre>"},{"location":"api/#pcntoolkit.normative.main","title":"<code>main() -&gt; None</code>","text":"<p>Main function to run the normative modeling functions.</p> <p>Raises:     ValueError: If the function specified in the configuration dictionary is unknown.</p> Source code in <code>pcntoolkit/normative.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main function to run the normative modeling functions.\n\n    Raises:\n        ValueError: If the function specified in the configuration dictionary is unknown.\n\n    \"\"\"\n    conf_dict = {\n        \"responses\": \"/home/stijn/Projects/PCNtoolkit/pytest_tests/resources/data/responses.csv\",\n        \"func\": \"predict\",\n        \"maskfile\": None,\n        \"covfile\": \"/home/stijn/Projects/PCNtoolkit/pytest_tests/resources/data/covariates.csv\",\n        \"cvfolds\": None,\n        \"testcov\": None,\n        \"testresp\": None,\n        \"alg\": \"hbr\",\n        \"trbefile\": \"/home/stijn/Projects/PCNtoolkit/pytest_tests/resources/data/batch_effects.csv\",\n        \"save_dir\": \"/home/stijn/Projects/PCNtoolkit/pytest_tests/resources/hbr/save_load_test\",\n        \"log_dir\": \"/home/stijn/Projects/PCNtoolkit/pytest_tests/resources/hbr/log_test\",\n        \"basis_function\": \"bspline\",\n    }\n\n    func = conf_dict.pop(\"func\")\n    if func == \"fit\":\n        fit(conf_dict)\n    elif func == \"predict\":\n        predict(conf_dict)\n    elif func == \"estimate\":\n        estimate(conf_dict)\n    else:\n        raise ValueError(f\"Unknown function {func}.\")\n</code></pre>"},{"location":"api/#pcntoolkit.normative.make_synthetic_data","title":"<code>make_synthetic_data() -&gt; None</code>","text":"<p>Create synthetic data for testing.</p> Source code in <code>pcntoolkit/normative.py</code> <pre><code>def make_synthetic_data() -&gt; None:\n    \"\"\"Create synthetic data for testing.\"\"\"\n    np.random.seed(42)\n    X = np.random.randn(1000, 2)\n    Y = np.random.randn(1000, 2)\n    batch_effects = []\n    for i in range(2):\n        batch_effects.append(np.random.choice(list(range(i + 2)), size=1000))\n    batch_effects = np.stack(batch_effects, axis=1)\n    np.savetxt(\"covariates.csv\", X)\n    np.savetxt(\"responses.csv\", Y)\n    np.savetxt(\"batch_effects.csv\", batch_effects)\n\n    X = np.random.randn(1000, 2)\n    Y = np.random.randn(1000, 2)\n    batch_effects = []\n    for i in range(2):\n        batch_effects.append(np.random.choice(list(range(i + 2)), size=1000))\n    batch_effects = np.stack(batch_effects, axis=1)\n    np.savetxt(\"covariates_test.csv\", X)\n    np.savetxt(\"responses_test.csv\", Y)\n    np.savetxt(\"batch_effects_test.csv\", batch_effects)\n</code></pre>"},{"location":"api/#pcntoolkit.normative.predict","title":"<code>predict(conf_dict: dict) -&gt; None</code>","text":"<p>Predict response variables using a normative model.</p> <p>:param conf_dict: Dictionary containing configuration options</p> Source code in <code>pcntoolkit/normative.py</code> <pre><code>def predict(conf_dict: dict) -&gt; None:\n    \"\"\"\n    Predict response variables using a normative model.\n\n    :param conf_dict: Dictionary containing configuration options\n    \"\"\"\n\n    predict_data = load_data(conf_dict)\n    normative_model = load_normative_model(conf_dict[\"save_dir\"])\n    normative_model.predict(predict_data)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model","title":"<code>normative_model</code>","text":""},{"location":"api/#pcntoolkit.normative_model.norm_base","title":"<code>norm_base</code>","text":""},{"location":"api/#pcntoolkit.normative_model.norm_base--normbase-module-documentation","title":"NormBase Module Documentation","text":"<p>This module provides the abstract base class for normative modeling implementations.</p> <p>The module implements a flexible framework for creating, training, and applying normative models to neuroimaging or other high-dimensional data. It supports various regression models and provides functionality for data preprocessing, model fitting, prediction, and evaluation.</p> <p>Classes:</p> Name Description <code>NormBase</code> <p>Abstract base class for normative modeling implementations.</p> Notes <p>The NormBase class is designed to be subclassed to implement specific normative modeling approaches. It provides a comprehensive interface for: - Model fitting and prediction - Data preprocessing and postprocessing - Model evaluation and visualization - Model persistence (saving/loading) - Transfer learning capabilities</p> <p>The class supports multiple regression model types including: - Bayesian Linear Regression (BLR) - Gaussian Process Regression (GPR) - Hierarchical Bayesian Regression (HBR)</p> <p>The class structure of a normative model (using BLR in this example) is:</p> <ul> <li>NormBLR (abstract base class)<ul> <li>NormConf</li> <li>RegressionModels<ol> <li>BLR (feature 1)<ul> <li>BLRConf</li> </ul> </li> <li>BLR (feature 2)<ul> <li>BLRConf</li> </ul> </li> <li>...</li> </ol> </li> </ul> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pcntoolkit.normative_model import NormBase\n&gt;&gt;&gt; from pcntoolkit.normative_model.norm_conf import NormConf\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create configuration for normative model\n&gt;&gt;&gt; norm_conf = NormConf(save_dir=\"./models\", log_dir=\"./logs\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Create configuration for regression models\n&gt;&gt;&gt; hbr_conf = HBRConf()\n</code></pre> <pre><code>&gt;&gt;&gt; # Create normative model\n&gt;&gt;&gt; model = NormHBR(norm_conf, hbr_conf)\n</code></pre> <pre><code>&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit model\n&gt;&gt;&gt; model.fit(training_data)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Make predictions\n&gt;&gt;&gt; predictions = model.predict(test_data)\n</code></pre> See Also <p>pcntoolkit.regression_model : Package containing various regression model implementations pcntoolkit.dataio : Package for data input/output operations</p>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase","title":"<code>NormBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for normative modeling implementations.</p> <p>This class provides the foundation for building normative models, handling multiple response variables through separate regression models. It manages data preprocessing, model fitting, prediction, and evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>norm_conf</code> <code>NormConf</code> <p>Configuration object containing normative model parameters.</p> required <p>Attributes:</p> Name Type Description <code>response_vars</code> <code>list[str]</code> <p>List of response variable names.</p> <code>regression_model_type</code> <code>Any</code> <p>Type of regression model being used.</p> <code>default_reg_conf</code> <code>RegConf</code> <p>Default regression configuration.</p> <code>regression_models</code> <code>dict[str, RegressionModel]</code> <p>Dictionary mapping response variables to their regression models.</p> <code>focused_var</code> <code>str</code> <p>Currently focused response variable.</p> <code>evaluator</code> <code>Evaluator</code> <p>Model evaluation utility instance.</p> <code>inscalers</code> <code>dict</code> <p>Input data scalers.</p> <code>outscalers</code> <code>dict</code> <p>Output data scalers.</p> <code>bspline_basis</code> <code>Any</code> <p>B-spline basis for covariate expansion.</p> <p>Methods:</p> Name Description <code>fit</code> <p>Fit the normative model to training data.</p> <code>predict</code> <p>Make predictions using the fitted model.</p> <code>fit_predict</code> <p>Fit the model and make predictions in one step.</p> <code>transfer</code> <p>Transfer the model to new data.</p> <code>extend</code> <p>Extend the model with additional data.</p> <code>tune</code> <p>Tune model parameters using validation data.</p> <code>merge</code> <p>Merge current model with another normative model.</p> <code>evaluate</code> <p>Evaluate model performance.</p> <code>compute_centiles</code> <p>Compute prediction centiles</p> <code>compute_zscores</code> <p>Compute z-scores for predictions.</p> <code>save</code> <p>Save model to disk.</p> <code>load</code> <p>Load model from disk.</p> Notes <p>The NormBase class implements the Template Method pattern, where the main workflow is defined in the base class, but specific implementations are delegated to subclasses through abstract methods.</p> Abstract Methods <p>_fit(data: NormData) -&gt; NormData     Internal fitting implementation.</p> <p>_predict(data: NormData) -&gt; NormData     Internal prediction implementation.</p> <p>_fit_predict(fit_data: NormData, predict_data: NormData) -&gt; NormData     Internal fit-predict implementation.</p> <p>_transfer(data: NormData, **kwargs: Any) -&gt; RegressionModel     Internal transfer learning implementation.</p> <p>_extend(data: NormData) -&gt; NormBase     Internal model extension implementation.</p> <p>_tune(data: NormData) -&gt; NormBase     Internal parameter tuning implementation.</p> <p>_merge(other: NormBase) -&gt; NormBase     Internal model merging implementation.</p> <p>_centiles(data: NormData, centiles: np.ndarray) -&gt; xr.DataArray     Internal centile computation implementation.</p> <p>_zscores(data: NormData) -&gt; xr.DataArray     Internal z-score computation implementation.</p> <p>Examples:</p> <p>Example of implementing a concrete normative model:</p> <pre><code>&gt;&gt;&gt; class ConcreteNormModel(NormBase):\n...     def _fit(self, data):\n...         # Implementation\n...         pass\n...\n...     def _predict(self, data):\n...         # Implementation\n...         pass\n...\n...     # Implement other abstract methods...\n</code></pre> <p>Example of using a normative model:</p> <pre><code>&gt;&gt;&gt; from pcntoolkit.dataio import NormData\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Prepare data\n&gt;&gt;&gt; train_data = NormData(X=train_covariates, y=train_responses)\n&gt;&gt;&gt; test_data = NormData(X=test_covariates, y=test_responses)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create and fit model\n&gt;&gt;&gt; model = ConcreteNormModel(norm_conf, reg_conf)\n&gt;&gt;&gt; model.fit(train_data)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Make predictions\n&gt;&gt;&gt; predictions = model.predict(test_data)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compute evaluation metrics\n&gt;&gt;&gt; model.evaluate(predictions)\n</code></pre> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>class NormBase(ABC):\n    \"\"\"\n    Abstract base class for normative modeling implementations.\n\n    This class provides the foundation for building normative models, handling multiple\n    response variables through separate regression models. It manages data preprocessing,\n    model fitting, prediction, and evaluation.\n\n    Parameters\n    ----------\n    norm_conf : NormConf\n        Configuration object containing normative model parameters.\n\n    Attributes\n    ----------\n    response_vars : list[str]\n        List of response variable names.\n    regression_model_type : Any\n        Type of regression model being used.\n    default_reg_conf : RegConf\n        Default regression configuration.\n    regression_models : dict[str, RegressionModel]\n        Dictionary mapping response variables to their regression models.\n    focused_var : str\n        Currently focused response variable.\n    evaluator : Evaluator\n        Model evaluation utility instance.\n    inscalers : dict\n        Input data scalers.\n    outscalers : dict\n        Output data scalers.\n    bspline_basis : Any\n        B-spline basis for covariate expansion.\n\n    Methods\n    -------\n    fit(data: NormData) -&gt; None\n        Fit the normative model to training data.\n\n    predict(data: NormData) -&gt; NormData\n        Make predictions using the fitted model.\n\n    fit_predict(fit_data: NormData, predict_data: NormData) -&gt; NormData\n        Fit the model and make predictions in one step.\n\n    transfer(data: NormData, *args: Any, **kwargs: Any) -&gt; NormBase\n        Transfer the model to new data.\n\n    extend(data: NormData) -&gt; None\n        Extend the model with additional data.\n\n    tune(data: NormData) -&gt; None\n        Tune model parameters using validation data.\n\n    merge(other: NormBase) -&gt; None\n        Merge current model with another normative model.\n\n    evaluate(data: NormData) -&gt; None\n        Evaluate model performance.\n\n    compute_centiles(data: NormData, cdf: Optional[List | np.ndarray] = None) -&gt; NormData\n        Compute prediction centiles\n    compute_zscores(data: NormData) -&gt; NormData\n        Compute z-scores for predictions.\n\n    save(path: Optional[str] = None) -&gt; None\n        Save model to disk.\n\n    load(path: str) -&gt; NormBase\n        Load model from disk.\n\n    Notes\n    -----\n    The NormBase class implements the Template Method pattern, where the main workflow\n    is defined in the base class, but specific implementations are delegated to\n    subclasses through abstract methods.\n\n    Abstract Methods\n    ---------------\n    _fit(data: NormData) -&gt; NormData\n        Internal fitting implementation.\n\n    _predict(data: NormData) -&gt; NormData\n        Internal prediction implementation.\n\n    _fit_predict(fit_data: NormData, predict_data: NormData) -&gt; NormData\n        Internal fit-predict implementation.\n\n    _transfer(data: NormData, **kwargs: Any) -&gt; RegressionModel\n        Internal transfer learning implementation.\n\n    _extend(data: NormData) -&gt; NormBase\n        Internal model extension implementation.\n\n    _tune(data: NormData) -&gt; NormBase\n        Internal parameter tuning implementation.\n\n    _merge(other: NormBase) -&gt; NormBase\n        Internal model merging implementation.\n\n    _centiles(data: NormData, centiles: np.ndarray) -&gt; xr.DataArray\n        Internal centile computation implementation.\n\n    _zscores(data: NormData) -&gt; xr.DataArray\n        Internal z-score computation implementation.\n\n    Examples\n    --------\n    Example of implementing a concrete normative model:\n\n    &gt;&gt;&gt; class ConcreteNormModel(NormBase):\n    ...     def _fit(self, data):\n    ...         # Implementation\n    ...         pass\n    ...\n    ...     def _predict(self, data):\n    ...         # Implementation\n    ...         pass\n    ...\n    ...     # Implement other abstract methods...\n\n    Example of using a normative model:\n\n    &gt;&gt;&gt; from pcntoolkit.dataio import NormData\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Prepare data\n    &gt;&gt;&gt; train_data = NormData(X=train_covariates, y=train_responses)\n    &gt;&gt;&gt; test_data = NormData(X=test_covariates, y=test_responses)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Create and fit model\n    &gt;&gt;&gt; model = ConcreteNormModel(norm_conf, reg_conf)\n    &gt;&gt;&gt; model.fit(train_data)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Make predictions\n    &gt;&gt;&gt; predictions = model.predict(test_data)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Compute evaluation metrics\n    &gt;&gt;&gt; model.evaluate(predictions)\n    \"\"\"\n\n    def __init__(self, norm_conf: NormConf):\n        self._norm_conf: NormConf = norm_conf\n        object.__setattr__(\n            self._norm_conf, \"normative_model_name\", self.__class__.__name__\n        )\n\n        self.response_vars: list[str] = None  # type: ignore\n        self.regression_model_type: Any = None  # type: ignore\n        self.default_reg_conf: RegConf = None  # type: ignore\n        self.regression_models: dict[str, RegressionModel] = {}\n        self.focused_var: str = None  # type: ignore\n        self.evaluator = Evaluator()\n        self.inscalers: dict = {}\n        self.outscalers: dict = {}\n        self.bspline_basis: Any = None\n\n    def fit(self, data: NormData) -&gt; None:\n        \"\"\"\n        Fits a regression model for each response variable in the data.\n\n        This method performs the following steps:\n        1. Preprocesses the input data (scaling and basis expansion)\n        2. Extracts response variables\n        3. Fits individual regression models for each response variable\n\n        Parameters\n        ----------\n        data : NormData\n            Training data containing covariates (X) and response variables (y).\n            Must be a valid NormData object with properly formatted dimensions:\n            - X: (n_samples, n_covariates)\n            - y: (n_samples, n_response_vars)\n\n        Returns\n        -------\n        None\n            The method modifies the model's internal state by fitting regression models.\n\n        Notes\n        -----\n        - The method fits one regression model per response variable\n        - Each model is stored in self.regression_models with response variable name as key\n        - Preprocessing includes scaling and basis expansion based on norm_conf settings\n\n        Examples\n        --------\n        &gt;&gt;&gt; from pcntoolkit.dataio import NormData\n        &gt;&gt;&gt; model = NormBase(norm_conf)\n        &gt;&gt;&gt; train_data = NormData(X=covariates, y=responses)\n        &gt;&gt;&gt; model.fit(train_data)\n\n        Raises\n        ------\n        ValueError\n            If data is not properly formatted or contains invalid values\n        RuntimeError\n            If preprocessing or model fitting fails\n        \"\"\"\n        self.preprocess(data)\n        self.response_vars = data.response_vars.to_numpy().copy().tolist()\n        print(f\"Going to fit {len(self.response_vars)} models\")\n        for responsevar in self.response_vars:\n            resp_fit_data = data.sel(response_vars=responsevar)\n            self.focus(responsevar)\n            print(f\"Fitting model for {responsevar}\")\n            self._fit(resp_fit_data)\n            self.reset()\n\n    def predict(self, data: NormData) -&gt; NormData:\n        \"\"\"\n        Makes predictions for each response variable using fitted regression models.\n\n        This method performs the following steps:\n        1. Preprocesses the input data\n        2. Generates predictions for each response variable\n        3. Evaluates prediction performance\n        4. Postprocesses the predictions\n\n        Parameters\n        ----------\n        data : NormData\n            Test data containing covariates (X) for which to generate predictions.\n            Must have the same covariate structure as training data.\n\n        Returns\n        -------\n        NormData\n            Prediction results containing:\n            - yhat: predicted values\n            - ys2: prediction variances (if applicable)\n            - Additional metrics (z-scores, centiles, etc.)\n\n        Notes\n        -----\n        - Requires models to be previously fitted using fit()\n        - Predictions are made independently for each response variable\n        - Automatically computes evaluation metrics after prediction\n\n        Examples\n        --------\n        &gt;&gt;&gt; test_data = NormData(X=test_covariates)\n        &gt;&gt;&gt; predictions = model.predict(test_data)\n        &gt;&gt;&gt; print(predictions.yhat)  # access predictions\n        &gt;&gt;&gt; print(predictions.ys2)   # access variances\n\n        Raises\n        ------\n        ValueError\n            If model hasn't been fitted or data format is invalid\n        RuntimeError\n            If prediction process fails for any response variable\n        \"\"\"\n        self.preprocess(data)\n        print(f\"Going to predict {len(self.response_vars)} models\")\n        for responsevar in self.response_vars:\n            resp_predict_data = data.sel(response_vars=responsevar)\n            if responsevar not in self.regression_models:\n                raise ValueError(\n                    f\"Attempted to predict model {responsevar}, but it does not exist.\"\n                )\n            self.focus(responsevar)\n            print(f\"Predicting model for {responsevar}\")\n            self._predict(resp_predict_data)\n            self.reset()\n        self.evaluate(data)\n        return data\n\n    def fit_predict(self, fit_data: NormData, predict_data: NormData) -&gt; NormData:\n        \"\"\"\n        Combines model fitting and prediction in a single operation.\n\n        This method provides a convenient way to fit models and generate predictions\n        in one step, which can be more efficient than separate fit() and predict()\n        calls.\n\n        Parameters\n        ----------\n        fit_data : NormData\n            Training data containing covariates (X) and response variables (y)\n            for model fitting. Must be a valid NormData object with dimensions:\n            - X: (n_train_samples, n_covariates)\n            - y: (n_train_samples, n_response_vars)\n\n        predict_data : NormData\n            Test data containing covariates (X) for prediction. Must have the same\n            covariate structure as fit_data with dimensions:\n            - X: (n_test_samples, n_covariates)\n\n        Returns\n        -------\n        NormData\n            Prediction results containing:\n            - yhat: predicted values (n_test_samples, n_response_vars)\n            - ys2: prediction variances (n_test_samples, n_response_vars)\n            - zscores: standardized residuals\n            - centiles: prediction percentiles\n            - Additional evaluation metrics\n\n        Notes\n        -----\n        - Performs compatibility check between fit_data and predict_data\n        - More memory efficient than separate fit() and predict() calls\n        - Automatically handles preprocessing and postprocessing\n        - Computes evaluation metrics after prediction\n\n        Examples\n        --------\n        &gt;&gt;&gt; train_data = NormData(X=train_covariates, y=train_responses)\n        &gt;&gt;&gt; test_data = NormData(X=test_covariates)\n        &gt;&gt;&gt; results = model.fit_predict(train_data, test_data)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Access predictions\n        &gt;&gt;&gt; predictions = results.yhat\n        &gt;&gt;&gt; variances = results.ys2\n        &gt;&gt;&gt; zscores = results.zscores\n        &gt;&gt;&gt; centiles = results.centiles\n\n        Raises\n        ------\n        ValueError\n            If data formats are incompatible or invalid\n        AssertionError\n            If fit_data and predict_data have incompatible structures\n        RuntimeError\n            If fitting or prediction process fails\n\n        See Also\n        --------\n        fit : Method for model fitting only\n        predict : Method for prediction using pre-fitted model\n        compute_zscores : Method for computing standardized residuals\n        compute_centiles : Method for computing prediction percentiles\n        \"\"\"\n\n        assert fit_data.is_compatible_with(\n            predict_data\n        ), \"Fit data and predict data are not compatible!\"\n\n        self.preprocess(fit_data)\n        self.preprocess(predict_data)\n        self.response_vars = fit_data.response_vars.to_numpy().copy().tolist()\n        print(f\"Going to fit and predict {len(self.response_vars)} models\")\n        for responsevar in self.response_vars:\n            resp_fit_data = fit_data.sel(response_vars=responsevar)\n            resp_predict_data = predict_data.sel(response_vars=responsevar)\n            if responsevar not in self.regression_models:\n                self.regression_models[responsevar] = self.regression_model_type(\n                    responsevar, self.default_reg_conf\n                )\n            self.focus(responsevar)\n            print(f\"Fitting and predicting model for {responsevar}\")\n            self._fit_predict(resp_fit_data, resp_predict_data)\n\n            self.reset()\n\n        # Get the results\n        self.evaluate(predict_data)\n        return predict_data\n\n    def transfer(self, data: NormData, *args: Any, **kwargs: Any) -&gt; \"NormBase\":\n        \"\"\"\n        Transfers the normative model to new data, creating a new adapted model.\n\n        This method performs transfer learning by adapting the existing model to new data\n        while preserving knowledge from the original training. It creates a new normative\n        model instance with transferred parameters for each response variable.\n\n        Parameters\n        ----------\n        data : NormData\n            Transfer data containing covariates (X) and response variables (y).\n            Must have compatible structure with the original training data.\n        *args : Any\n            Additional positional arguments passed to the underlying transfer implementation.\n        **kwargs : Any\n            Additional keyword arguments passed to the underlying transfer implementation.\n            Common options include:\n            - 'transfer_type': str, type of transfer learning to perform\n            - 'learning_rate': float, adaptation rate for transfer\n            - 'regularization': float, strength of regularization during transfer\n\n        Returns\n        -------\n        NormBase\n            A new normative model instance adapted to the transfer data, containing:\n            - Transferred regression models for each response variable\n            - Updated configuration with transfer-specific settings\n            - Preserved preprocessing transformations from original model\n\n        Notes\n        -----\n        The transfer process:\n        1. Preprocesses transfer data using original scalers\n        2. Transfers each response variable's model separately\n        3. Creates new model instance with transferred parameters\n        4. Maintains original model's configuration with transfer-specific adjustments\n\n        The method supports different transfer learning approaches depending on the\n        underlying regression model implementation.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Original model trained on source domain\n        &gt;&gt;&gt; original_model = NormBase(norm_conf)\n        &gt;&gt;&gt; original_model.fit(source_data)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Transfer to target domain\n        &gt;&gt;&gt; transfer_data = NormData(X=target_covariates, y=target_responses)\n        &gt;&gt;&gt; transferred_model = original_model.transfer(\n        ...     transfer_data,\n        ...     transfer_type='fine_tune',\n        ...     learning_rate=0.01\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Make predictions with transferred model\n        &gt;&gt;&gt; predictions = transferred_model.predict(test_data)\n\n        Raises\n        ------\n        ValueError\n            If the model hasn't been fitted before transfer attempt\n        AssertionError\n            If transfer data is incompatible with original model structure\n        RuntimeError\n            If transfer process fails for any response variable\n\n        See Also\n        --------\n        _transfer : Abstract method implementing specific transfer logic\n        fit : Method for initial model fitting\n        predict : Method for making predictions\n\n        Notes\n        -----\n        Transfer learning considerations:\n        - Ensures knowledge preservation from source domain\n        - Adapts to target domain characteristics\n        - Maintains model structure and constraints\n        - Supports various transfer strategies\n\n        The effectiveness of transfer depends on:\n        - Similarity between source and target domains\n        - Amount of transfer data available\n        - Choice of transfer learning parameters\n        - Underlying model architecture\n        \"\"\"\n        self.preprocess(data)\n        transfered_models = {}\n        print(f\"Going to transfer {len(self.response_vars)} models\")\n        for responsevar in self.response_vars:\n            resp_transfer_data = data.sel(response_vars=responsevar)\n            if responsevar not in self.regression_models:\n                raise ValueError(\n                    \"Attempted to transfer a model that has not been fitted.\"\n                )\n            self.focus(responsevar)\n            print(f\"Transferring model for {responsevar}\")\n            transfered_models[responsevar] = self._transfer(\n                resp_transfer_data, *args, **kwargs\n            )\n            self.reset()\n\n        transfered_norm_conf_dict = self.norm_conf.to_dict()\n        transfered_norm_conf_dict[\"save_dir\"] = self.norm_conf.save_dir + \"_transfer\"\n        transfered_norm_conf_dict[\"log_dir\"] = self.norm_conf.log_dir + \"_transfer\"\n        transfered_norm_conf = NormConf.from_dict(transfered_norm_conf_dict)\n\n        # pylint: disable=too-many-function-args\n        transfered_normative_model = self.__class__(\n            transfered_norm_conf,\n            self.default_reg_conf,  # type: ignore\n        )\n\n        transfered_normative_model.response_vars = (\n            data.response_vars.to_numpy().copy().tolist()\n        )\n        transfered_normative_model.regression_models = transfered_models\n        return transfered_normative_model\n\n    def extend(self, data: NormData) -&gt; None:\n        \"\"\"Extends the normative model with new data.\n\n        Args:\n            data (NormData): Data containing the covariates and response variables to extend the model with.\n        \"\"\"\n        self._extend(data)\n\n    def tune(self, data: NormData) -&gt; None:\n        \"\"\"Tunes the normative model with new data.\n\n        Args:\n            data (NormData): Data containing the covariates and response variables to tune the model with.\n        \"\"\"\n        self._tune(data)\n\n    def merge(self, other: \"NormBase\") -&gt; None:\n        \"\"\"Merges the current normative model with another normative model.\n\n        Args:\n            other (NormBase): The other normative model to merge with.\n\n        Raises:\n            ValueError: Error if the models are not of the same type.\n        \"\"\"\n        if not self.__class__ == other.__class__:\n            raise ValueError(\"Attempted to merge normative models of different types.\")\n        self._merge(other)\n\n    def evaluate(self, data: NormData) -&gt; None:\n        \"\"\"\n        Evaluates model performance by computing z-scores, centiles, and additional evaluation metrics.\n\n        This method performs a comprehensive evaluation of the normative model by:\n        1. Computing standardized residuals (z-scores)\n        2. Computing prediction centiles\n        3. Calculating various evaluation metrics through the Evaluator class\n\n        Parameters\n        ----------\n        data : NormData\n            Data object containing prediction results to evaluate, including:\n            - yhat: predicted values\n            - ys2: prediction variances\n            - y: actual values (if available for evaluation)\n\n        Returns\n        -------\n        None\n            Modifies the input data object in-place by adding evaluation metrics:\n            - zscores: standardized residuals\n            - centiles: prediction percentiles\n            - Additional metrics from Evaluator (e.g., MSE, R\u00b2, etc.)\n\n        Notes\n        -----\n        The evaluation process includes:\n        - Z-score computation to identify outliers\n        - Centile computation for distributional analysis\n        - Performance metrics calculation through Evaluator\n\n        The method modifies the input data object by adding computed metrics\n        as new variables/attributes.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # After making predictions\n        &gt;&gt;&gt; predictions = model.predict(test_data)\n        &gt;&gt;&gt; model.evaluate(predictions)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Access evaluation results\n        &gt;&gt;&gt; zscores = predictions.zscores\n        &gt;&gt;&gt; centiles = predictions.centiles\n        &gt;&gt;&gt; metrics = predictions.metrics  # Additional evaluation metrics\n\n        See Also\n        --------\n        compute_zscores : Method for computing standardized residuals\n        compute_centiles : Method for computing prediction percentiles\n        Evaluator : Class handling additional evaluation metrics\n\n        Notes\n        -----\n        Evaluation metrics typically include:\n        - Mean Squared Error (MSE)\n        - R-squared (R\u00b2)\n        - Mean Absolute Error (MAE)\n        - Additional metrics defined in Evaluator\n\n        The exact metrics computed depend on:\n        - Availability of true values (y)\n        - Model type and capabilities\n        - Evaluator configuration\n\n        Warnings\n        --------\n        - Ensure data contains necessary fields for evaluation\n        - Some metrics may be unavailable without true values\n        - Large datasets may require significant computation time\n        \"\"\"\n        data = self.compute_zscores(data)\n        data = self.compute_centiles(data)\n        data = self.evaluator.evaluate(data)\n\n    def compute_centiles(\n        self,\n        data: NormData,\n        cdf: Optional[List | np.ndarray] = None,\n        **kwargs: Any,\n    ) -&gt; NormData:\n        \"\"\"\n        Computes prediction centiles for each response variable in the data.\n\n        This method calculates percentile values for model predictions, providing a way to\n        assess the distribution of predicted values and identify potential outliers.\n\n        Parameters\n        ----------\n        data : NormData\n            Input data containing predictions for which to compute centiles.\n            Must contain:\n            - X: covariates array\n            - yhat: predicted values (if already predicted)\n            - ys2: prediction variances (if applicable)\n\n        cdf : array-like, optional\n            Cumulative distribution function values at which to compute centiles.\n            Default values are [0.05, 0.25, 0.5, 0.75, 0.95], corresponding to:\n            - 5th percentile\n            - 25th percentile (Q1)\n            - 50th percentile (median)\n            - 75th percentile (Q3)\n            - 95th percentile\n\n        **kwargs : Any\n            Additional keyword arguments passed to the underlying _centiles implementation.\n            Model-specific parameters that affect centile computation.\n\n        Returns\n        -------\n        NormData\n            Input data extended with centile computations, adding:\n            - scaled_centiles: centiles in scaled space\n            - centiles: centiles in original space\n            Both arrays have dimensions (cdf, datapoints, response_vars)\n\n        Notes\n        -----\n        The computation process:\n        1. Preprocesses input data (scaling)\n        2. Computes centiles for each response variable\n        3. Postprocesses results (inverse scaling)\n        4. Handles existing centile computations by removing them first\n\n        The method supports both parametric and non-parametric centile computations\n        depending on the underlying model implementation.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Compute default centiles\n        &gt;&gt;&gt; results = model.compute_centiles(prediction_data)\n        &gt;&gt;&gt; centiles = results.centiles\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compute specific centiles\n        &gt;&gt;&gt; custom_centiles = model.compute_centiles(\n        ...     prediction_data,\n        ...     cdf=[0.1, 0.5, 0.9]  # 10th, 50th, 90th percentiles\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Access specific centile (e.g., median)\n        &gt;&gt;&gt; median = results.centiles.sel(cdf=0.5)\n\n        Raises\n        ------\n        ValueError\n            If attempting to compute centiles for a model that hasn't been fitted\n        AssertionError\n            If input data format is invalid or incompatible\n\n        See Also\n        --------\n        compute_zscores : Method for computing standardized scores\n        _centiles : Abstract method implementing specific centile computation\n        scale_forward : Method for data preprocessing\n        scale_backward : Method for data postprocessing\n\n        Notes\n        -----\n        Centile interpretation:\n        - Values below 5th or above 95th percentile may indicate outliers\n        - Median (50th) provides central tendency\n        - Q1-Q3 range (25th-75th) shows typical variation\n\n        Performance considerations:\n        - Computation time scales with number of response variables\n        - Memory usage depends on number of centile points requested\n        - Consider using fewer centile points for large datasets\n        \"\"\"\n\n        self.preprocess(data)\n\n        if cdf is None:\n            cdf = np.array([0.05, 0.25, 0.5, 0.75, 0.95])\n        if isinstance(cdf, list):\n            cdf = np.array(cdf)\n\n        # Drop the centiles and dimensions if they already exist\n        centiles_already_computed = (\n            \"scaled_centiles\" in data or \"centiles\" in data or \"cdf\" in data.coords\n        )\n        if centiles_already_computed:\n            data = data.drop_vars([\"scaled_centiles\", \"centiles\"])\n            data = data.drop_dims([\"cdf\"])\n\n        data[\"scaled_centiles\"] = xr.DataArray(\n            np.zeros((cdf.shape[0], data.X.shape[0], len(self.response_vars))),\n            dims=(\"cdf\", \"datapoints\", \"response_vars\"),\n            coords={\"cdf\": cdf},\n        )\n        for responsevar in self.response_vars:\n            resp_predict_data = data.sel(response_vars=responsevar)\n            if responsevar not in self.regression_models:\n                raise ValueError(\n                    f\"Attempted to find quantiles for model {responsevar}, but it does not exist.\"\n                )\n            self.focus(responsevar)\n            print(\"Computing centiles for\", responsevar)\n            data[\"scaled_centiles\"].loc[{\"response_vars\": responsevar}] = (\n                self._centiles(resp_predict_data, cdf, **kwargs)\n            )\n            self.reset()\n        self.postprocess(data)\n        return data\n\n    def compute_zscores(self, data: NormData) -&gt; NormData:\n        \"\"\"\n        Computes standardized z-scores for each response variable in the data.\n\n        Z-scores represent the number of standard deviations an observation is from the model's\n        predicted mean. The specific computation depends on the underlying regression model.\n\n        Parameters\n        ----------\n        data : NormData\n            Input data containing:\n            - X: covariates array (n_samples, n_features)\n            - y: observed responses (n_samples, n_response_vars)\n            - yhat: predicted means (n_samples, n_response_vars)\n            - ys2: predicted variances (n_samples, n_response_vars)\n\n        Returns\n        -------\n        NormData\n            Input data extended with:\n            - zscores: array of z-scores (n_samples, n_response_vars)\n            Original data structure is preserved with additional z-score information.\n\n        Notes\n        -----\n        The method:\n        1. Preprocesses input data using model's scalers\n        2. Computes z-scores independently for each response variable\n        3. Uses model-specific z-score computation (_zscores abstract method)\n        4. Postprocesses results back to original scale\n\n        Z-scores can be interpreted as:\n        - |z| &lt; 1: observation within 1 SD of prediction\n        - |z| &lt; 2: observation within 2 SD of prediction\n        - |z| &gt; 3: potential outlier\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Compute z-scores for test data\n        &gt;&gt;&gt; test_data = NormData(X=test_covariates, y=test_responses)\n        &gt;&gt;&gt; results = model.compute_zscores(test_data)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Access z-scores\n        &gt;&gt;&gt; zscores = results.zscores\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Identify potential outliers\n        &gt;&gt;&gt; outliers = np.abs(zscores) &gt; 3\n\n        Raises\n        ------\n        ValueError\n            If model hasn't been fitted for any response variable\n\n        See Also\n        --------\n        _zscores : Abstract method implementing specific z-score computation\n        compute_centiles : Method for computing prediction percentiles\n        preprocess : Method for data preprocessing\n        postprocess : Method for data postprocessing\n\n        Notes\n        -----\n        Implementation considerations:\n        - Handles missing data appropriately\n        - Supports multiple response variables\n        - Maintains data dimensionality\n        - Computationally efficient for large datasets\n\n        The z-scores are particularly useful for:\n        - Identifying outliers\n        - Assessing prediction accuracy\n        - Comparing across different response variables\n        - Quality control in clinical applications\n        \"\"\"\n\n        self.preprocess(data)\n        data[\"zscores\"] = xr.DataArray(\n            np.zeros((data.X.shape[0], len(self.response_vars))),\n            dims=(\"datapoints\", \"response_vars\"),\n            coords={\"datapoints\": data.datapoints, \"response_vars\": self.response_vars},\n        )\n        for responsevar in self.response_vars:\n            resp_predict_data = data.sel(response_vars=responsevar)\n            if responsevar not in self.regression_models:\n                raise ValueError(\n                    f\"Attempted to find zscores for self {responsevar}, but it does not exist.\"\n                )\n            self.focus(responsevar)\n            print(\"Computing zscores for\", responsevar)\n            data[\"zscores\"].loc[{\"response_vars\": responsevar}] = self._zscores(\n                resp_predict_data\n            )\n            self.reset()\n        self.postprocess(data)\n        return data\n\n    def preprocess(self, data: NormData) -&gt; None:\n        \"\"\"\n        Applies preprocessing transformations to the input data.\n\n        This method performs two main preprocessing steps:\n        1. Data scaling using configured scalers\n        2. Basis expansion of covariates (if specified)\n\n        Parameters\n        ----------\n        data : NormData\n            Data to preprocess containing:\n            - X: covariates array\n            - y: response variables array (optional)\n            Must be a valid NormData object with proper dimensions.\n\n        Returns\n        -------\n        None\n            Modifies the input data object in-place by adding:\n            - scaled_X: scaled covariates\n            - scaled_y: scaled responses (if y exists)\n            - expanded_X: basis-expanded covariates (if basis_function specified)\n\n        Notes\n        -----\n        Scaling operations:\n        - Creates and fits scalers if they don't exist\n        - Applies existing scalers if already created\n        - Supports different scaling methods via norm_conf.inscaler/outscaler\n\n        Basis expansion options:\n        - B-spline expansion: Creates basis using specified knots and order\n        - Other basis functions as specified in norm_conf.basis_function\n\n        Examples\n        --------\n        &gt;&gt;&gt; data = NormData(X=covariates, y=responses)\n        &gt;&gt;&gt; model.preprocess(data)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Access preprocessed data\n        &gt;&gt;&gt; scaled_X = data.scaled_X\n        &gt;&gt;&gt; scaled_y = data.scaled_y\n        &gt;&gt;&gt; expanded_X = data.expanded_X  # if basis expansion enabled\n\n        See Also\n        --------\n        scale_forward : Method handling data scaling\n        scale_backward : Method for inverse scaling\n        NormData.expand_basis : Method for covariate basis expansion\n\n        Notes\n        -----\n        B-spline basis expansion:\n        - Creates basis only once and reuses for subsequent calls\n        - Basis is created using min/max of specified column\n        - Supports linear component inclusion\n\n        Other basis expansions:\n        - Applied directly without storing basis\n        - Linear component handling configurable\n\n        Warnings\n        --------\n        - Ensure consistent preprocessing between training and test data\n        - B-spline basis requires sufficient data range in basis column\n        - Memory usage increases with basis expansion complexity\n        \"\"\"\n        self.scale_forward(data)\n\n        if self.norm_conf.basis_function == \"bspline\":\n            if not hasattr(self, \"bspline_basis\"):\n                source_array = data.X.isel(covariates=self.norm_conf.basis_column)\n                self.bspline_basis = create_bspline_basis(\n                    source_array.min(),\n                    source_array.max(),\n                    self.norm_conf.order,\n                    self.norm_conf.nknots,\n                )\n            data.expand_basis(\n                \"scaled_X\",\n                self.norm_conf.basis_function,\n                self.norm_conf.basis_column,\n                linear_component=True,\n                bspline_basis=self.bspline_basis,\n            )\n        else:\n            data.expand_basis(\n                \"scaled_X\",\n                self.norm_conf.basis_function,\n                self.norm_conf.basis_column,\n                linear_component=False,\n            )\n\n    def postprocess(self, data: NormData) -&gt; None:\n        \"\"\"Apply postprocessing to the data.\n\n        Args:\n            data (NormData): Data to postprocess.\n        \"\"\"\n        self.scale_backward(data)\n\n    def scale_forward(self, data: NormData, overwrite: bool = False) -&gt; None:\n        \"\"\"\n        Scales input data to standardized form using configured scalers.\n\n        This method handles the forward scaling transformation of both covariates (X)\n        and response variables (y) using separate scalers for each variable. It creates\n        and fits new scalers if they don't exist or if overwrite is True.\n\n        Parameters\n        ----------\n        data : NormData\n            Data object containing arrays to be scaled:\n            - X : array-like, shape (n_samples, n_covariates)\n                Covariate data to be scaled\n            - y : array-like, shape (n_samples, n_response_vars), optional\n                Response variable data to be scaled\n\n        overwrite : bool, default=False\n            If True, creates new scalers even if they already exist.\n            If False, uses existing scalers when available.\n\n        Returns\n        -------\n        None\n            Modifies the input data object in-place by adding:\n            - scaled_X : scaled covariate data\n            - scaled_y : scaled response data (if y exists)\n\n        Notes\n        -----\n        Scaling operations:\n        1. For each covariate:\n            - Creates/retrieves scaler using norm_conf.inscaler type\n            - Fits scaler if new or overwrite=True\n            - Transforms data using scaler\n\n        2. For each response variable:\n            - Creates/retrieves scaler using norm_conf.outscaler type\n            - Fits scaler if new or overwrite=True\n            - Transforms data using scaler\n\n        The scalers are stored in:\n        - self.inscalers : dict mapping covariate names to their scalers\n        - self.outscalers : dict mapping response variable names to their scalers\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Basic usage\n        &gt;&gt;&gt; data = NormData(X=covariates, y=responses)\n        &gt;&gt;&gt; model.scale_forward(data)\n        &gt;&gt;&gt; scaled_X = data.scaled_X\n        &gt;&gt;&gt; scaled_y = data.scaled_y\n\n        &gt;&gt;&gt; # Force new scalers\n        &gt;&gt;&gt; model.scale_forward(data, overwrite=True)\n\n        See Also\n        --------\n        scale_backward : Method for inverse scaling transformation\n        NormData : Class containing data structures\n        scaler : Factory function for creating scalers\n\n        Notes\n        -----\n        Supported scaler types (configured in norm_conf):\n        - 'StandardScaler': zero mean, unit variance\n        - 'MinMaxScaler': scales to specified range\n        - 'RobustScaler': scales using statistics robust to outliers\n        - Custom scalers implementing fit/transform interface\n\n        References\n        ----------\n        .. [1] Scikit-learn preprocessing: https://scikit-learn.org/stable/modules/preprocessing.html\n        \"\"\"\n        for covariate in data.covariates.to_numpy():\n            if (covariate not in self.inscalers) or overwrite:\n                self.inscalers[covariate] = Scaler.from_string(self.norm_conf.inscaler)\n                self.inscalers[covariate].fit(data.X.sel(covariates=covariate).data)\n\n        for responsevar in data.response_vars.to_numpy():\n            if (responsevar not in self.outscalers) or overwrite:\n                self.outscalers[responsevar] = Scaler.from_string(self.norm_conf.outscaler)\n                self.outscalers[responsevar].fit(\n                    data.y.sel(response_vars=responsevar).data\n                )\n\n        data.scale_forward(self.inscalers, self.outscalers)\n\n    def scale_backward(self, data: NormData) -&gt; None:\n        \"\"\"\n        Scales data back to its original scale using stored scalers.\n\n        This method performs inverse scaling transformation on the data using previously\n        fitted scalers. It reverses the scaling applied during preprocessing to return\n        predictions and other computed values to their original scale.\n\n        Parameters\n        ----------\n        data : NormData\n            Data object containing scaled values to be transformed back. May include:\n            - scaled_X: scaled covariates\n            - scaled_y: scaled responses\n            - scaled_yhat: scaled predictions\n            - scaled_ys2: scaled prediction variances\n            - scaled_centiles: scaled prediction centiles\n\n        Returns\n        -------\n        None\n            Modifies the input data object in-place by adding or updating:\n            - X: unscaled covariates\n            - y: unscaled responses\n            - yhat: unscaled predictions\n            - ys2: unscaled prediction variances\n            - centiles: unscaled prediction centiles\n\n        Notes\n        -----\n        The method:\n        - Uses stored inscalers for covariates\n        - Uses stored outscalers for responses and predictions\n        - Preserves the original data structure and coordinates\n        - Handles missing data appropriately\n        - Maintains data consistency across all variables\n\n        The inverse scaling is applied to all relevant variables that were previously\n        scaled during preprocessing. The original scalers must have been created and\n        stored during the forward scaling process.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Assuming model has been fitted and predictions made\n        &gt;&gt;&gt; predictions = model.predict(test_data)\n        &gt;&gt;&gt; # Scale predictions back to original scale\n        &gt;&gt;&gt; model.scale_backward(predictions)\n        &gt;&gt;&gt; # Access unscaled predictions\n        &gt;&gt;&gt; unscaled_yhat = predictions.yhat\n        &gt;&gt;&gt; unscaled_ys2 = predictions.ys2\n\n        See Also\n        --------\n        scale_forward : Method for forward scaling of data\n        preprocess : Method handling complete preprocessing pipeline\n        NormData.scale_backward : Underlying scaling implementation\n\n        Warnings\n        --------\n        - Requires scalers to be previously fitted\n        - May produce unexpected results if applied multiple times\n        - Should be used only on data scaled with corresponding forward scalers\n        \"\"\"\n        data.scale_backward(self.inscalers, self.outscalers)\n\n    def save(self, path: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Save the normative model to disk.\n\n        This method serializes the entire normative model, including configuration,\n        fitted regression models, scalers, and basis functions (if applicable).\n\n        Parameters\n        ----------\n        path : str , optional\n            Directory path where the model should be saved. If None, uses the\n            path specified in norm_conf.save_dir. Default is None.\n\n        Returns\n        -------\n        None\n            Saves files to disk but doesn't return any value.\n\n        Notes\n        -----\n        The method saves the following components:\n        1. Metadata JSON file ('normative_model_dict.json') containing:\n            - Normative model configuration\n            - Response variable names\n            - Regression model type\n            - Default regression configuration\n            - B-spline basis parameters (if used)\n            - Input/output scaler configurations\n\n        2. Individual regression model files:\n            - One JSON file per response variable\n            - Named as 'model_{response_var}.json'\n            - Contains model-specific parameters and states\n\n        File Structure\n        -------------\n        save_dir/\n        \u251c\u2500\u2500 normative_model_dict.json\n        \u251c\u2500\u2500 model_response1.json\n        \u251c\u2500\u2500 model_response2.json\n        \u2514\u2500\u2500 ...\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Save model to default location\n        &gt;&gt;&gt; model.save()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Save model to specific path\n        &gt;&gt;&gt; model.save(\"/path/to/save/directory\")\n\n        See Also\n        --------\n        load : Class method for loading a saved model\n        set_save_dir : Method to change the save directory\n\n        Notes\n        -----\n        - Ensures thread-safety when multiple models save to same directory\n        - Maintains backward compatibility with older saved models\n        - Handles complex nested objects through JSON serialization\n        - Preserves all necessary information for model reconstruction\n\n        Warnings\n        --------\n        - Ensure sufficient disk space before saving large models\n        - Avoid modifying saved files manually to prevent corruption\n        - Consider backup strategies for important models\n        \"\"\"\n        # TODO save the model in such a format that parallel models with identical save dirs result in a single model\n        metadata = {\n            \"norm_conf\": self.norm_conf.to_dict(),\n            \"response_vars\": self.response_vars,\n            \"regression_model_type\": self.regression_model_type.__name__,\n            \"default_reg_conf\": self.default_reg_conf.to_dict(),\n        }\n        if self.norm_conf.basis_function == \"bspline\" and hasattr(\n            self, \"bspline_basis\"\n        ):\n            knots = self.bspline_basis.knot_vector\n            metadata[\"bspline_basis\"] = {\n                \"xmin\": knots[0],\n                \"xmax\": knots[-1],\n                \"nknots\": self.norm_conf.nknots,\n                \"p\": self.norm_conf.order,\n            }\n        metadata[\"inscalers\"] = {k: v.to_dict() for k, v in self.inscalers.items()}\n        metadata[\"outscalers\"] = {k: v.to_dict() for k, v in self.outscalers.items()}\n\n        if path is not None:\n            self.norm_conf.set_save_dir(path)\n        with open(\n            os.path.join(self.norm_conf.save_dir, \"normative_model_dict.json\"),\n            mode=\"w\",\n            encoding=\"utf-8\",\n        ) as f:\n            json.dump(metadata, f, indent=4)\n        for responsevar, model in self.regression_models.items():\n            model_dict = model.to_dict(self.norm_conf.save_dir)\n            with open(\n                os.path.join(self.norm_conf.save_dir, f\"model_{responsevar}.json\"),\n                mode=\"w\",\n                encoding=\"utf-8\",\n            ) as f:\n                json.dump(model_dict, f, indent=4)\n\n    @classmethod\n    def load(cls, path: str) -&gt; NormBase:\n        \"\"\"\n        Load a normative model from disk.\n\n        This class method reconstructs a normative model from saved files, including model\n        configuration, regression models, scalers, and basis functions.\n\n        Parameters\n        ----------\n        path : str\n            Directory path containing the saved model files. Must contain:\n            - normative_model_dict.json: Model metadata and configuration\n            - model_{response_var}.json: Individual regression model files\n            - Additional model-specific files referenced in the JSONs\n\n        Returns\n        -------\n        NormBase\n            Reconstructed normative model instance with:\n            - Loaded configuration settings\n            - Reconstructed regression models\n            - Restored preprocessing transformations\n            - Recreated basis functions (if applicable)\n\n        Notes\n        -----\n        Loading process:\n        1. Reads model metadata from normative_model_dict.json\n        2. Reconstructs configuration objects\n        3. Loads individual regression models\n        4. Recreates preprocessing transformations\n        5. Rebuilds basis functions if used\n\n        The method expects a specific directory structure and file format:\n        ```\n        path/\n        \u251c\u2500\u2500 normative_model_dict.json\n        \u251c\u2500\u2500 model_response1.json\n        \u251c\u2500\u2500 model_response2.json\n        \u2514\u2500\u2500 ...\n        ```\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Save model\n        &gt;&gt;&gt; original_model.save(\"./saved_model\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load model\n        &gt;&gt;&gt; loaded_model = NormBase.load(\"./saved_model\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use loaded model\n        &gt;&gt;&gt; predictions = loaded_model.predict(test_data)\n\n        Raises\n        ------\n        FileNotFoundError\n            If required model files are missing\n        JSONDecodeError\n            If model files are corrupted or improperly formatted\n        ValueError\n            If model configuration is invalid or incompatible\n        ImportError\n            If required model classes cannot be imported\n\n        See Also\n        --------\n        save : Method for saving model to disk\n        NormConf : Configuration class for normative models\n        RegressionModel : Base class for regression models\n\n        Notes\n        -----\n        Loaded components:\n        - Model configuration (NormConf)\n        - Response variables list\n        - Regression model type and instances\n        - Input/output scalers\n        - B-spline basis (if used)\n        - Default regression configuration\n\n        The method supports loading:\n        - Different regression model types (BLR, GPR, HBR)\n        - Various preprocessing configurations\n        - Multiple response variables\n        - Custom basis functions\n\n        Compatibility considerations:\n        - Python version compatibility\n        - Package version compatibility\n        - Hardware/platform independence\n        - Backward compatibility with older saved models\n        \"\"\"\n        with open(\n            os.path.join(path, \"normative_model_dict.json\"), mode=\"r\", encoding=\"utf-8\"\n        ) as f:\n            metadata = json.load(f)\n\n        self = cls(NormConf.from_dict(metadata[\"norm_conf\"]))\n        self.response_vars = metadata[\"response_vars\"]\n        self.regression_model_type = globals()[metadata[\"regression_model_type\"]]\n        if \"bspline_basis\" in metadata:\n            self.bspline_basis = create_bspline_basis(**metadata[\"bspline_basis\"])\n        self.inscalers = {\n            k: Scaler.from_dict(v) for k, v in metadata[\"inscalers\"].items()\n        }\n        self.outscalers = {\n            k: Scaler.from_dict(v) for k, v in metadata[\"outscalers\"].items()\n        }\n        self.regression_models = {}\n        for responsevar in self.response_vars:\n            model_path = os.path.join(path, f\"model_{responsevar}.json\")\n            with open(model_path, mode=\"r\", encoding=\"utf-8\") as f:\n                model_dict = json.load(f)\n            self.regression_models[responsevar] = self.regression_model_type.from_dict(\n                model_dict, path\n            )\n        self.default_reg_conf = type(\n            self.regression_models[self.response_vars[0]].reg_conf\n        ).from_dict(metadata[\"default_reg_conf\"])\n        return self\n\n    def focus(self, responsevar: str) -&gt; None:\n        \"\"\"\n        Prepares the model for operations on a specific response variable by setting up\n        the corresponding regression model.\n\n        This method serves as an internal state manager that:\n        1. Sets the current focused response variable\n        2. Creates a new regression model if one doesn't exist for this variable\n        3. Makes the focused model easily accessible for subsequent operations\n\n        Parameters\n        ----------\n        responsevar : str\n            The name of the response variable to focus on. Must be one of the variables\n            present in the training data.\n\n        Returns\n        -------\n        None\n            Modifies the internal state of the normative model by:\n            - Setting self.focused_var\n            - Creating/accessing self.regression_models[responsevar]\n            - Setting self.focused_model reference\n\n        Notes\n        -----\n        This method is typically called before performing operations that work with\n        individual response variables, such as:\n        - Model fitting\n        - Prediction\n        - Transfer learning\n        - Model evaluation\n\n        The method implements a lazy initialization strategy for regression models,\n        creating them only when needed.\n\n        Examples\n        --------\n        &gt;&gt;&gt; model = NormBase(norm_conf)\n        &gt;&gt;&gt; model.focus('brain_region_1')\n        &gt;&gt;&gt; # Now model.focused_model refers to the regression model for 'brain_region_1'\n        &gt;&gt;&gt; model.focused_model.fit(data)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Switch focus to another variable\n        &gt;&gt;&gt; model.focus('brain_region_2')\n        &gt;&gt;&gt; # Now working with the model for 'brain_region_2'\n        &gt;&gt;&gt; predictions = model.focused_model.predict(test_data)\n\n        See Also\n        --------\n        reset : Method to clear the current focus\n        get_reg_conf : Method to get regression configuration for a response variable\n\n        Notes\n        -----\n        - The focused model is accessible through self.focused_model property\n        - New regression models are initialized with default configuration\n        - Focus state persists until explicitly changed or reset\n        - Thread safety should be considered in concurrent operations\n\n        Warnings\n        --------\n        - Ensure responsevar exists in the training data\n        - Be mindful of memory usage when creating many regression models\n        - Consider resetting focus when switching between response variables\n        \"\"\"\n        self.focused_var = responsevar\n        if responsevar not in self.regression_models:\n            self.regression_models[responsevar] = self.regression_model_type(\n                responsevar, self.get_reg_conf(responsevar)\n            )\n        self.focused_model = self.regression_models.get(responsevar, None)  # type: ignore\n\n    def get_reg_conf(self, responsevar: str) -&gt; RegConf:\n        \"\"\"\n        Get regression configuration for a specific response variable.\n\n        This method retrieves the regression configuration for a given response variable,\n        either returning an existing configuration if the model exists, or the default\n        configuration if it doesn't.\n\n        Parameters\n        ----------\n        responsevar : str\n            Name of the response variable to get configuration for.\n\n        Returns\n        -------\n        RegConf\n            Regression configuration object for the specified response variable.\n\n        Notes\n        -----\n        The method implements a simple lookup strategy:\n        1. Check if model exists for response variable\n        2. If yes, return its configuration\n        3. If no, return default configuration\n\n        Examples\n        --------\n        &gt;&gt;&gt; model = NormBase(norm_conf)\n        &gt;&gt;&gt; # Get config for existing model\n        &gt;&gt;&gt; config = model.get_reg_conf('brain_region_1')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get default config for new variable\n        &gt;&gt;&gt; default_config = model.get_reg_conf('new_region')\n\n        See Also\n        --------\n        focus : Method to set focus on a response variable\n        RegConf : Regression configuration class\n        \"\"\"\n        if responsevar in self.regression_models:\n            return self.regression_models[responsevar].reg_conf\n        else:\n            return self.default_reg_conf\n\n    def set_save_dir(self, save_dir: str) -&gt; None:\n        \"\"\"Override the save_dir in the norm_conf.\n\n        Args:\n            save_dir (str): New save directory.\n        \"\"\"\n        self.norm_conf.set_save_dir(save_dir)\n\n    def set_log_dir(self, log_dir: str) -&gt; None:\n        \"\"\"Override the log_dir in the norm_conf.\n\n        Args:\n            log_dir (str): New log directory.\n        \"\"\"\n        self.norm_conf.set_log_dir(log_dir)\n\n    def reset(self) -&gt; None:\n        \"\"\"Does nothing. Can be overridden by subclasses.\"\"\"\n\n    def __getitem__(self, key: str) -&gt; RegressionModel:\n        return self.regression_models[key]\n\n    #######################################################################################################\n\n    # Abstract methods\n\n    #######################################################################################################\n\n    @abstractmethod\n    def _fit(self, data: NormData, make_new_model: bool = False) -&gt; None:\n        \"\"\"\n        Fits the specific regression model to the provided data.\n\n        Parameters\n        ----------\n        data : NormData\n            The data to fit the model to, containing covariates and response variables.\n        make_new_model : bool, optional\n            If True, creates a new model instance for fitting. Default is False.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        NotImplementedError\n            If the method is not implemented in a subclass.\n\n        Examples\n        --------\n        &gt;&gt;&gt; model = ConcreteNormModel(norm_conf)\n        &gt;&gt;&gt; model._fit(training_data)\n        \"\"\"\n\n    @abstractmethod\n    def _predict(self, data: NormData) -&gt; None:\n        \"\"\"\n        Predicts response variables using the fitted regression model.\n\n        Parameters\n        ----------\n        data : NormData\n            The data for which to make predictions, containing covariates.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        NotImplementedError\n            If the method is not implemented in a subclass.\n\n        Examples\n        --------\n        &gt;&gt;&gt; model = ConcreteNormModel(norm_conf)\n        &gt;&gt;&gt; model._predict(test_data)\n        \"\"\"\n\n    @abstractmethod\n    def _fit_predict(self, fit_data: NormData, predict_data: NormData) -&gt; None:\n        \"\"\"\n        Fits the model to the fit_data and predicts on the predict_data.\n\n        Parameters\n        ----------\n        fit_data : NormData\n            The data to fit the model to, containing covariates and response variables.\n        predict_data : NormData\n            The data for which to make predictions, containing covariates.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        NotImplementedError\n            If the method is not implemented in a subclass.\n\n        Examples\n        --------\n        &gt;&gt;&gt; model = ConcreteNormModel(norm_conf)\n        &gt;&gt;&gt; model._fit_predict(training_data, test_data)\n        \"\"\"\n\n    @abstractmethod\n    def _transfer(self, data: NormData, **kwargs: Any) -&gt; RegressionModel:\n        \"\"\"\n        Transfers the current regression model to new data.\n\n        Parameters\n        ----------\n        data : NormData\n            The data to transfer the model to, containing covariates and response variables.\n        **kwargs : Any\n            Additional keyword arguments for the transfer process.\n\n        Returns\n        -------\n        RegressionModel\n            A new regression model adapted to the new data.\n\n        Raises\n        ------\n        NotImplementedError\n            If the method is not implemented in a subclass.\n\n        Examples\n        --------\n        &gt;&gt;&gt; model = ConcreteNormModel(norm_conf)\n        &gt;&gt;&gt; new_model = model._transfer(transfer_data)\n        \"\"\"\n\n    @abstractmethod\n    def _extend(self, data: NormData) -&gt; NormBase:\n        \"\"\"\n        Extends the current regression model with new data.\n\n        Parameters\n        ----------\n        data : NormData\n            The data to extend the model with, containing covariates and response variables.\n\n        Returns\n        -------\n        NormBase\n            The extended normative model.\n\n        Raises\n        ------\n        NotImplementedError\n            If the method is not implemented in a subclass.\n\n        Examples\n        --------\n        &gt;&gt;&gt; model = ConcreteNormModel(norm_conf)\n        &gt;&gt;&gt; extended_model = model._extend(additional_data)\n        \"\"\"\n\n    @abstractmethod\n    def _tune(self, data: NormData) -&gt; NormBase:\n        \"\"\"\n        Tunes the model parameters using the provided data.\n\n        Parameters\n        ----------\n        data : NormData\n            The data to use for tuning the model, containing covariates and response variables.\n\n        Returns\n        -------\n        NormBase\n            The tuned normative model.\n\n        Raises\n        ------\n        NotImplementedError\n            If the method is not implemented in a subclass.\n\n        Examples\n        --------\n        &gt;&gt;&gt; model = ConcreteNormModel(norm_conf)\n        &gt;&gt;&gt; tuned_model = model._tune(validation_data)\n        \"\"\"\n\n    @abstractmethod\n    def _merge(self, other: NormBase) -&gt; NormBase:\n        \"\"\"\n        Merges the current model with another normative model.\n\n        Parameters\n        ----------\n        other : NormBase\n            The other normative model to merge with.\n\n        Returns\n        -------\n        NormBase\n            The merged normative model.\n\n        Raises\n        ------\n        NotImplementedError\n            If the method is not implemented in a subclass.\n\n        Examples\n        --------\n        &gt;&gt;&gt; model1 = ConcreteNormModel(norm_conf)\n        &gt;&gt;&gt; model2 = ConcreteNormModel(norm_conf)\n        &gt;&gt;&gt; merged_model = model1._merge(model2)\n        \"\"\"\n\n    @abstractmethod\n    def _centiles(self, data: NormData, cdf: np.ndarray, **kwargs: Any) -&gt; xr.DataArray:\n        \"\"\"Computes centiles of the model predictions for given cumulative density values.\n\n        Parameters\n        ----------\n        data : NormData\n            Data object containing model predictions to compute centiles for\n        cdf : np.ndarray\n            Array of cumulative density values to compute centiles at (between 0 and 1)\n        **kwargs : Any\n            Additional keyword arguments passed to the centile computation\n\n        Returns\n        -------\n        xr.DataArray\n            DataArray containing the computed centiles with dimensions (cdf, datapoints)\n            where:\n            - cdf dimension corresponds to the input cumulative density values\n            - datapoints dimension corresponds to the samples in the input data\n\n        Notes\n        -----\n        Centiles represent the values below which a given percentage of observations fall.\n        For example, the 50th centile is the median value.\n\n        The centile computation depends on the specific regression model implementation\n        and its underlying distributional assumptions.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Compute 25th, 50th and 75th centiles\n        &gt;&gt;&gt; centiles = model._centiles(data, np.array([0.25, 0.5, 0.75]))\n        &gt;&gt;&gt; median = centiles.sel(cdf=0.5)\n        \"\"\"\n\n    @abstractmethod\n    def _zscores(self, data: NormData) -&gt; xr.DataArray:\n        \"\"\"Computes standardized residuals (z-scores) for model predictions.\n\n        Parameters\n        ----------\n        data : NormData\n            Data object containing model predictions to compute z-scores for,\n            including predicted values (yhat), prediction variances (ys2),\n            and actual values (y)\n\n        Returns\n        -------\n        xr.DataArray\n            DataArray containing the computed z-scores with dimensions (datapoints)\n            where datapoints corresponds to the samples in the input data\n\n        Notes\n        -----\n        Z-scores measure how many standard deviations an observation is from the mean\n        of the predicted distribution. They are useful for:\n        - Identifying outliers\n        - Assessing prediction accuracy\n        - Standardizing residuals across different scales\n\n        The computation depends on the specific regression model implementation\n        and its underlying distributional assumptions.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Compute z-scores for predictions\n        &gt;&gt;&gt; zscores = model._zscores(predictions)\n        &gt;&gt;&gt; outliers = abs(zscores) &gt; 2  # Find outliers beyond 2 SD\n        \"\"\"\n\n    @abstractmethod\n    def n_params(self) -&gt; int:\n        \"\"\"\n        Returns the number of parameters of the model.\n\n        Returns\n        -------\n        int\n            The number of parameters in the model.\n\n        Raises\n        ------\n        NotImplementedError\n            If the method is not implemented in a subclass.\n\n        Examples\n        --------\n        &gt;&gt;&gt; num_params = model.n_params()\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def from_args(cls, args: dict) -&gt; NormBase:\n        \"\"\"\n        Creates a normative model from command line arguments.\n\n        Parameters\n        ----------\n        args : dict\n            Dictionary of command line arguments.\n\n        Returns\n        -------\n        NormBase\n            An instance of the normative model.\n\n        Raises\n        ------\n        NotImplementedError\n            If the method is not implemented in a subclass.\n\n        Examples\n        --------\n        &gt;&gt;&gt; args = {'param1': value1, 'param2': value2}\n        &gt;&gt;&gt; model = ConcreteNormModel.from_args(args)\n        \"\"\"\n\n    #######################################################################################################\n\n    # Properties\n\n    #######################################################################################################\n\n    @property\n    def norm_conf(self) -&gt; NormConf:\n        \"\"\"Returns the norm_conf attribute.\n\n        Returns:\n            NormConf: The norm_conf attribute.\n        \"\"\"\n        return self._norm_conf\n\n    @property\n    def has_random_effect(self) -&gt; bool:\n        \"\"\"Returns whether the model has a random effect.\n\n        Returns:\n            bool: True if the model has a random effect, False otherwise.\n        \"\"\"\n        return self.focused_model.has_random_effect\n\n    @property\n    def focused_model(self) -&gt; RegressionModel:\n        \"\"\"Returns the regression model that is currently focused on.\"\"\"\n        return self[self.focused_var]\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.focused_model","title":"<code>focused_model: RegressionModel</code>  <code>property</code>","text":"<p>Returns the regression model that is currently focused on.</p>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.has_random_effect","title":"<code>has_random_effect: bool</code>  <code>property</code>","text":"<p>Returns whether the model has a random effect.</p> <p>Returns:     bool: True if the model has a random effect, False otherwise.</p>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.norm_conf","title":"<code>norm_conf: NormConf</code>  <code>property</code>","text":"<p>Returns the norm_conf attribute.</p> <p>Returns:     NormConf: The norm_conf attribute.</p>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.compute_centiles","title":"<code>compute_centiles(data: NormData, cdf: Optional[List | np.ndarray] = None, **kwargs: Any) -&gt; NormData</code>","text":"<p>Computes prediction centiles for each response variable in the data.</p> <p>This method calculates percentile values for model predictions, providing a way to assess the distribution of predicted values and identify potential outliers.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NormData</code> <p>Input data containing predictions for which to compute centiles. Must contain: - X: covariates array - yhat: predicted values (if already predicted) - ys2: prediction variances (if applicable)</p> required <code>cdf</code> <code>array - like</code> <p>Cumulative distribution function values at which to compute centiles. Default values are [0.05, 0.25, 0.5, 0.75, 0.95], corresponding to: - 5th percentile - 25th percentile (Q1) - 50th percentile (median) - 75th percentile (Q3) - 95th percentile</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the underlying _centiles implementation. Model-specific parameters that affect centile computation.</p> <code>{}</code> <p>Returns:</p> Type Description <code>NormData</code> <p>Input data extended with centile computations, adding: - scaled_centiles: centiles in scaled space - centiles: centiles in original space Both arrays have dimensions (cdf, datapoints, response_vars)</p> Notes <p>The computation process: 1. Preprocesses input data (scaling) 2. Computes centiles for each response variable 3. Postprocesses results (inverse scaling) 4. Handles existing centile computations by removing them first</p> <p>The method supports both parametric and non-parametric centile computations depending on the underlying model implementation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Compute default centiles\n&gt;&gt;&gt; results = model.compute_centiles(prediction_data)\n&gt;&gt;&gt; centiles = results.centiles\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compute specific centiles\n&gt;&gt;&gt; custom_centiles = model.compute_centiles(\n...     prediction_data,\n...     cdf=[0.1, 0.5, 0.9]  # 10th, 50th, 90th percentiles\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Access specific centile (e.g., median)\n&gt;&gt;&gt; median = results.centiles.sel(cdf=0.5)\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If attempting to compute centiles for a model that hasn't been fitted</p> <code>AssertionError</code> <p>If input data format is invalid or incompatible</p> See Also <p>compute_zscores : Method for computing standardized scores _centiles : Abstract method implementing specific centile computation scale_forward : Method for data preprocessing scale_backward : Method for data postprocessing</p> Notes <p>Centile interpretation: - Values below 5th or above 95th percentile may indicate outliers - Median (50th) provides central tendency - Q1-Q3 range (25th-75th) shows typical variation</p> <p>Performance considerations: - Computation time scales with number of response variables - Memory usage depends on number of centile points requested - Consider using fewer centile points for large datasets</p> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def compute_centiles(\n    self,\n    data: NormData,\n    cdf: Optional[List | np.ndarray] = None,\n    **kwargs: Any,\n) -&gt; NormData:\n    \"\"\"\n    Computes prediction centiles for each response variable in the data.\n\n    This method calculates percentile values for model predictions, providing a way to\n    assess the distribution of predicted values and identify potential outliers.\n\n    Parameters\n    ----------\n    data : NormData\n        Input data containing predictions for which to compute centiles.\n        Must contain:\n        - X: covariates array\n        - yhat: predicted values (if already predicted)\n        - ys2: prediction variances (if applicable)\n\n    cdf : array-like, optional\n        Cumulative distribution function values at which to compute centiles.\n        Default values are [0.05, 0.25, 0.5, 0.75, 0.95], corresponding to:\n        - 5th percentile\n        - 25th percentile (Q1)\n        - 50th percentile (median)\n        - 75th percentile (Q3)\n        - 95th percentile\n\n    **kwargs : Any\n        Additional keyword arguments passed to the underlying _centiles implementation.\n        Model-specific parameters that affect centile computation.\n\n    Returns\n    -------\n    NormData\n        Input data extended with centile computations, adding:\n        - scaled_centiles: centiles in scaled space\n        - centiles: centiles in original space\n        Both arrays have dimensions (cdf, datapoints, response_vars)\n\n    Notes\n    -----\n    The computation process:\n    1. Preprocesses input data (scaling)\n    2. Computes centiles for each response variable\n    3. Postprocesses results (inverse scaling)\n    4. Handles existing centile computations by removing them first\n\n    The method supports both parametric and non-parametric centile computations\n    depending on the underlying model implementation.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Compute default centiles\n    &gt;&gt;&gt; results = model.compute_centiles(prediction_data)\n    &gt;&gt;&gt; centiles = results.centiles\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Compute specific centiles\n    &gt;&gt;&gt; custom_centiles = model.compute_centiles(\n    ...     prediction_data,\n    ...     cdf=[0.1, 0.5, 0.9]  # 10th, 50th, 90th percentiles\n    ... )\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Access specific centile (e.g., median)\n    &gt;&gt;&gt; median = results.centiles.sel(cdf=0.5)\n\n    Raises\n    ------\n    ValueError\n        If attempting to compute centiles for a model that hasn't been fitted\n    AssertionError\n        If input data format is invalid or incompatible\n\n    See Also\n    --------\n    compute_zscores : Method for computing standardized scores\n    _centiles : Abstract method implementing specific centile computation\n    scale_forward : Method for data preprocessing\n    scale_backward : Method for data postprocessing\n\n    Notes\n    -----\n    Centile interpretation:\n    - Values below 5th or above 95th percentile may indicate outliers\n    - Median (50th) provides central tendency\n    - Q1-Q3 range (25th-75th) shows typical variation\n\n    Performance considerations:\n    - Computation time scales with number of response variables\n    - Memory usage depends on number of centile points requested\n    - Consider using fewer centile points for large datasets\n    \"\"\"\n\n    self.preprocess(data)\n\n    if cdf is None:\n        cdf = np.array([0.05, 0.25, 0.5, 0.75, 0.95])\n    if isinstance(cdf, list):\n        cdf = np.array(cdf)\n\n    # Drop the centiles and dimensions if they already exist\n    centiles_already_computed = (\n        \"scaled_centiles\" in data or \"centiles\" in data or \"cdf\" in data.coords\n    )\n    if centiles_already_computed:\n        data = data.drop_vars([\"scaled_centiles\", \"centiles\"])\n        data = data.drop_dims([\"cdf\"])\n\n    data[\"scaled_centiles\"] = xr.DataArray(\n        np.zeros((cdf.shape[0], data.X.shape[0], len(self.response_vars))),\n        dims=(\"cdf\", \"datapoints\", \"response_vars\"),\n        coords={\"cdf\": cdf},\n    )\n    for responsevar in self.response_vars:\n        resp_predict_data = data.sel(response_vars=responsevar)\n        if responsevar not in self.regression_models:\n            raise ValueError(\n                f\"Attempted to find quantiles for model {responsevar}, but it does not exist.\"\n            )\n        self.focus(responsevar)\n        print(\"Computing centiles for\", responsevar)\n        data[\"scaled_centiles\"].loc[{\"response_vars\": responsevar}] = (\n            self._centiles(resp_predict_data, cdf, **kwargs)\n        )\n        self.reset()\n    self.postprocess(data)\n    return data\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.compute_zscores","title":"<code>compute_zscores(data: NormData) -&gt; NormData</code>","text":"<p>Computes standardized z-scores for each response variable in the data.</p> <p>Z-scores represent the number of standard deviations an observation is from the model's predicted mean. The specific computation depends on the underlying regression model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NormData</code> <p>Input data containing: - X: covariates array (n_samples, n_features) - y: observed responses (n_samples, n_response_vars) - yhat: predicted means (n_samples, n_response_vars) - ys2: predicted variances (n_samples, n_response_vars)</p> required <p>Returns:</p> Type Description <code>NormData</code> <p>Input data extended with: - zscores: array of z-scores (n_samples, n_response_vars) Original data structure is preserved with additional z-score information.</p> Notes <p>The method: 1. Preprocesses input data using model's scalers 2. Computes z-scores independently for each response variable 3. Uses model-specific z-score computation (_zscores abstract method) 4. Postprocesses results back to original scale</p> <p>Z-scores can be interpreted as: - |z| &lt; 1: observation within 1 SD of prediction - |z| &lt; 2: observation within 2 SD of prediction - |z| &gt; 3: potential outlier</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Compute z-scores for test data\n&gt;&gt;&gt; test_data = NormData(X=test_covariates, y=test_responses)\n&gt;&gt;&gt; results = model.compute_zscores(test_data)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Access z-scores\n&gt;&gt;&gt; zscores = results.zscores\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Identify potential outliers\n&gt;&gt;&gt; outliers = np.abs(zscores) &gt; 3\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model hasn't been fitted for any response variable</p> See Also <p>_zscores : Abstract method implementing specific z-score computation compute_centiles : Method for computing prediction percentiles preprocess : Method for data preprocessing postprocess : Method for data postprocessing</p> Notes <p>Implementation considerations: - Handles missing data appropriately - Supports multiple response variables - Maintains data dimensionality - Computationally efficient for large datasets</p> <p>The z-scores are particularly useful for: - Identifying outliers - Assessing prediction accuracy - Comparing across different response variables - Quality control in clinical applications</p> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def compute_zscores(self, data: NormData) -&gt; NormData:\n    \"\"\"\n    Computes standardized z-scores for each response variable in the data.\n\n    Z-scores represent the number of standard deviations an observation is from the model's\n    predicted mean. The specific computation depends on the underlying regression model.\n\n    Parameters\n    ----------\n    data : NormData\n        Input data containing:\n        - X: covariates array (n_samples, n_features)\n        - y: observed responses (n_samples, n_response_vars)\n        - yhat: predicted means (n_samples, n_response_vars)\n        - ys2: predicted variances (n_samples, n_response_vars)\n\n    Returns\n    -------\n    NormData\n        Input data extended with:\n        - zscores: array of z-scores (n_samples, n_response_vars)\n        Original data structure is preserved with additional z-score information.\n\n    Notes\n    -----\n    The method:\n    1. Preprocesses input data using model's scalers\n    2. Computes z-scores independently for each response variable\n    3. Uses model-specific z-score computation (_zscores abstract method)\n    4. Postprocesses results back to original scale\n\n    Z-scores can be interpreted as:\n    - |z| &lt; 1: observation within 1 SD of prediction\n    - |z| &lt; 2: observation within 2 SD of prediction\n    - |z| &gt; 3: potential outlier\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Compute z-scores for test data\n    &gt;&gt;&gt; test_data = NormData(X=test_covariates, y=test_responses)\n    &gt;&gt;&gt; results = model.compute_zscores(test_data)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Access z-scores\n    &gt;&gt;&gt; zscores = results.zscores\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Identify potential outliers\n    &gt;&gt;&gt; outliers = np.abs(zscores) &gt; 3\n\n    Raises\n    ------\n    ValueError\n        If model hasn't been fitted for any response variable\n\n    See Also\n    --------\n    _zscores : Abstract method implementing specific z-score computation\n    compute_centiles : Method for computing prediction percentiles\n    preprocess : Method for data preprocessing\n    postprocess : Method for data postprocessing\n\n    Notes\n    -----\n    Implementation considerations:\n    - Handles missing data appropriately\n    - Supports multiple response variables\n    - Maintains data dimensionality\n    - Computationally efficient for large datasets\n\n    The z-scores are particularly useful for:\n    - Identifying outliers\n    - Assessing prediction accuracy\n    - Comparing across different response variables\n    - Quality control in clinical applications\n    \"\"\"\n\n    self.preprocess(data)\n    data[\"zscores\"] = xr.DataArray(\n        np.zeros((data.X.shape[0], len(self.response_vars))),\n        dims=(\"datapoints\", \"response_vars\"),\n        coords={\"datapoints\": data.datapoints, \"response_vars\": self.response_vars},\n    )\n    for responsevar in self.response_vars:\n        resp_predict_data = data.sel(response_vars=responsevar)\n        if responsevar not in self.regression_models:\n            raise ValueError(\n                f\"Attempted to find zscores for self {responsevar}, but it does not exist.\"\n            )\n        self.focus(responsevar)\n        print(\"Computing zscores for\", responsevar)\n        data[\"zscores\"].loc[{\"response_vars\": responsevar}] = self._zscores(\n            resp_predict_data\n        )\n        self.reset()\n    self.postprocess(data)\n    return data\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.evaluate","title":"<code>evaluate(data: NormData) -&gt; None</code>","text":"<p>Evaluates model performance by computing z-scores, centiles, and additional evaluation metrics.</p> <p>This method performs a comprehensive evaluation of the normative model by: 1. Computing standardized residuals (z-scores) 2. Computing prediction centiles 3. Calculating various evaluation metrics through the Evaluator class</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NormData</code> <p>Data object containing prediction results to evaluate, including: - yhat: predicted values - ys2: prediction variances - y: actual values (if available for evaluation)</p> required <p>Returns:</p> Type Description <code>None</code> <p>Modifies the input data object in-place by adding evaluation metrics: - zscores: standardized residuals - centiles: prediction percentiles - Additional metrics from Evaluator (e.g., MSE, R\u00b2, etc.)</p> Notes <p>The evaluation process includes: - Z-score computation to identify outliers - Centile computation for distributional analysis - Performance metrics calculation through Evaluator</p> <p>The method modifies the input data object by adding computed metrics as new variables/attributes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # After making predictions\n&gt;&gt;&gt; predictions = model.predict(test_data)\n&gt;&gt;&gt; model.evaluate(predictions)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Access evaluation results\n&gt;&gt;&gt; zscores = predictions.zscores\n&gt;&gt;&gt; centiles = predictions.centiles\n&gt;&gt;&gt; metrics = predictions.metrics  # Additional evaluation metrics\n</code></pre> See Also <p>compute_zscores : Method for computing standardized residuals compute_centiles : Method for computing prediction percentiles Evaluator : Class handling additional evaluation metrics</p> Notes <p>Evaluation metrics typically include: - Mean Squared Error (MSE) - R-squared (R\u00b2) - Mean Absolute Error (MAE) - Additional metrics defined in Evaluator</p> <p>The exact metrics computed depend on: - Availability of true values (y) - Model type and capabilities - Evaluator configuration</p> Warnings <ul> <li>Ensure data contains necessary fields for evaluation</li> <li>Some metrics may be unavailable without true values</li> <li>Large datasets may require significant computation time</li> </ul> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def evaluate(self, data: NormData) -&gt; None:\n    \"\"\"\n    Evaluates model performance by computing z-scores, centiles, and additional evaluation metrics.\n\n    This method performs a comprehensive evaluation of the normative model by:\n    1. Computing standardized residuals (z-scores)\n    2. Computing prediction centiles\n    3. Calculating various evaluation metrics through the Evaluator class\n\n    Parameters\n    ----------\n    data : NormData\n        Data object containing prediction results to evaluate, including:\n        - yhat: predicted values\n        - ys2: prediction variances\n        - y: actual values (if available for evaluation)\n\n    Returns\n    -------\n    None\n        Modifies the input data object in-place by adding evaluation metrics:\n        - zscores: standardized residuals\n        - centiles: prediction percentiles\n        - Additional metrics from Evaluator (e.g., MSE, R\u00b2, etc.)\n\n    Notes\n    -----\n    The evaluation process includes:\n    - Z-score computation to identify outliers\n    - Centile computation for distributional analysis\n    - Performance metrics calculation through Evaluator\n\n    The method modifies the input data object by adding computed metrics\n    as new variables/attributes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # After making predictions\n    &gt;&gt;&gt; predictions = model.predict(test_data)\n    &gt;&gt;&gt; model.evaluate(predictions)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Access evaluation results\n    &gt;&gt;&gt; zscores = predictions.zscores\n    &gt;&gt;&gt; centiles = predictions.centiles\n    &gt;&gt;&gt; metrics = predictions.metrics  # Additional evaluation metrics\n\n    See Also\n    --------\n    compute_zscores : Method for computing standardized residuals\n    compute_centiles : Method for computing prediction percentiles\n    Evaluator : Class handling additional evaluation metrics\n\n    Notes\n    -----\n    Evaluation metrics typically include:\n    - Mean Squared Error (MSE)\n    - R-squared (R\u00b2)\n    - Mean Absolute Error (MAE)\n    - Additional metrics defined in Evaluator\n\n    The exact metrics computed depend on:\n    - Availability of true values (y)\n    - Model type and capabilities\n    - Evaluator configuration\n\n    Warnings\n    --------\n    - Ensure data contains necessary fields for evaluation\n    - Some metrics may be unavailable without true values\n    - Large datasets may require significant computation time\n    \"\"\"\n    data = self.compute_zscores(data)\n    data = self.compute_centiles(data)\n    data = self.evaluator.evaluate(data)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.extend","title":"<code>extend(data: NormData) -&gt; None</code>","text":"<p>Extends the normative model with new data.</p> <p>Args:     data (NormData): Data containing the covariates and response variables to extend the model with.</p> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def extend(self, data: NormData) -&gt; None:\n    \"\"\"Extends the normative model with new data.\n\n    Args:\n        data (NormData): Data containing the covariates and response variables to extend the model with.\n    \"\"\"\n    self._extend(data)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.fit","title":"<code>fit(data: NormData) -&gt; None</code>","text":"<p>Fits a regression model for each response variable in the data.</p> <p>This method performs the following steps: 1. Preprocesses the input data (scaling and basis expansion) 2. Extracts response variables 3. Fits individual regression models for each response variable</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NormData</code> <p>Training data containing covariates (X) and response variables (y). Must be a valid NormData object with properly formatted dimensions: - X: (n_samples, n_covariates) - y: (n_samples, n_response_vars)</p> required <p>Returns:</p> Type Description <code>None</code> <p>The method modifies the model's internal state by fitting regression models.</p> Notes <ul> <li>The method fits one regression model per response variable</li> <li>Each model is stored in self.regression_models with response variable name as key</li> <li>Preprocessing includes scaling and basis expansion based on norm_conf settings</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pcntoolkit.dataio import NormData\n&gt;&gt;&gt; model = NormBase(norm_conf)\n&gt;&gt;&gt; train_data = NormData(X=covariates, y=responses)\n&gt;&gt;&gt; model.fit(train_data)\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data is not properly formatted or contains invalid values</p> <code>RuntimeError</code> <p>If preprocessing or model fitting fails</p> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def fit(self, data: NormData) -&gt; None:\n    \"\"\"\n    Fits a regression model for each response variable in the data.\n\n    This method performs the following steps:\n    1. Preprocesses the input data (scaling and basis expansion)\n    2. Extracts response variables\n    3. Fits individual regression models for each response variable\n\n    Parameters\n    ----------\n    data : NormData\n        Training data containing covariates (X) and response variables (y).\n        Must be a valid NormData object with properly formatted dimensions:\n        - X: (n_samples, n_covariates)\n        - y: (n_samples, n_response_vars)\n\n    Returns\n    -------\n    None\n        The method modifies the model's internal state by fitting regression models.\n\n    Notes\n    -----\n    - The method fits one regression model per response variable\n    - Each model is stored in self.regression_models with response variable name as key\n    - Preprocessing includes scaling and basis expansion based on norm_conf settings\n\n    Examples\n    --------\n    &gt;&gt;&gt; from pcntoolkit.dataio import NormData\n    &gt;&gt;&gt; model = NormBase(norm_conf)\n    &gt;&gt;&gt; train_data = NormData(X=covariates, y=responses)\n    &gt;&gt;&gt; model.fit(train_data)\n\n    Raises\n    ------\n    ValueError\n        If data is not properly formatted or contains invalid values\n    RuntimeError\n        If preprocessing or model fitting fails\n    \"\"\"\n    self.preprocess(data)\n    self.response_vars = data.response_vars.to_numpy().copy().tolist()\n    print(f\"Going to fit {len(self.response_vars)} models\")\n    for responsevar in self.response_vars:\n        resp_fit_data = data.sel(response_vars=responsevar)\n        self.focus(responsevar)\n        print(f\"Fitting model for {responsevar}\")\n        self._fit(resp_fit_data)\n        self.reset()\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.fit_predict","title":"<code>fit_predict(fit_data: NormData, predict_data: NormData) -&gt; NormData</code>","text":"<p>Combines model fitting and prediction in a single operation.</p> <p>This method provides a convenient way to fit models and generate predictions in one step, which can be more efficient than separate fit() and predict() calls.</p> <p>Parameters:</p> Name Type Description Default <code>fit_data</code> <code>NormData</code> <p>Training data containing covariates (X) and response variables (y) for model fitting. Must be a valid NormData object with dimensions: - X: (n_train_samples, n_covariates) - y: (n_train_samples, n_response_vars)</p> required <code>predict_data</code> <code>NormData</code> <p>Test data containing covariates (X) for prediction. Must have the same covariate structure as fit_data with dimensions: - X: (n_test_samples, n_covariates)</p> required <p>Returns:</p> Type Description <code>NormData</code> <p>Prediction results containing: - yhat: predicted values (n_test_samples, n_response_vars) - ys2: prediction variances (n_test_samples, n_response_vars) - zscores: standardized residuals - centiles: prediction percentiles - Additional evaluation metrics</p> Notes <ul> <li>Performs compatibility check between fit_data and predict_data</li> <li>More memory efficient than separate fit() and predict() calls</li> <li>Automatically handles preprocessing and postprocessing</li> <li>Computes evaluation metrics after prediction</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; train_data = NormData(X=train_covariates, y=train_responses)\n&gt;&gt;&gt; test_data = NormData(X=test_covariates)\n&gt;&gt;&gt; results = model.fit_predict(train_data, test_data)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Access predictions\n&gt;&gt;&gt; predictions = results.yhat\n&gt;&gt;&gt; variances = results.ys2\n&gt;&gt;&gt; zscores = results.zscores\n&gt;&gt;&gt; centiles = results.centiles\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data formats are incompatible or invalid</p> <code>AssertionError</code> <p>If fit_data and predict_data have incompatible structures</p> <code>RuntimeError</code> <p>If fitting or prediction process fails</p> See Also <p>fit : Method for model fitting only predict : Method for prediction using pre-fitted model compute_zscores : Method for computing standardized residuals compute_centiles : Method for computing prediction percentiles</p> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def fit_predict(self, fit_data: NormData, predict_data: NormData) -&gt; NormData:\n    \"\"\"\n    Combines model fitting and prediction in a single operation.\n\n    This method provides a convenient way to fit models and generate predictions\n    in one step, which can be more efficient than separate fit() and predict()\n    calls.\n\n    Parameters\n    ----------\n    fit_data : NormData\n        Training data containing covariates (X) and response variables (y)\n        for model fitting. Must be a valid NormData object with dimensions:\n        - X: (n_train_samples, n_covariates)\n        - y: (n_train_samples, n_response_vars)\n\n    predict_data : NormData\n        Test data containing covariates (X) for prediction. Must have the same\n        covariate structure as fit_data with dimensions:\n        - X: (n_test_samples, n_covariates)\n\n    Returns\n    -------\n    NormData\n        Prediction results containing:\n        - yhat: predicted values (n_test_samples, n_response_vars)\n        - ys2: prediction variances (n_test_samples, n_response_vars)\n        - zscores: standardized residuals\n        - centiles: prediction percentiles\n        - Additional evaluation metrics\n\n    Notes\n    -----\n    - Performs compatibility check between fit_data and predict_data\n    - More memory efficient than separate fit() and predict() calls\n    - Automatically handles preprocessing and postprocessing\n    - Computes evaluation metrics after prediction\n\n    Examples\n    --------\n    &gt;&gt;&gt; train_data = NormData(X=train_covariates, y=train_responses)\n    &gt;&gt;&gt; test_data = NormData(X=test_covariates)\n    &gt;&gt;&gt; results = model.fit_predict(train_data, test_data)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Access predictions\n    &gt;&gt;&gt; predictions = results.yhat\n    &gt;&gt;&gt; variances = results.ys2\n    &gt;&gt;&gt; zscores = results.zscores\n    &gt;&gt;&gt; centiles = results.centiles\n\n    Raises\n    ------\n    ValueError\n        If data formats are incompatible or invalid\n    AssertionError\n        If fit_data and predict_data have incompatible structures\n    RuntimeError\n        If fitting or prediction process fails\n\n    See Also\n    --------\n    fit : Method for model fitting only\n    predict : Method for prediction using pre-fitted model\n    compute_zscores : Method for computing standardized residuals\n    compute_centiles : Method for computing prediction percentiles\n    \"\"\"\n\n    assert fit_data.is_compatible_with(\n        predict_data\n    ), \"Fit data and predict data are not compatible!\"\n\n    self.preprocess(fit_data)\n    self.preprocess(predict_data)\n    self.response_vars = fit_data.response_vars.to_numpy().copy().tolist()\n    print(f\"Going to fit and predict {len(self.response_vars)} models\")\n    for responsevar in self.response_vars:\n        resp_fit_data = fit_data.sel(response_vars=responsevar)\n        resp_predict_data = predict_data.sel(response_vars=responsevar)\n        if responsevar not in self.regression_models:\n            self.regression_models[responsevar] = self.regression_model_type(\n                responsevar, self.default_reg_conf\n            )\n        self.focus(responsevar)\n        print(f\"Fitting and predicting model for {responsevar}\")\n        self._fit_predict(resp_fit_data, resp_predict_data)\n\n        self.reset()\n\n    # Get the results\n    self.evaluate(predict_data)\n    return predict_data\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.focus","title":"<code>focus(responsevar: str) -&gt; None</code>","text":"<p>Prepares the model for operations on a specific response variable by setting up the corresponding regression model.</p> <p>This method serves as an internal state manager that: 1. Sets the current focused response variable 2. Creates a new regression model if one doesn't exist for this variable 3. Makes the focused model easily accessible for subsequent operations</p> <p>Parameters:</p> Name Type Description Default <code>responsevar</code> <code>str</code> <p>The name of the response variable to focus on. Must be one of the variables present in the training data.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Modifies the internal state of the normative model by: - Setting self.focused_var - Creating/accessing self.regression_models[responsevar] - Setting self.focused_model reference</p> Notes <p>This method is typically called before performing operations that work with individual response variables, such as: - Model fitting - Prediction - Transfer learning - Model evaluation</p> <p>The method implements a lazy initialization strategy for regression models, creating them only when needed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = NormBase(norm_conf)\n&gt;&gt;&gt; model.focus('brain_region_1')\n&gt;&gt;&gt; # Now model.focused_model refers to the regression model for 'brain_region_1'\n&gt;&gt;&gt; model.focused_model.fit(data)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Switch focus to another variable\n&gt;&gt;&gt; model.focus('brain_region_2')\n&gt;&gt;&gt; # Now working with the model for 'brain_region_2'\n&gt;&gt;&gt; predictions = model.focused_model.predict(test_data)\n</code></pre> See Also <p>reset : Method to clear the current focus get_reg_conf : Method to get regression configuration for a response variable</p> Notes <ul> <li>The focused model is accessible through self.focused_model property</li> <li>New regression models are initialized with default configuration</li> <li>Focus state persists until explicitly changed or reset</li> <li>Thread safety should be considered in concurrent operations</li> </ul> Warnings <ul> <li>Ensure responsevar exists in the training data</li> <li>Be mindful of memory usage when creating many regression models</li> <li>Consider resetting focus when switching between response variables</li> </ul> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def focus(self, responsevar: str) -&gt; None:\n    \"\"\"\n    Prepares the model for operations on a specific response variable by setting up\n    the corresponding regression model.\n\n    This method serves as an internal state manager that:\n    1. Sets the current focused response variable\n    2. Creates a new regression model if one doesn't exist for this variable\n    3. Makes the focused model easily accessible for subsequent operations\n\n    Parameters\n    ----------\n    responsevar : str\n        The name of the response variable to focus on. Must be one of the variables\n        present in the training data.\n\n    Returns\n    -------\n    None\n        Modifies the internal state of the normative model by:\n        - Setting self.focused_var\n        - Creating/accessing self.regression_models[responsevar]\n        - Setting self.focused_model reference\n\n    Notes\n    -----\n    This method is typically called before performing operations that work with\n    individual response variables, such as:\n    - Model fitting\n    - Prediction\n    - Transfer learning\n    - Model evaluation\n\n    The method implements a lazy initialization strategy for regression models,\n    creating them only when needed.\n\n    Examples\n    --------\n    &gt;&gt;&gt; model = NormBase(norm_conf)\n    &gt;&gt;&gt; model.focus('brain_region_1')\n    &gt;&gt;&gt; # Now model.focused_model refers to the regression model for 'brain_region_1'\n    &gt;&gt;&gt; model.focused_model.fit(data)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Switch focus to another variable\n    &gt;&gt;&gt; model.focus('brain_region_2')\n    &gt;&gt;&gt; # Now working with the model for 'brain_region_2'\n    &gt;&gt;&gt; predictions = model.focused_model.predict(test_data)\n\n    See Also\n    --------\n    reset : Method to clear the current focus\n    get_reg_conf : Method to get regression configuration for a response variable\n\n    Notes\n    -----\n    - The focused model is accessible through self.focused_model property\n    - New regression models are initialized with default configuration\n    - Focus state persists until explicitly changed or reset\n    - Thread safety should be considered in concurrent operations\n\n    Warnings\n    --------\n    - Ensure responsevar exists in the training data\n    - Be mindful of memory usage when creating many regression models\n    - Consider resetting focus when switching between response variables\n    \"\"\"\n    self.focused_var = responsevar\n    if responsevar not in self.regression_models:\n        self.regression_models[responsevar] = self.regression_model_type(\n            responsevar, self.get_reg_conf(responsevar)\n        )\n    self.focused_model = self.regression_models.get(responsevar, None)  # type: ignore\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.from_args","title":"<code>from_args(args: dict) -&gt; NormBase</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Creates a normative model from command line arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>dict</code> <p>Dictionary of command line arguments.</p> required <p>Returns:</p> Type Description <code>NormBase</code> <p>An instance of the normative model.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented in a subclass.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; args = {'param1': value1, 'param2': value2}\n&gt;&gt;&gt; model = ConcreteNormModel.from_args(args)\n</code></pre> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_args(cls, args: dict) -&gt; NormBase:\n    \"\"\"\n    Creates a normative model from command line arguments.\n\n    Parameters\n    ----------\n    args : dict\n        Dictionary of command line arguments.\n\n    Returns\n    -------\n    NormBase\n        An instance of the normative model.\n\n    Raises\n    ------\n    NotImplementedError\n        If the method is not implemented in a subclass.\n\n    Examples\n    --------\n    &gt;&gt;&gt; args = {'param1': value1, 'param2': value2}\n    &gt;&gt;&gt; model = ConcreteNormModel.from_args(args)\n    \"\"\"\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.get_reg_conf","title":"<code>get_reg_conf(responsevar: str) -&gt; RegConf</code>","text":"<p>Get regression configuration for a specific response variable.</p> <p>This method retrieves the regression configuration for a given response variable, either returning an existing configuration if the model exists, or the default configuration if it doesn't.</p> <p>Parameters:</p> Name Type Description Default <code>responsevar</code> <code>str</code> <p>Name of the response variable to get configuration for.</p> required <p>Returns:</p> Type Description <code>RegConf</code> <p>Regression configuration object for the specified response variable.</p> Notes <p>The method implements a simple lookup strategy: 1. Check if model exists for response variable 2. If yes, return its configuration 3. If no, return default configuration</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = NormBase(norm_conf)\n&gt;&gt;&gt; # Get config for existing model\n&gt;&gt;&gt; config = model.get_reg_conf('brain_region_1')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get default config for new variable\n&gt;&gt;&gt; default_config = model.get_reg_conf('new_region')\n</code></pre> See Also <p>focus : Method to set focus on a response variable RegConf : Regression configuration class</p> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def get_reg_conf(self, responsevar: str) -&gt; RegConf:\n    \"\"\"\n    Get regression configuration for a specific response variable.\n\n    This method retrieves the regression configuration for a given response variable,\n    either returning an existing configuration if the model exists, or the default\n    configuration if it doesn't.\n\n    Parameters\n    ----------\n    responsevar : str\n        Name of the response variable to get configuration for.\n\n    Returns\n    -------\n    RegConf\n        Regression configuration object for the specified response variable.\n\n    Notes\n    -----\n    The method implements a simple lookup strategy:\n    1. Check if model exists for response variable\n    2. If yes, return its configuration\n    3. If no, return default configuration\n\n    Examples\n    --------\n    &gt;&gt;&gt; model = NormBase(norm_conf)\n    &gt;&gt;&gt; # Get config for existing model\n    &gt;&gt;&gt; config = model.get_reg_conf('brain_region_1')\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Get default config for new variable\n    &gt;&gt;&gt; default_config = model.get_reg_conf('new_region')\n\n    See Also\n    --------\n    focus : Method to set focus on a response variable\n    RegConf : Regression configuration class\n    \"\"\"\n    if responsevar in self.regression_models:\n        return self.regression_models[responsevar].reg_conf\n    else:\n        return self.default_reg_conf\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.load","title":"<code>load(path: str) -&gt; NormBase</code>  <code>classmethod</code>","text":"<p>Load a normative model from disk.</p> <p>This class method reconstructs a normative model from saved files, including model configuration, regression models, scalers, and basis functions.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path containing the saved model files. Must contain: - normative_model_dict.json: Model metadata and configuration - model_{response_var}.json: Individual regression model files - Additional model-specific files referenced in the JSONs</p> required <p>Returns:</p> Type Description <code>NormBase</code> <p>Reconstructed normative model instance with: - Loaded configuration settings - Reconstructed regression models - Restored preprocessing transformations - Recreated basis functions (if applicable)</p> Notes <p>Loading process: 1. Reads model metadata from normative_model_dict.json 2. Reconstructs configuration objects 3. Loads individual regression models 4. Recreates preprocessing transformations 5. Rebuilds basis functions if used</p> <p>The method expects a specific directory structure and file format: <pre><code>path/\n\u251c\u2500\u2500 normative_model_dict.json\n\u251c\u2500\u2500 model_response1.json\n\u251c\u2500\u2500 model_response2.json\n\u2514\u2500\u2500 ...\n</code></pre></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Save model\n&gt;&gt;&gt; original_model.save(\"./saved_model\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load model\n&gt;&gt;&gt; loaded_model = NormBase.load(\"./saved_model\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use loaded model\n&gt;&gt;&gt; predictions = loaded_model.predict(test_data)\n</code></pre> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If required model files are missing</p> <code>JSONDecodeError</code> <p>If model files are corrupted or improperly formatted</p> <code>ValueError</code> <p>If model configuration is invalid or incompatible</p> <code>ImportError</code> <p>If required model classes cannot be imported</p> See Also <p>save : Method for saving model to disk NormConf : Configuration class for normative models RegressionModel : Base class for regression models</p> Notes <p>Loaded components: - Model configuration (NormConf) - Response variables list - Regression model type and instances - Input/output scalers - B-spline basis (if used) - Default regression configuration</p> <p>The method supports loading: - Different regression model types (BLR, GPR, HBR) - Various preprocessing configurations - Multiple response variables - Custom basis functions</p> <p>Compatibility considerations: - Python version compatibility - Package version compatibility - Hardware/platform independence - Backward compatibility with older saved models</p> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>@classmethod\ndef load(cls, path: str) -&gt; NormBase:\n    \"\"\"\n    Load a normative model from disk.\n\n    This class method reconstructs a normative model from saved files, including model\n    configuration, regression models, scalers, and basis functions.\n\n    Parameters\n    ----------\n    path : str\n        Directory path containing the saved model files. Must contain:\n        - normative_model_dict.json: Model metadata and configuration\n        - model_{response_var}.json: Individual regression model files\n        - Additional model-specific files referenced in the JSONs\n\n    Returns\n    -------\n    NormBase\n        Reconstructed normative model instance with:\n        - Loaded configuration settings\n        - Reconstructed regression models\n        - Restored preprocessing transformations\n        - Recreated basis functions (if applicable)\n\n    Notes\n    -----\n    Loading process:\n    1. Reads model metadata from normative_model_dict.json\n    2. Reconstructs configuration objects\n    3. Loads individual regression models\n    4. Recreates preprocessing transformations\n    5. Rebuilds basis functions if used\n\n    The method expects a specific directory structure and file format:\n    ```\n    path/\n    \u251c\u2500\u2500 normative_model_dict.json\n    \u251c\u2500\u2500 model_response1.json\n    \u251c\u2500\u2500 model_response2.json\n    \u2514\u2500\u2500 ...\n    ```\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Save model\n    &gt;&gt;&gt; original_model.save(\"./saved_model\")\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Load model\n    &gt;&gt;&gt; loaded_model = NormBase.load(\"./saved_model\")\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Use loaded model\n    &gt;&gt;&gt; predictions = loaded_model.predict(test_data)\n\n    Raises\n    ------\n    FileNotFoundError\n        If required model files are missing\n    JSONDecodeError\n        If model files are corrupted or improperly formatted\n    ValueError\n        If model configuration is invalid or incompatible\n    ImportError\n        If required model classes cannot be imported\n\n    See Also\n    --------\n    save : Method for saving model to disk\n    NormConf : Configuration class for normative models\n    RegressionModel : Base class for regression models\n\n    Notes\n    -----\n    Loaded components:\n    - Model configuration (NormConf)\n    - Response variables list\n    - Regression model type and instances\n    - Input/output scalers\n    - B-spline basis (if used)\n    - Default regression configuration\n\n    The method supports loading:\n    - Different regression model types (BLR, GPR, HBR)\n    - Various preprocessing configurations\n    - Multiple response variables\n    - Custom basis functions\n\n    Compatibility considerations:\n    - Python version compatibility\n    - Package version compatibility\n    - Hardware/platform independence\n    - Backward compatibility with older saved models\n    \"\"\"\n    with open(\n        os.path.join(path, \"normative_model_dict.json\"), mode=\"r\", encoding=\"utf-8\"\n    ) as f:\n        metadata = json.load(f)\n\n    self = cls(NormConf.from_dict(metadata[\"norm_conf\"]))\n    self.response_vars = metadata[\"response_vars\"]\n    self.regression_model_type = globals()[metadata[\"regression_model_type\"]]\n    if \"bspline_basis\" in metadata:\n        self.bspline_basis = create_bspline_basis(**metadata[\"bspline_basis\"])\n    self.inscalers = {\n        k: Scaler.from_dict(v) for k, v in metadata[\"inscalers\"].items()\n    }\n    self.outscalers = {\n        k: Scaler.from_dict(v) for k, v in metadata[\"outscalers\"].items()\n    }\n    self.regression_models = {}\n    for responsevar in self.response_vars:\n        model_path = os.path.join(path, f\"model_{responsevar}.json\")\n        with open(model_path, mode=\"r\", encoding=\"utf-8\") as f:\n            model_dict = json.load(f)\n        self.regression_models[responsevar] = self.regression_model_type.from_dict(\n            model_dict, path\n        )\n    self.default_reg_conf = type(\n        self.regression_models[self.response_vars[0]].reg_conf\n    ).from_dict(metadata[\"default_reg_conf\"])\n    return self\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.merge","title":"<code>merge(other: 'NormBase') -&gt; None</code>","text":"<p>Merges the current normative model with another normative model.</p> <p>Args:     other (NormBase): The other normative model to merge with.</p> <p>Raises:     ValueError: Error if the models are not of the same type.</p> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def merge(self, other: \"NormBase\") -&gt; None:\n    \"\"\"Merges the current normative model with another normative model.\n\n    Args:\n        other (NormBase): The other normative model to merge with.\n\n    Raises:\n        ValueError: Error if the models are not of the same type.\n    \"\"\"\n    if not self.__class__ == other.__class__:\n        raise ValueError(\"Attempted to merge normative models of different types.\")\n    self._merge(other)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.n_params","title":"<code>n_params() -&gt; int</code>  <code>abstractmethod</code>","text":"<p>Returns the number of parameters of the model.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of parameters in the model.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented in a subclass.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; num_params = model.n_params()\n</code></pre> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>@abstractmethod\ndef n_params(self) -&gt; int:\n    \"\"\"\n    Returns the number of parameters of the model.\n\n    Returns\n    -------\n    int\n        The number of parameters in the model.\n\n    Raises\n    ------\n    NotImplementedError\n        If the method is not implemented in a subclass.\n\n    Examples\n    --------\n    &gt;&gt;&gt; num_params = model.n_params()\n    \"\"\"\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.postprocess","title":"<code>postprocess(data: NormData) -&gt; None</code>","text":"<p>Apply postprocessing to the data.</p> <p>Args:     data (NormData): Data to postprocess.</p> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def postprocess(self, data: NormData) -&gt; None:\n    \"\"\"Apply postprocessing to the data.\n\n    Args:\n        data (NormData): Data to postprocess.\n    \"\"\"\n    self.scale_backward(data)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.predict","title":"<code>predict(data: NormData) -&gt; NormData</code>","text":"<p>Makes predictions for each response variable using fitted regression models.</p> <p>This method performs the following steps: 1. Preprocesses the input data 2. Generates predictions for each response variable 3. Evaluates prediction performance 4. Postprocesses the predictions</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NormData</code> <p>Test data containing covariates (X) for which to generate predictions. Must have the same covariate structure as training data.</p> required <p>Returns:</p> Type Description <code>NormData</code> <p>Prediction results containing: - yhat: predicted values - ys2: prediction variances (if applicable) - Additional metrics (z-scores, centiles, etc.)</p> Notes <ul> <li>Requires models to be previously fitted using fit()</li> <li>Predictions are made independently for each response variable</li> <li>Automatically computes evaluation metrics after prediction</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; test_data = NormData(X=test_covariates)\n&gt;&gt;&gt; predictions = model.predict(test_data)\n&gt;&gt;&gt; print(predictions.yhat)  # access predictions\n&gt;&gt;&gt; print(predictions.ys2)   # access variances\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model hasn't been fitted or data format is invalid</p> <code>RuntimeError</code> <p>If prediction process fails for any response variable</p> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def predict(self, data: NormData) -&gt; NormData:\n    \"\"\"\n    Makes predictions for each response variable using fitted regression models.\n\n    This method performs the following steps:\n    1. Preprocesses the input data\n    2. Generates predictions for each response variable\n    3. Evaluates prediction performance\n    4. Postprocesses the predictions\n\n    Parameters\n    ----------\n    data : NormData\n        Test data containing covariates (X) for which to generate predictions.\n        Must have the same covariate structure as training data.\n\n    Returns\n    -------\n    NormData\n        Prediction results containing:\n        - yhat: predicted values\n        - ys2: prediction variances (if applicable)\n        - Additional metrics (z-scores, centiles, etc.)\n\n    Notes\n    -----\n    - Requires models to be previously fitted using fit()\n    - Predictions are made independently for each response variable\n    - Automatically computes evaluation metrics after prediction\n\n    Examples\n    --------\n    &gt;&gt;&gt; test_data = NormData(X=test_covariates)\n    &gt;&gt;&gt; predictions = model.predict(test_data)\n    &gt;&gt;&gt; print(predictions.yhat)  # access predictions\n    &gt;&gt;&gt; print(predictions.ys2)   # access variances\n\n    Raises\n    ------\n    ValueError\n        If model hasn't been fitted or data format is invalid\n    RuntimeError\n        If prediction process fails for any response variable\n    \"\"\"\n    self.preprocess(data)\n    print(f\"Going to predict {len(self.response_vars)} models\")\n    for responsevar in self.response_vars:\n        resp_predict_data = data.sel(response_vars=responsevar)\n        if responsevar not in self.regression_models:\n            raise ValueError(\n                f\"Attempted to predict model {responsevar}, but it does not exist.\"\n            )\n        self.focus(responsevar)\n        print(f\"Predicting model for {responsevar}\")\n        self._predict(resp_predict_data)\n        self.reset()\n    self.evaluate(data)\n    return data\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.preprocess","title":"<code>preprocess(data: NormData) -&gt; None</code>","text":"<p>Applies preprocessing transformations to the input data.</p> <p>This method performs two main preprocessing steps: 1. Data scaling using configured scalers 2. Basis expansion of covariates (if specified)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NormData</code> <p>Data to preprocess containing: - X: covariates array - y: response variables array (optional) Must be a valid NormData object with proper dimensions.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Modifies the input data object in-place by adding: - scaled_X: scaled covariates - scaled_y: scaled responses (if y exists) - expanded_X: basis-expanded covariates (if basis_function specified)</p> Notes <p>Scaling operations: - Creates and fits scalers if they don't exist - Applies existing scalers if already created - Supports different scaling methods via norm_conf.inscaler/outscaler</p> <p>Basis expansion options: - B-spline expansion: Creates basis using specified knots and order - Other basis functions as specified in norm_conf.basis_function</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = NormData(X=covariates, y=responses)\n&gt;&gt;&gt; model.preprocess(data)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Access preprocessed data\n&gt;&gt;&gt; scaled_X = data.scaled_X\n&gt;&gt;&gt; scaled_y = data.scaled_y\n&gt;&gt;&gt; expanded_X = data.expanded_X  # if basis expansion enabled\n</code></pre> See Also <p>scale_forward : Method handling data scaling scale_backward : Method for inverse scaling NormData.expand_basis : Method for covariate basis expansion</p> Notes <p>B-spline basis expansion: - Creates basis only once and reuses for subsequent calls - Basis is created using min/max of specified column - Supports linear component inclusion</p> <p>Other basis expansions: - Applied directly without storing basis - Linear component handling configurable</p> Warnings <ul> <li>Ensure consistent preprocessing between training and test data</li> <li>B-spline basis requires sufficient data range in basis column</li> <li>Memory usage increases with basis expansion complexity</li> </ul> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def preprocess(self, data: NormData) -&gt; None:\n    \"\"\"\n    Applies preprocessing transformations to the input data.\n\n    This method performs two main preprocessing steps:\n    1. Data scaling using configured scalers\n    2. Basis expansion of covariates (if specified)\n\n    Parameters\n    ----------\n    data : NormData\n        Data to preprocess containing:\n        - X: covariates array\n        - y: response variables array (optional)\n        Must be a valid NormData object with proper dimensions.\n\n    Returns\n    -------\n    None\n        Modifies the input data object in-place by adding:\n        - scaled_X: scaled covariates\n        - scaled_y: scaled responses (if y exists)\n        - expanded_X: basis-expanded covariates (if basis_function specified)\n\n    Notes\n    -----\n    Scaling operations:\n    - Creates and fits scalers if they don't exist\n    - Applies existing scalers if already created\n    - Supports different scaling methods via norm_conf.inscaler/outscaler\n\n    Basis expansion options:\n    - B-spline expansion: Creates basis using specified knots and order\n    - Other basis functions as specified in norm_conf.basis_function\n\n    Examples\n    --------\n    &gt;&gt;&gt; data = NormData(X=covariates, y=responses)\n    &gt;&gt;&gt; model.preprocess(data)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Access preprocessed data\n    &gt;&gt;&gt; scaled_X = data.scaled_X\n    &gt;&gt;&gt; scaled_y = data.scaled_y\n    &gt;&gt;&gt; expanded_X = data.expanded_X  # if basis expansion enabled\n\n    See Also\n    --------\n    scale_forward : Method handling data scaling\n    scale_backward : Method for inverse scaling\n    NormData.expand_basis : Method for covariate basis expansion\n\n    Notes\n    -----\n    B-spline basis expansion:\n    - Creates basis only once and reuses for subsequent calls\n    - Basis is created using min/max of specified column\n    - Supports linear component inclusion\n\n    Other basis expansions:\n    - Applied directly without storing basis\n    - Linear component handling configurable\n\n    Warnings\n    --------\n    - Ensure consistent preprocessing between training and test data\n    - B-spline basis requires sufficient data range in basis column\n    - Memory usage increases with basis expansion complexity\n    \"\"\"\n    self.scale_forward(data)\n\n    if self.norm_conf.basis_function == \"bspline\":\n        if not hasattr(self, \"bspline_basis\"):\n            source_array = data.X.isel(covariates=self.norm_conf.basis_column)\n            self.bspline_basis = create_bspline_basis(\n                source_array.min(),\n                source_array.max(),\n                self.norm_conf.order,\n                self.norm_conf.nknots,\n            )\n        data.expand_basis(\n            \"scaled_X\",\n            self.norm_conf.basis_function,\n            self.norm_conf.basis_column,\n            linear_component=True,\n            bspline_basis=self.bspline_basis,\n        )\n    else:\n        data.expand_basis(\n            \"scaled_X\",\n            self.norm_conf.basis_function,\n            self.norm_conf.basis_column,\n            linear_component=False,\n        )\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.reset","title":"<code>reset() -&gt; None</code>","text":"<p>Does nothing. Can be overridden by subclasses.</p> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Does nothing. Can be overridden by subclasses.\"\"\"\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.save","title":"<code>save(path: Optional[str] = None) -&gt; None</code>","text":"<p>Save the normative model to disk.</p> <p>This method serializes the entire normative model, including configuration, fitted regression models, scalers, and basis functions (if applicable).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path where the model should be saved. If None, uses the path specified in norm_conf.save_dir. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Saves files to disk but doesn't return any value.</p> Notes <p>The method saves the following components: 1. Metadata JSON file ('normative_model_dict.json') containing:     - Normative model configuration     - Response variable names     - Regression model type     - Default regression configuration     - B-spline basis parameters (if used)     - Input/output scaler configurations</p> <ol> <li>Individual regression model files:<ul> <li>One JSON file per response variable</li> <li>Named as 'model_{response_var}.json'</li> <li>Contains model-specific parameters and states</li> </ul> </li> </ol> File Structure <p>save_dir/ \u251c\u2500\u2500 normative_model_dict.json \u251c\u2500\u2500 model_response1.json \u251c\u2500\u2500 model_response2.json \u2514\u2500\u2500 ...</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Save model to default location\n&gt;&gt;&gt; model.save()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Save model to specific path\n&gt;&gt;&gt; model.save(\"/path/to/save/directory\")\n</code></pre> See Also <p>load : Class method for loading a saved model set_save_dir : Method to change the save directory</p> Notes <ul> <li>Ensures thread-safety when multiple models save to same directory</li> <li>Maintains backward compatibility with older saved models</li> <li>Handles complex nested objects through JSON serialization</li> <li>Preserves all necessary information for model reconstruction</li> </ul> Warnings <ul> <li>Ensure sufficient disk space before saving large models</li> <li>Avoid modifying saved files manually to prevent corruption</li> <li>Consider backup strategies for important models</li> </ul> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def save(self, path: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Save the normative model to disk.\n\n    This method serializes the entire normative model, including configuration,\n    fitted regression models, scalers, and basis functions (if applicable).\n\n    Parameters\n    ----------\n    path : str , optional\n        Directory path where the model should be saved. If None, uses the\n        path specified in norm_conf.save_dir. Default is None.\n\n    Returns\n    -------\n    None\n        Saves files to disk but doesn't return any value.\n\n    Notes\n    -----\n    The method saves the following components:\n    1. Metadata JSON file ('normative_model_dict.json') containing:\n        - Normative model configuration\n        - Response variable names\n        - Regression model type\n        - Default regression configuration\n        - B-spline basis parameters (if used)\n        - Input/output scaler configurations\n\n    2. Individual regression model files:\n        - One JSON file per response variable\n        - Named as 'model_{response_var}.json'\n        - Contains model-specific parameters and states\n\n    File Structure\n    -------------\n    save_dir/\n    \u251c\u2500\u2500 normative_model_dict.json\n    \u251c\u2500\u2500 model_response1.json\n    \u251c\u2500\u2500 model_response2.json\n    \u2514\u2500\u2500 ...\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Save model to default location\n    &gt;&gt;&gt; model.save()\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Save model to specific path\n    &gt;&gt;&gt; model.save(\"/path/to/save/directory\")\n\n    See Also\n    --------\n    load : Class method for loading a saved model\n    set_save_dir : Method to change the save directory\n\n    Notes\n    -----\n    - Ensures thread-safety when multiple models save to same directory\n    - Maintains backward compatibility with older saved models\n    - Handles complex nested objects through JSON serialization\n    - Preserves all necessary information for model reconstruction\n\n    Warnings\n    --------\n    - Ensure sufficient disk space before saving large models\n    - Avoid modifying saved files manually to prevent corruption\n    - Consider backup strategies for important models\n    \"\"\"\n    # TODO save the model in such a format that parallel models with identical save dirs result in a single model\n    metadata = {\n        \"norm_conf\": self.norm_conf.to_dict(),\n        \"response_vars\": self.response_vars,\n        \"regression_model_type\": self.regression_model_type.__name__,\n        \"default_reg_conf\": self.default_reg_conf.to_dict(),\n    }\n    if self.norm_conf.basis_function == \"bspline\" and hasattr(\n        self, \"bspline_basis\"\n    ):\n        knots = self.bspline_basis.knot_vector\n        metadata[\"bspline_basis\"] = {\n            \"xmin\": knots[0],\n            \"xmax\": knots[-1],\n            \"nknots\": self.norm_conf.nknots,\n            \"p\": self.norm_conf.order,\n        }\n    metadata[\"inscalers\"] = {k: v.to_dict() for k, v in self.inscalers.items()}\n    metadata[\"outscalers\"] = {k: v.to_dict() for k, v in self.outscalers.items()}\n\n    if path is not None:\n        self.norm_conf.set_save_dir(path)\n    with open(\n        os.path.join(self.norm_conf.save_dir, \"normative_model_dict.json\"),\n        mode=\"w\",\n        encoding=\"utf-8\",\n    ) as f:\n        json.dump(metadata, f, indent=4)\n    for responsevar, model in self.regression_models.items():\n        model_dict = model.to_dict(self.norm_conf.save_dir)\n        with open(\n            os.path.join(self.norm_conf.save_dir, f\"model_{responsevar}.json\"),\n            mode=\"w\",\n            encoding=\"utf-8\",\n        ) as f:\n            json.dump(model_dict, f, indent=4)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.scale_backward","title":"<code>scale_backward(data: NormData) -&gt; None</code>","text":"<p>Scales data back to its original scale using stored scalers.</p> <p>This method performs inverse scaling transformation on the data using previously fitted scalers. It reverses the scaling applied during preprocessing to return predictions and other computed values to their original scale.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NormData</code> <p>Data object containing scaled values to be transformed back. May include: - scaled_X: scaled covariates - scaled_y: scaled responses - scaled_yhat: scaled predictions - scaled_ys2: scaled prediction variances - scaled_centiles: scaled prediction centiles</p> required <p>Returns:</p> Type Description <code>None</code> <p>Modifies the input data object in-place by adding or updating: - X: unscaled covariates - y: unscaled responses - yhat: unscaled predictions - ys2: unscaled prediction variances - centiles: unscaled prediction centiles</p> Notes <p>The method: - Uses stored inscalers for covariates - Uses stored outscalers for responses and predictions - Preserves the original data structure and coordinates - Handles missing data appropriately - Maintains data consistency across all variables</p> <p>The inverse scaling is applied to all relevant variables that were previously scaled during preprocessing. The original scalers must have been created and stored during the forward scaling process.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Assuming model has been fitted and predictions made\n&gt;&gt;&gt; predictions = model.predict(test_data)\n&gt;&gt;&gt; # Scale predictions back to original scale\n&gt;&gt;&gt; model.scale_backward(predictions)\n&gt;&gt;&gt; # Access unscaled predictions\n&gt;&gt;&gt; unscaled_yhat = predictions.yhat\n&gt;&gt;&gt; unscaled_ys2 = predictions.ys2\n</code></pre> See Also <p>scale_forward : Method for forward scaling of data preprocess : Method handling complete preprocessing pipeline NormData.scale_backward : Underlying scaling implementation</p> Warnings <ul> <li>Requires scalers to be previously fitted</li> <li>May produce unexpected results if applied multiple times</li> <li>Should be used only on data scaled with corresponding forward scalers</li> </ul> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def scale_backward(self, data: NormData) -&gt; None:\n    \"\"\"\n    Scales data back to its original scale using stored scalers.\n\n    This method performs inverse scaling transformation on the data using previously\n    fitted scalers. It reverses the scaling applied during preprocessing to return\n    predictions and other computed values to their original scale.\n\n    Parameters\n    ----------\n    data : NormData\n        Data object containing scaled values to be transformed back. May include:\n        - scaled_X: scaled covariates\n        - scaled_y: scaled responses\n        - scaled_yhat: scaled predictions\n        - scaled_ys2: scaled prediction variances\n        - scaled_centiles: scaled prediction centiles\n\n    Returns\n    -------\n    None\n        Modifies the input data object in-place by adding or updating:\n        - X: unscaled covariates\n        - y: unscaled responses\n        - yhat: unscaled predictions\n        - ys2: unscaled prediction variances\n        - centiles: unscaled prediction centiles\n\n    Notes\n    -----\n    The method:\n    - Uses stored inscalers for covariates\n    - Uses stored outscalers for responses and predictions\n    - Preserves the original data structure and coordinates\n    - Handles missing data appropriately\n    - Maintains data consistency across all variables\n\n    The inverse scaling is applied to all relevant variables that were previously\n    scaled during preprocessing. The original scalers must have been created and\n    stored during the forward scaling process.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Assuming model has been fitted and predictions made\n    &gt;&gt;&gt; predictions = model.predict(test_data)\n    &gt;&gt;&gt; # Scale predictions back to original scale\n    &gt;&gt;&gt; model.scale_backward(predictions)\n    &gt;&gt;&gt; # Access unscaled predictions\n    &gt;&gt;&gt; unscaled_yhat = predictions.yhat\n    &gt;&gt;&gt; unscaled_ys2 = predictions.ys2\n\n    See Also\n    --------\n    scale_forward : Method for forward scaling of data\n    preprocess : Method handling complete preprocessing pipeline\n    NormData.scale_backward : Underlying scaling implementation\n\n    Warnings\n    --------\n    - Requires scalers to be previously fitted\n    - May produce unexpected results if applied multiple times\n    - Should be used only on data scaled with corresponding forward scalers\n    \"\"\"\n    data.scale_backward(self.inscalers, self.outscalers)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.scale_forward","title":"<code>scale_forward(data: NormData, overwrite: bool = False) -&gt; None</code>","text":"<p>Scales input data to standardized form using configured scalers.</p> <p>This method handles the forward scaling transformation of both covariates (X) and response variables (y) using separate scalers for each variable. It creates and fits new scalers if they don't exist or if overwrite is True.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NormData</code> <p>Data object containing arrays to be scaled: - X : array-like, shape (n_samples, n_covariates)     Covariate data to be scaled - y : array-like, shape (n_samples, n_response_vars), optional     Response variable data to be scaled</p> required <code>overwrite</code> <code>bool</code> <p>If True, creates new scalers even if they already exist. If False, uses existing scalers when available.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>Modifies the input data object in-place by adding: - scaled_X : scaled covariate data - scaled_y : scaled response data (if y exists)</p> Notes <p>Scaling operations: 1. For each covariate:     - Creates/retrieves scaler using norm_conf.inscaler type     - Fits scaler if new or overwrite=True     - Transforms data using scaler</p> <ol> <li>For each response variable:<ul> <li>Creates/retrieves scaler using norm_conf.outscaler type</li> <li>Fits scaler if new or overwrite=True</li> <li>Transforms data using scaler</li> </ul> </li> </ol> <p>The scalers are stored in: - self.inscalers : dict mapping covariate names to their scalers - self.outscalers : dict mapping response variable names to their scalers</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic usage\n&gt;&gt;&gt; data = NormData(X=covariates, y=responses)\n&gt;&gt;&gt; model.scale_forward(data)\n&gt;&gt;&gt; scaled_X = data.scaled_X\n&gt;&gt;&gt; scaled_y = data.scaled_y\n</code></pre> <pre><code>&gt;&gt;&gt; # Force new scalers\n&gt;&gt;&gt; model.scale_forward(data, overwrite=True)\n</code></pre> See Also <p>scale_backward : Method for inverse scaling transformation NormData : Class containing data structures scaler : Factory function for creating scalers</p> Notes <p>Supported scaler types (configured in norm_conf): - 'StandardScaler': zero mean, unit variance - 'MinMaxScaler': scales to specified range - 'RobustScaler': scales using statistics robust to outliers - Custom scalers implementing fit/transform interface</p> References <p>.. [1] Scikit-learn preprocessing: https://scikit-learn.org/stable/modules/preprocessing.html</p> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def scale_forward(self, data: NormData, overwrite: bool = False) -&gt; None:\n    \"\"\"\n    Scales input data to standardized form using configured scalers.\n\n    This method handles the forward scaling transformation of both covariates (X)\n    and response variables (y) using separate scalers for each variable. It creates\n    and fits new scalers if they don't exist or if overwrite is True.\n\n    Parameters\n    ----------\n    data : NormData\n        Data object containing arrays to be scaled:\n        - X : array-like, shape (n_samples, n_covariates)\n            Covariate data to be scaled\n        - y : array-like, shape (n_samples, n_response_vars), optional\n            Response variable data to be scaled\n\n    overwrite : bool, default=False\n        If True, creates new scalers even if they already exist.\n        If False, uses existing scalers when available.\n\n    Returns\n    -------\n    None\n        Modifies the input data object in-place by adding:\n        - scaled_X : scaled covariate data\n        - scaled_y : scaled response data (if y exists)\n\n    Notes\n    -----\n    Scaling operations:\n    1. For each covariate:\n        - Creates/retrieves scaler using norm_conf.inscaler type\n        - Fits scaler if new or overwrite=True\n        - Transforms data using scaler\n\n    2. For each response variable:\n        - Creates/retrieves scaler using norm_conf.outscaler type\n        - Fits scaler if new or overwrite=True\n        - Transforms data using scaler\n\n    The scalers are stored in:\n    - self.inscalers : dict mapping covariate names to their scalers\n    - self.outscalers : dict mapping response variable names to their scalers\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Basic usage\n    &gt;&gt;&gt; data = NormData(X=covariates, y=responses)\n    &gt;&gt;&gt; model.scale_forward(data)\n    &gt;&gt;&gt; scaled_X = data.scaled_X\n    &gt;&gt;&gt; scaled_y = data.scaled_y\n\n    &gt;&gt;&gt; # Force new scalers\n    &gt;&gt;&gt; model.scale_forward(data, overwrite=True)\n\n    See Also\n    --------\n    scale_backward : Method for inverse scaling transformation\n    NormData : Class containing data structures\n    scaler : Factory function for creating scalers\n\n    Notes\n    -----\n    Supported scaler types (configured in norm_conf):\n    - 'StandardScaler': zero mean, unit variance\n    - 'MinMaxScaler': scales to specified range\n    - 'RobustScaler': scales using statistics robust to outliers\n    - Custom scalers implementing fit/transform interface\n\n    References\n    ----------\n    .. [1] Scikit-learn preprocessing: https://scikit-learn.org/stable/modules/preprocessing.html\n    \"\"\"\n    for covariate in data.covariates.to_numpy():\n        if (covariate not in self.inscalers) or overwrite:\n            self.inscalers[covariate] = Scaler.from_string(self.norm_conf.inscaler)\n            self.inscalers[covariate].fit(data.X.sel(covariates=covariate).data)\n\n    for responsevar in data.response_vars.to_numpy():\n        if (responsevar not in self.outscalers) or overwrite:\n            self.outscalers[responsevar] = Scaler.from_string(self.norm_conf.outscaler)\n            self.outscalers[responsevar].fit(\n                data.y.sel(response_vars=responsevar).data\n            )\n\n    data.scale_forward(self.inscalers, self.outscalers)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.set_log_dir","title":"<code>set_log_dir(log_dir: str) -&gt; None</code>","text":"<p>Override the log_dir in the norm_conf.</p> <p>Args:     log_dir (str): New log directory.</p> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def set_log_dir(self, log_dir: str) -&gt; None:\n    \"\"\"Override the log_dir in the norm_conf.\n\n    Args:\n        log_dir (str): New log directory.\n    \"\"\"\n    self.norm_conf.set_log_dir(log_dir)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.set_save_dir","title":"<code>set_save_dir(save_dir: str) -&gt; None</code>","text":"<p>Override the save_dir in the norm_conf.</p> <p>Args:     save_dir (str): New save directory.</p> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def set_save_dir(self, save_dir: str) -&gt; None:\n    \"\"\"Override the save_dir in the norm_conf.\n\n    Args:\n        save_dir (str): New save directory.\n    \"\"\"\n    self.norm_conf.set_save_dir(save_dir)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.transfer","title":"<code>transfer(data: NormData, *args: Any, **kwargs: Any) -&gt; 'NormBase'</code>","text":"<p>Transfers the normative model to new data, creating a new adapted model.</p> <p>This method performs transfer learning by adapting the existing model to new data while preserving knowledge from the original training. It creates a new normative model instance with transferred parameters for each response variable.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NormData</code> <p>Transfer data containing covariates (X) and response variables (y). Must have compatible structure with the original training data.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments passed to the underlying transfer implementation.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the underlying transfer implementation. Common options include: - 'transfer_type': str, type of transfer learning to perform - 'learning_rate': float, adaptation rate for transfer - 'regularization': float, strength of regularization during transfer</p> <code>{}</code> <p>Returns:</p> Type Description <code>NormBase</code> <p>A new normative model instance adapted to the transfer data, containing: - Transferred regression models for each response variable - Updated configuration with transfer-specific settings - Preserved preprocessing transformations from original model</p> Notes <p>The transfer process: 1. Preprocesses transfer data using original scalers 2. Transfers each response variable's model separately 3. Creates new model instance with transferred parameters 4. Maintains original model's configuration with transfer-specific adjustments</p> <p>The method supports different transfer learning approaches depending on the underlying regression model implementation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Original model trained on source domain\n&gt;&gt;&gt; original_model = NormBase(norm_conf)\n&gt;&gt;&gt; original_model.fit(source_data)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Transfer to target domain\n&gt;&gt;&gt; transfer_data = NormData(X=target_covariates, y=target_responses)\n&gt;&gt;&gt; transferred_model = original_model.transfer(\n...     transfer_data,\n...     transfer_type='fine_tune',\n...     learning_rate=0.01\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Make predictions with transferred model\n&gt;&gt;&gt; predictions = transferred_model.predict(test_data)\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model hasn't been fitted before transfer attempt</p> <code>AssertionError</code> <p>If transfer data is incompatible with original model structure</p> <code>RuntimeError</code> <p>If transfer process fails for any response variable</p> See Also <p>_transfer : Abstract method implementing specific transfer logic fit : Method for initial model fitting predict : Method for making predictions</p> Notes <p>Transfer learning considerations: - Ensures knowledge preservation from source domain - Adapts to target domain characteristics - Maintains model structure and constraints - Supports various transfer strategies</p> <p>The effectiveness of transfer depends on: - Similarity between source and target domains - Amount of transfer data available - Choice of transfer learning parameters - Underlying model architecture</p> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def transfer(self, data: NormData, *args: Any, **kwargs: Any) -&gt; \"NormBase\":\n    \"\"\"\n    Transfers the normative model to new data, creating a new adapted model.\n\n    This method performs transfer learning by adapting the existing model to new data\n    while preserving knowledge from the original training. It creates a new normative\n    model instance with transferred parameters for each response variable.\n\n    Parameters\n    ----------\n    data : NormData\n        Transfer data containing covariates (X) and response variables (y).\n        Must have compatible structure with the original training data.\n    *args : Any\n        Additional positional arguments passed to the underlying transfer implementation.\n    **kwargs : Any\n        Additional keyword arguments passed to the underlying transfer implementation.\n        Common options include:\n        - 'transfer_type': str, type of transfer learning to perform\n        - 'learning_rate': float, adaptation rate for transfer\n        - 'regularization': float, strength of regularization during transfer\n\n    Returns\n    -------\n    NormBase\n        A new normative model instance adapted to the transfer data, containing:\n        - Transferred regression models for each response variable\n        - Updated configuration with transfer-specific settings\n        - Preserved preprocessing transformations from original model\n\n    Notes\n    -----\n    The transfer process:\n    1. Preprocesses transfer data using original scalers\n    2. Transfers each response variable's model separately\n    3. Creates new model instance with transferred parameters\n    4. Maintains original model's configuration with transfer-specific adjustments\n\n    The method supports different transfer learning approaches depending on the\n    underlying regression model implementation.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Original model trained on source domain\n    &gt;&gt;&gt; original_model = NormBase(norm_conf)\n    &gt;&gt;&gt; original_model.fit(source_data)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Transfer to target domain\n    &gt;&gt;&gt; transfer_data = NormData(X=target_covariates, y=target_responses)\n    &gt;&gt;&gt; transferred_model = original_model.transfer(\n    ...     transfer_data,\n    ...     transfer_type='fine_tune',\n    ...     learning_rate=0.01\n    ... )\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Make predictions with transferred model\n    &gt;&gt;&gt; predictions = transferred_model.predict(test_data)\n\n    Raises\n    ------\n    ValueError\n        If the model hasn't been fitted before transfer attempt\n    AssertionError\n        If transfer data is incompatible with original model structure\n    RuntimeError\n        If transfer process fails for any response variable\n\n    See Also\n    --------\n    _transfer : Abstract method implementing specific transfer logic\n    fit : Method for initial model fitting\n    predict : Method for making predictions\n\n    Notes\n    -----\n    Transfer learning considerations:\n    - Ensures knowledge preservation from source domain\n    - Adapts to target domain characteristics\n    - Maintains model structure and constraints\n    - Supports various transfer strategies\n\n    The effectiveness of transfer depends on:\n    - Similarity between source and target domains\n    - Amount of transfer data available\n    - Choice of transfer learning parameters\n    - Underlying model architecture\n    \"\"\"\n    self.preprocess(data)\n    transfered_models = {}\n    print(f\"Going to transfer {len(self.response_vars)} models\")\n    for responsevar in self.response_vars:\n        resp_transfer_data = data.sel(response_vars=responsevar)\n        if responsevar not in self.regression_models:\n            raise ValueError(\n                \"Attempted to transfer a model that has not been fitted.\"\n            )\n        self.focus(responsevar)\n        print(f\"Transferring model for {responsevar}\")\n        transfered_models[responsevar] = self._transfer(\n            resp_transfer_data, *args, **kwargs\n        )\n        self.reset()\n\n    transfered_norm_conf_dict = self.norm_conf.to_dict()\n    transfered_norm_conf_dict[\"save_dir\"] = self.norm_conf.save_dir + \"_transfer\"\n    transfered_norm_conf_dict[\"log_dir\"] = self.norm_conf.log_dir + \"_transfer\"\n    transfered_norm_conf = NormConf.from_dict(transfered_norm_conf_dict)\n\n    # pylint: disable=too-many-function-args\n    transfered_normative_model = self.__class__(\n        transfered_norm_conf,\n        self.default_reg_conf,  # type: ignore\n    )\n\n    transfered_normative_model.response_vars = (\n        data.response_vars.to_numpy().copy().tolist()\n    )\n    transfered_normative_model.regression_models = transfered_models\n    return transfered_normative_model\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_base.NormBase.tune","title":"<code>tune(data: NormData) -&gt; None</code>","text":"<p>Tunes the normative model with new data.</p> <p>Args:     data (NormData): Data containing the covariates and response variables to tune the model with.</p> Source code in <code>pcntoolkit/normative_model/norm_base.py</code> <pre><code>def tune(self, data: NormData) -&gt; None:\n    \"\"\"Tunes the normative model with new data.\n\n    Args:\n        data (NormData): Data containing the covariates and response variables to tune the model with.\n    \"\"\"\n    self._tune(data)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_blr","title":"<code>norm_blr</code>","text":""},{"location":"api/#pcntoolkit.normative_model.norm_blr.NormBLR","title":"<code>NormBLR</code>","text":"<p>               Bases: <code>NormBase</code></p> <p>Bayesian Linear Regression implementation of the normative modeling framework.</p> <p>This class implements normative modeling using Bayesian Linear Regression (BLR) as the underlying regression model. It supports both homoscedastic and heteroscedastic noise models, as well as random intercepts for handling batch effects.</p> <p>Parameters:</p> Name Type Description Default <code>norm_conf</code> <code>NormConf</code> <p>Normative model configuration object containing general settings.</p> required <code>reg_conf</code> <code>Optional[BLRConf]</code> <p>BLR-specific regression configuration. If None, default configuration is used.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>default_reg_conf</code> <code>BLRConf</code> <p>Default regression configuration for BLR.</p> <code>regression_model_type</code> <code>Type[BLR]</code> <p>Type of the regression model (BLR).</p> <code>current_regression_model</code> <code>BLR</code> <p>Current active regression model instance.</p> Notes <p>The BLR implementation supports: - Linear and non-linear regression - Homoscedastic and heteroscedastic noise models - Random intercepts for batch effect correction - Computation of centiles and z-scores</p> Source code in <code>pcntoolkit/normative_model/norm_blr.py</code> <pre><code>class NormBLR(NormBase):\n    \"\"\"Bayesian Linear Regression implementation of the normative modeling framework.\n\n    This class implements normative modeling using Bayesian Linear Regression (BLR) as the\n    underlying regression model. It supports both homoscedastic and heteroscedastic noise models,\n    as well as random intercepts for handling batch effects.\n\n    Parameters\n    ----------\n    norm_conf : NormConf\n        Normative model configuration object containing general settings.\n    reg_conf : Optional[BLRConf], default=None\n        BLR-specific regression configuration. If None, default configuration is used.\n\n    Attributes\n    ----------\n    default_reg_conf : BLRConf\n        Default regression configuration for BLR.\n    regression_model_type : Type[BLR]\n        Type of the regression model (BLR).\n    current_regression_model : BLR\n        Current active regression model instance.\n\n    Notes\n    -----\n    The BLR implementation supports:\n    - Linear and non-linear regression\n    - Homoscedastic and heteroscedastic noise models\n    - Random intercepts for batch effect correction\n    - Computation of centiles and z-scores\n    \"\"\"\n\n    def __init__(self, norm_conf: NormConf, reg_conf: Optional[BLRConf] = None) -&gt; None:\n        super().__init__(norm_conf)\n        if reg_conf is None:\n            reg_conf = BLRConf()\n        self.default_reg_conf: BLRConf = reg_conf\n        self.regression_model_type: Type[BLR] = BLR\n        self.current_regression_model: BLR = None  # type:ignore\n\n    @classmethod\n    def from_args(cls, args: Any) -&gt; \"NormBLR\":\n        norm_conf = NormConf.from_args(args)\n        hbrconf = BLRConf.from_args(args)\n        self = cls(norm_conf, hbrconf)\n        return self\n\n    def _fit(self, data: NormData, make_new_model: bool = False) -&gt; None:\n        blrdata = self.normdata_to_blrdata(data)\n        self.current_regression_model.fit(blrdata)\n        assert self.current_regression_model.is_fitted\n\n    def _predict(self, data: NormData) -&gt; None:\n        blrdata = self.normdata_to_blrdata(data)\n        self.current_regression_model.predict(blrdata)\n\n    def _fit_predict(self, fit_data: NormData, predict_data: NormData) -&gt; None:\n        raise NotImplementedError(\n            f\"Fit-predict method not implemented for {self.__class__.__name__}\"\n        )\n\n    def _transfer(self, data: NormData, **kwargs: Any) -&gt; \"BLR\":\n        raise NotImplementedError(\n            f\"Transfer method not implemented for {self.__class__.__name__}\"\n        )\n\n    def _extend(self, data: NormData) -&gt; \"NormBLR\":\n        raise NotImplementedError(\n            f\"Extend method not implemented for {self.__class__.__name__}\"\n        )\n\n    def _tune(self, data: NormData) -&gt; \"NormBLR\":\n        raise NotImplementedError(\n            f\"Tune method not implemented for {self.__class__.__name__}\"\n        )\n\n    def _merge(self, other: NormBase) -&gt; \"NormBLR\":\n        raise NotImplementedError(\n            f\"Merge method not implemented for {self.__class__.__name__}\"\n        )\n\n    def _centiles(self, data: NormData, cdf: np.ndarray, **kwargs: Any) -&gt; xr.DataArray:\n        blrdata = self.normdata_to_blrdata(data)\n        centiles = self.current_regression_model.centiles(blrdata, cdf)\n        return xr.DataArray(\n            centiles,\n            dims=[\"cdf\", \"datapoints\"],\n            coords={\"cdf\": cdf},\n        )\n\n    def _zscores(self, data: NormData) -&gt; xr.DataArray:\n        blrdata = self.normdata_to_blrdata(data)\n        zscores = self.current_regression_model.zscores(blrdata)\n        return xr.DataArray(\n            zscores,\n            dims=[\"datapoints\"],\n        )\n\n    def n_params(self) -&gt; int:\n        raise NotImplementedError(\n            f\"n_params method not implemented for {self.__class__.__name__}\"\n        )\n\n    def create_design_matrix(\n        self,\n        data: NormData,\n        linear: bool = False,\n        intercept: bool = False,\n        random_intercept: bool = False,\n    ) -&gt; np.ndarray:\n        \"\"\"Create design matrix for the model.\n\n        Parameters\n        ----------\n        data : NormData\n            Input data containing features and batch effects.\n        linear : bool, default=False\n            Include linear terms in the design matrix.\n        intercept : bool, default=False\n            Include intercept term in the design matrix.\n        random_intercept : bool, default=False\n            Include random intercepts for batch effects.\n\n        Returns\n        -------\n        np.ndarray\n            Design matrix combining all requested components.\n\n        Raises\n        ------\n        ValueError\n            If no components are selected for the design matrix.\n        \"\"\"\n        acc = []\n        if linear:\n            if hasattr(data, \"Phi\") and data.Phi is not None:\n                acc.append(data.Phi.to_numpy())\n            elif hasattr(data, \"scaled_X\") and data.scaled_X is not None:\n                acc.append(data.scaled_X.to_numpy())\n            else:\n                acc.append(data.X.to_numpy())\n\n        if intercept:\n            acc.append(np.ones((data.X.shape[0], 1)))\n\n        # Create one-hot encoding for random intercept\n        if random_intercept:\n            for i in data.batch_effect_dims:\n                cur_be = data.batch_effects.sel(batch_effect_dims=i)\n                cur_be_id = np.vectorize(\n                    data.attrs[\"batch_effects_maps\"][i.item()].get\n                )(cur_be.values)\n                acc.append(\n                    np.eye(len(data.attrs[\"batch_effects_maps\"][i.item()]))[cur_be_id],\n                )\n        if len(acc) == 0:\n            raise ValueError(\"No design matrix created\")\n\n        return np.concatenate(acc, axis=1)\n\n    def normdata_to_blrdata(self, data: NormData) -&gt; BLRData:\n        \"\"\"Convert NormData to BLRData format.\n\n        Parameters\n        ----------\n        data : NormData\n            Input data in NormData format.\n\n        Returns\n        -------\n        BLRData\n            Converted data in BLRData format.\n\n        Raises\n        ------\n        ValueError\n            If regression configuration is not of type BLRConf.\n        \"\"\"\n        reg_conf: RegConf = self.current_regression_model.reg_conf\n        if not isinstance(reg_conf, BLRConf):\n            raise ValueError(\n                f\"Regression configuration is not of type BLRConf, got {type(reg_conf)}\"\n            )\n\n        this_X = self.create_design_matrix(\n            data,\n            linear=True,\n            intercept=reg_conf.intercept,\n            random_intercept=reg_conf.random_intercept,\n        )\n\n        this_var_X = self.create_design_matrix(\n            data,\n            linear=reg_conf.heteroskedastic,\n            intercept=reg_conf.intercept_var,\n            random_intercept=reg_conf.random_intercept_var,\n        )\n\n        if hasattr(data, \"scaled_y\") and data.scaled_y is not None:\n            this_y = data.scaled_y.to_numpy()\n        else:\n            this_y = data.y.to_numpy()\n\n        blrdata = BLRData(\n            X=this_X,\n            y=this_y,\n            var_X=this_var_X,\n            batch_effects=data.batch_effects.to_numpy(),\n            response_var=data.response_vars.to_numpy().item(),\n        )\n        blrdata.set_batch_effects_maps(data.attrs[\"batch_effects_maps\"])\n        return blrdata\n\n    @property\n    def focused_model(self) -&gt; BLR:\n        \"\"\"Get the currently focused BLR model.\n\n        Returns\n        -------\n        BLR\n            The currently focused Bayesian Linear Regression model.\n        \"\"\"\n        return self[self.focused_var]  # type:ignore\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_blr.NormBLR.focused_model","title":"<code>focused_model: BLR</code>  <code>property</code>","text":"<p>Get the currently focused BLR model.</p> <p>Returns:</p> Type Description <code>BLR</code> <p>The currently focused Bayesian Linear Regression model.</p>"},{"location":"api/#pcntoolkit.normative_model.norm_blr.NormBLR.create_design_matrix","title":"<code>create_design_matrix(data: NormData, linear: bool = False, intercept: bool = False, random_intercept: bool = False) -&gt; np.ndarray</code>","text":"<p>Create design matrix for the model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NormData</code> <p>Input data containing features and batch effects.</p> required <code>linear</code> <code>bool</code> <p>Include linear terms in the design matrix.</p> <code>False</code> <code>intercept</code> <code>bool</code> <p>Include intercept term in the design matrix.</p> <code>False</code> <code>random_intercept</code> <code>bool</code> <p>Include random intercepts for batch effects.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Design matrix combining all requested components.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no components are selected for the design matrix.</p> Source code in <code>pcntoolkit/normative_model/norm_blr.py</code> <pre><code>def create_design_matrix(\n    self,\n    data: NormData,\n    linear: bool = False,\n    intercept: bool = False,\n    random_intercept: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Create design matrix for the model.\n\n    Parameters\n    ----------\n    data : NormData\n        Input data containing features and batch effects.\n    linear : bool, default=False\n        Include linear terms in the design matrix.\n    intercept : bool, default=False\n        Include intercept term in the design matrix.\n    random_intercept : bool, default=False\n        Include random intercepts for batch effects.\n\n    Returns\n    -------\n    np.ndarray\n        Design matrix combining all requested components.\n\n    Raises\n    ------\n    ValueError\n        If no components are selected for the design matrix.\n    \"\"\"\n    acc = []\n    if linear:\n        if hasattr(data, \"Phi\") and data.Phi is not None:\n            acc.append(data.Phi.to_numpy())\n        elif hasattr(data, \"scaled_X\") and data.scaled_X is not None:\n            acc.append(data.scaled_X.to_numpy())\n        else:\n            acc.append(data.X.to_numpy())\n\n    if intercept:\n        acc.append(np.ones((data.X.shape[0], 1)))\n\n    # Create one-hot encoding for random intercept\n    if random_intercept:\n        for i in data.batch_effect_dims:\n            cur_be = data.batch_effects.sel(batch_effect_dims=i)\n            cur_be_id = np.vectorize(\n                data.attrs[\"batch_effects_maps\"][i.item()].get\n            )(cur_be.values)\n            acc.append(\n                np.eye(len(data.attrs[\"batch_effects_maps\"][i.item()]))[cur_be_id],\n            )\n    if len(acc) == 0:\n        raise ValueError(\"No design matrix created\")\n\n    return np.concatenate(acc, axis=1)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_blr.NormBLR.normdata_to_blrdata","title":"<code>normdata_to_blrdata(data: NormData) -&gt; BLRData</code>","text":"<p>Convert NormData to BLRData format.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NormData</code> <p>Input data in NormData format.</p> required <p>Returns:</p> Type Description <code>BLRData</code> <p>Converted data in BLRData format.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If regression configuration is not of type BLRConf.</p> Source code in <code>pcntoolkit/normative_model/norm_blr.py</code> <pre><code>def normdata_to_blrdata(self, data: NormData) -&gt; BLRData:\n    \"\"\"Convert NormData to BLRData format.\n\n    Parameters\n    ----------\n    data : NormData\n        Input data in NormData format.\n\n    Returns\n    -------\n    BLRData\n        Converted data in BLRData format.\n\n    Raises\n    ------\n    ValueError\n        If regression configuration is not of type BLRConf.\n    \"\"\"\n    reg_conf: RegConf = self.current_regression_model.reg_conf\n    if not isinstance(reg_conf, BLRConf):\n        raise ValueError(\n            f\"Regression configuration is not of type BLRConf, got {type(reg_conf)}\"\n        )\n\n    this_X = self.create_design_matrix(\n        data,\n        linear=True,\n        intercept=reg_conf.intercept,\n        random_intercept=reg_conf.random_intercept,\n    )\n\n    this_var_X = self.create_design_matrix(\n        data,\n        linear=reg_conf.heteroskedastic,\n        intercept=reg_conf.intercept_var,\n        random_intercept=reg_conf.random_intercept_var,\n    )\n\n    if hasattr(data, \"scaled_y\") and data.scaled_y is not None:\n        this_y = data.scaled_y.to_numpy()\n    else:\n        this_y = data.y.to_numpy()\n\n    blrdata = BLRData(\n        X=this_X,\n        y=this_y,\n        var_X=this_var_X,\n        batch_effects=data.batch_effects.to_numpy(),\n        response_var=data.response_vars.to_numpy().item(),\n    )\n    blrdata.set_batch_effects_maps(data.attrs[\"batch_effects_maps\"])\n    return blrdata\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_conf","title":"<code>norm_conf</code>","text":"<p>Configuration class for normative modeling.</p> <p>This module provides the NormConf dataclass that handles configuration settings for normative modeling pipelines. It manages settings for: - Model persistence (saving/loading) - Directory management - Basis function configuration - Data scaling - Cross-validation parameters</p> <p>The class provides validation of all configuration parameters and handles directory creation when needed.</p>"},{"location":"api/#pcntoolkit.normative_model.norm_conf.NormConf","title":"<code>NormConf</code>  <code>dataclass</code>","text":"<p>Configuration container for normative modeling.</p> <p>Parameters:</p> Name Type Description Default <code>savemodel</code> <code>bool</code> <p>Whether to save the trained model, by default False</p> <code>False</code> <code>saveresults</code> <code>bool</code> <p>Whether to save prediction results, by default False</p> <code>False</code> <code>log_dir</code> <code>str</code> <p>Directory for logging output, by default \"./logs\"</p> <code>'./logs'</code> <code>save_dir</code> <code>str</code> <p>Directory for saving models and results, by default \"./saves\"</p> <code>'./saves'</code> <code>basis_function</code> <code>str</code> <p>Type of basis function to use ('linear', 'polynomial', 'bspline', 'none'), by default \"linear\"</p> <code>'linear'</code> <code>basis_column</code> <code>int</code> <p>Column index for basis expansion, by default 0</p> <code>0</code> <code>order</code> <code>int</code> <p>Order of polynomial or bspline basis functions, by default 3</p> <code>3</code> <code>nknots</code> <code>int</code> <p>Number of knots for bspline basis functions, by default 5</p> <code>5</code> <code>inscaler</code> <code>str</code> <p>Input data scaling method ('none', 'standardize', 'minmax'), by default \"none\"</p> <code>'none'</code> <code>outscaler</code> <code>str</code> <p>Output data scaling method ('none', 'standardize', 'minmax'), by default \"none\"</p> <code>'none'</code> <code>perform_cv</code> <code>bool</code> <p>Whether to perform cross-validation, by default False</p> <code>False</code> <code>cv_folds</code> <code>int</code> <p>Number of cross-validation folds, by default 0</p> <code>0</code> <code>normative_model_name</code> <code>Optional[str]</code> <p>Name identifier for the normative model, by default None</p> <code>None</code> Source code in <code>pcntoolkit/normative_model/norm_conf.py</code> <pre><code>@dataclass(frozen=True)\nclass NormConf:\n    \"\"\"Configuration container for normative modeling.\n\n    Parameters\n    ----------\n    savemodel : bool, optional\n        Whether to save the trained model, by default False\n    saveresults : bool, optional\n        Whether to save prediction results, by default False\n    log_dir : str, optional\n        Directory for logging output, by default \"./logs\"\n    save_dir : str, optional\n        Directory for saving models and results, by default \"./saves\"\n    basis_function : str, optional\n        Type of basis function to use ('linear', 'polynomial', 'bspline', 'none'),\n        by default \"linear\"\n    basis_column : int, optional\n        Column index for basis expansion, by default 0\n    order : int, optional\n        Order of polynomial or bspline basis functions, by default 3\n    nknots : int, optional\n        Number of knots for bspline basis functions, by default 5\n    inscaler : str, optional\n        Input data scaling method ('none', 'standardize', 'minmax'), by default \"none\"\n    outscaler : str, optional\n        Output data scaling method ('none', 'standardize', 'minmax'), by default \"none\"\n    perform_cv : bool, optional\n        Whether to perform cross-validation, by default False\n    cv_folds : int, optional\n        Number of cross-validation folds, by default 0\n    normative_model_name : Optional[str], optional\n        Name identifier for the normative model, by default None\n    \"\"\"\n\n    savemodel: bool = False\n    saveresults: bool = False\n    log_dir: str = \"./logs\"\n    save_dir: str = \"./saves\"\n    basis_function: str = \"linear\"\n    basis_column: int = 0\n    order: int = 3\n    nknots: int = 5\n    inscaler: str = \"none\"\n    outscaler: str = \"none\"\n    perform_cv: bool = False\n    cv_folds: int = 0\n    normative_model_name: Optional[str] = None\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate configuration after initialization.\"\"\"\n        configuration_problems = self.detect_configuration_problems()\n        if len(configuration_problems) &gt; 0:\n            problem_list = \"\\n\".join(\n                [f\"{i+1}:\\t{v}\" for i, v in enumerate(configuration_problems)]\n            )\n            raise ValueError(\n                f\"The following problems have been detected in the normative model configuration:\\n{problem_list}\"\n            )\n        else:\n            print(\"Configuration of normative model is valid.\")\n\n    @classmethod\n    def from_args(cls, args: Dict[str, Any]) -&gt; \"NormConf\":\n        \"\"\"Create configuration from command line arguments.\n\n        Parameters\n        ----------\n        args : Dict[str, Any]\n            Dictionary of argument names and values\n\n        Returns\n        -------\n        NormConf\n            New configuration instance\n        \"\"\"\n        norm_args:dict[str, Any] = {k: v for k, v in args.items() if k in fields(cls)}\n        return cls(**norm_args)\n\n    @classmethod\n    def from_dict(cls, dct: Dict[str, Any]) -&gt; \"NormConf\":\n        \"\"\"Create configuration from dictionary.\n\n        Parameters\n        ----------\n        dct : Dict[str, Any]\n            Dictionary of configuration parameters\n\n        Returns\n        -------\n        NormConf\n            New configuration instance\n        \"\"\"\n        return cls.from_args(dct)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert configuration to dictionary.\n\n        Returns\n        -------\n        Dict[str, Any]\n            Dictionary representation of configuration\n        \"\"\"\n        return self.__dict__\n\n    def detect_configuration_problems(self) -&gt; List[str]:\n        \"\"\"Detect any problems in the current configuration.\n\n        Returns\n        -------\n        List[str]\n            List of detected configuration problems\n        \"\"\"\n        configuration_problems: List[str] = []\n\n        def add_problem(problem: str) -&gt; None:\n            nonlocal configuration_problems\n            configuration_problems.append(f\"{problem}\")\n\n        self.detect_dir_problem(add_problem, \"log_dir\")\n        self.detect_dir_problem(add_problem, \"save_dir\")\n        self.detect_cv_problem(add_problem)\n        self.detect_basis_function_problem(add_problem)\n        self.detect_scaler_problem(add_problem, \"inscaler\")\n        self.detect_scaler_problem(add_problem, \"outscaler\")\n\n        return configuration_problems\n\n    def detect_dir_problem(\n        self, add_problem: Callable[[str], None], dir_attr_str: str\n    ) -&gt; None:\n        \"\"\"Detect problems with directory configuration.\n\n        Parameters\n        ----------\n        add_problem : Callable[[str], None]\n            Function to add problem description\n        dir_attr_str : str\n            Name of directory attribute to check\n        \"\"\"\n        dir_attr = getattr(self, dir_attr_str)\n        if not isinstance(dir_attr, str):\n            add_problem(\n                f\"{dir_attr_str} is not a string, but {type(dir_attr).__name__}\"\n            )\n        else:\n            if os.path.exists(dir_attr):\n                if not os.path.isdir(dir_attr):\n                    add_problem(\n                        f\"{dir_attr_str} is not a directory, but {get_type_of_object(dir_attr)}\"\n                    )\n            else:\n                warnings.warn(\n                    f\"{dir_attr_str} ({dir_attr}) does not exist, creating it for you\"\n                )\n                os.makedirs(dir_attr)\n\n    def detect_cv_problem(self, add_problem: Callable[[str], None]) -&gt; None:\n        \"\"\"Detect problems with cross-validation configuration.\n\n        Validates that:\n        - perform_cv is a boolean\n        - cv_folds is an integer\n        - If perform_cv is True, cv_folds must be &gt;= 2\n\n        Parameters\n        ----------\n        add_problem : Callable[[str], None]\n            Function to add problem description to the list of configuration problems\n        \"\"\"\n        performisbool: bool = isinstance(self.perform_cv, bool)\n        foldsisint: bool = isinstance(self.cv_folds, int)\n        if not performisbool:\n            add_problem(\n                f\"perform_cv is not a boolean, but {type(self.perform_cv).__name__}\"\n            )\n        if not foldsisint:\n            add_problem(\n                f\"cv_folds is not an integer, but {type(self.cv_folds).__name__}\"\n            )\n        if performisbool and foldsisint:\n            if self.perform_cv and self.cv_folds &lt; 2:\n                add_problem(f\"cv_folds must be at least 2, but is {self.cv_folds}\")\n\n    def detect_basis_function_problem(self, add_problem: Callable[[str], None]) -&gt; None:\n        \"\"\"Detect problems with basis function configuration.\n\n        Validates that:\n        - basis_function is a string and one of: 'linear', 'polynomial', 'bspline', 'none'\n        - For polynomial basis: validates order parameters\n        - For bspline basis: validates order and nknots parameters\n\n        Parameters\n        ----------\n        add_problem : Callable[[str], None]\n            Function to add problem description to the list of configuration problems\n        \"\"\"\n        acceptable_basis_functions = [\"linear\", \"polynomial\", \"bspline\", \"none\"]\n        if not isinstance(self.basis_function, str):\n            add_problem(\n                f\"basis_function_type is not a string, but {type(self.basis_function).__name__}\"\n            )\n        else:\n            if self.basis_function not in acceptable_basis_functions:\n                add_problem(\n                    f\"basis_function_type is not one of the possible values: {acceptable_basis_functions}\"\n                )\n\n            if self.basis_function == \"polynomial\":\n                self.detect_polynomial_basis_expansion_problem(add_problem)\n\n            if self.basis_function == \"bspline\":\n                self.detect_bspline_basis_expansion_problem(add_problem)\n\n    def detect_bspline_basis_expansion_problem(\n        self, add_problem: Callable[[str], None]\n    ) -&gt; None:\n        \"\"\"Detect problems with B-spline basis expansion configuration.\n\n        Validates that:\n        - nknots is an integer &gt;= 2\n        - order is an integer &gt;= 1\n        - order is less than nknots\n\n        Parameters\n        ----------\n        add_problem : Callable[[str], None]\n            Function to add problem description to the list of configuration problems\n        \"\"\"\n        nknotsisint = isinstance(self.nknots, int)\n        orderisint = isinstance(self.order, int)\n        if not nknotsisint:\n            add_problem(f\"nknots is not an integer, but {type(self.nknots).__name__}\")\n        else:\n            if self.nknots &lt; 2:\n                add_problem(f\"nknots must be at least 2, but is {self.nknots}\")\n\n        if not orderisint:\n            add_problem(f\"order is not an integer, but {type(self.order).__name__}\")\n\n        else:\n            if self.order &lt; 1:\n                add_problem(f\"order must be at least 1, but is {self.order}\")\n            if nknotsisint:\n                if self.order &gt; self.nknots:\n                    add_problem(\n                        f\"order must be smaller than nknots, but order is {self.order} and nknots is {self.nknots}\"\n                    )\n\n    def detect_polynomial_basis_expansion_problem(\n        self, add_problem: Callable[[str], None]\n    ) -&gt; None:\n        \"\"\"Detect problems with polynomial basis expansion configuration.\n\n        Validates that:\n        - order is an integer &gt;= 1\n\n        Parameters\n        ----------\n        add_problem : Callable[[str], None]\n            Function to add problem description to the list of configuration problems\n        \"\"\"\n        orderisint = isinstance(self.order, int)\n        if not orderisint:\n            add_problem(f\"order is not an integer, but {type(self.order).__name__}\")\n        else:\n            if self.order &lt; 1:\n                add_problem(f\"order must be at least 1, but is {self.order}\")\n\n    def detect_scaler_problem(\n        self, add_problem: Callable[[str], None], scaler_attr_str: str\n    ) -&gt; None:\n        \"\"\"Detect problems with data scaling configuration.\n\n                Validates that the specified scaler is one of:\n                - 'none'\n                - 'standardize'\n                - 'minmax'\n        ??\n                Parameters\n                ----------\n                add_problem : Callable[[str], None]\n                    Function to add problem description to the list of configuration problems\n                scaler_attr_str : str\n                    Name of the scaler attribute to check ('inscaler' or 'outscaler')\n        \"\"\"\n        acceptable_scalers = [\"none\", \"standardize\", \"minmax\"]\n        scaler_attr = getattr(self, scaler_attr_str)\n        if not isinstance(scaler_attr, str):\n            add_problem(\n                f\"{scaler_attr_str} is not a string, but {type(scaler_attr).__name__}\"\n            )\n        else:\n            if scaler_attr not in acceptable_scalers:\n                add_problem(\n                    f\"{scaler_attr_str} is not one of the possible values: {acceptable_scalers}\"\n                )\n\n    def set_save_dir(self, path: str) -&gt; None:\n        \"\"\"Set the save directory path.\n\n        Since this is a frozen dataclass, uses object.__setattr__ to modify\n        the save_dir attribute.\n\n        Parameters\n        ----------\n        path : str\n            New path for saving models and results\n        \"\"\"\n        object.__setattr__(self, \"save_dir\", path)\n\n    def set_log_dir(self, path: str) -&gt; None:\n        \"\"\"Set the log directory path.\n\n        Since this is a frozen dataclass, uses object.__setattr__ to modify\n        the log_dir attribute.\n\n        Parameters\n        ----------\n        path : str\n            New path for logging output\n        \"\"\"\n        object.__setattr__(self, \"log_dir\", path)\n\n    def copy(self) -&gt; \"NormConf\":\n        \"\"\"Create a deep copy of the configuration.\n\n        Returns\n        -------\n        NormConf\n            A new NormConf instance with the same configuration values\n        \"\"\"\n        return NormConf.from_args(self.to_dict())\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_conf.NormConf.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"<p>Validate configuration after initialization.</p> Source code in <code>pcntoolkit/normative_model/norm_conf.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate configuration after initialization.\"\"\"\n    configuration_problems = self.detect_configuration_problems()\n    if len(configuration_problems) &gt; 0:\n        problem_list = \"\\n\".join(\n            [f\"{i+1}:\\t{v}\" for i, v in enumerate(configuration_problems)]\n        )\n        raise ValueError(\n            f\"The following problems have been detected in the normative model configuration:\\n{problem_list}\"\n        )\n    else:\n        print(\"Configuration of normative model is valid.\")\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_conf.NormConf.copy","title":"<code>copy() -&gt; NormConf</code>","text":"<p>Create a deep copy of the configuration.</p> <p>Returns:</p> Type Description <code>NormConf</code> <p>A new NormConf instance with the same configuration values</p> Source code in <code>pcntoolkit/normative_model/norm_conf.py</code> <pre><code>def copy(self) -&gt; \"NormConf\":\n    \"\"\"Create a deep copy of the configuration.\n\n    Returns\n    -------\n    NormConf\n        A new NormConf instance with the same configuration values\n    \"\"\"\n    return NormConf.from_args(self.to_dict())\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_conf.NormConf.detect_basis_function_problem","title":"<code>detect_basis_function_problem(add_problem: Callable[[str], None]) -&gt; None</code>","text":"<p>Detect problems with basis function configuration.</p> <p>Validates that: - basis_function is a string and one of: 'linear', 'polynomial', 'bspline', 'none' - For polynomial basis: validates order parameters - For bspline basis: validates order and nknots parameters</p> <p>Parameters:</p> Name Type Description Default <code>add_problem</code> <code>Callable[[str], None]</code> <p>Function to add problem description to the list of configuration problems</p> required Source code in <code>pcntoolkit/normative_model/norm_conf.py</code> <pre><code>def detect_basis_function_problem(self, add_problem: Callable[[str], None]) -&gt; None:\n    \"\"\"Detect problems with basis function configuration.\n\n    Validates that:\n    - basis_function is a string and one of: 'linear', 'polynomial', 'bspline', 'none'\n    - For polynomial basis: validates order parameters\n    - For bspline basis: validates order and nknots parameters\n\n    Parameters\n    ----------\n    add_problem : Callable[[str], None]\n        Function to add problem description to the list of configuration problems\n    \"\"\"\n    acceptable_basis_functions = [\"linear\", \"polynomial\", \"bspline\", \"none\"]\n    if not isinstance(self.basis_function, str):\n        add_problem(\n            f\"basis_function_type is not a string, but {type(self.basis_function).__name__}\"\n        )\n    else:\n        if self.basis_function not in acceptable_basis_functions:\n            add_problem(\n                f\"basis_function_type is not one of the possible values: {acceptable_basis_functions}\"\n            )\n\n        if self.basis_function == \"polynomial\":\n            self.detect_polynomial_basis_expansion_problem(add_problem)\n\n        if self.basis_function == \"bspline\":\n            self.detect_bspline_basis_expansion_problem(add_problem)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_conf.NormConf.detect_bspline_basis_expansion_problem","title":"<code>detect_bspline_basis_expansion_problem(add_problem: Callable[[str], None]) -&gt; None</code>","text":"<p>Detect problems with B-spline basis expansion configuration.</p> <p>Validates that: - nknots is an integer &gt;= 2 - order is an integer &gt;= 1 - order is less than nknots</p> <p>Parameters:</p> Name Type Description Default <code>add_problem</code> <code>Callable[[str], None]</code> <p>Function to add problem description to the list of configuration problems</p> required Source code in <code>pcntoolkit/normative_model/norm_conf.py</code> <pre><code>def detect_bspline_basis_expansion_problem(\n    self, add_problem: Callable[[str], None]\n) -&gt; None:\n    \"\"\"Detect problems with B-spline basis expansion configuration.\n\n    Validates that:\n    - nknots is an integer &gt;= 2\n    - order is an integer &gt;= 1\n    - order is less than nknots\n\n    Parameters\n    ----------\n    add_problem : Callable[[str], None]\n        Function to add problem description to the list of configuration problems\n    \"\"\"\n    nknotsisint = isinstance(self.nknots, int)\n    orderisint = isinstance(self.order, int)\n    if not nknotsisint:\n        add_problem(f\"nknots is not an integer, but {type(self.nknots).__name__}\")\n    else:\n        if self.nknots &lt; 2:\n            add_problem(f\"nknots must be at least 2, but is {self.nknots}\")\n\n    if not orderisint:\n        add_problem(f\"order is not an integer, but {type(self.order).__name__}\")\n\n    else:\n        if self.order &lt; 1:\n            add_problem(f\"order must be at least 1, but is {self.order}\")\n        if nknotsisint:\n            if self.order &gt; self.nknots:\n                add_problem(\n                    f\"order must be smaller than nknots, but order is {self.order} and nknots is {self.nknots}\"\n                )\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_conf.NormConf.detect_configuration_problems","title":"<code>detect_configuration_problems() -&gt; List[str]</code>","text":"<p>Detect any problems in the current configuration.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of detected configuration problems</p> Source code in <code>pcntoolkit/normative_model/norm_conf.py</code> <pre><code>def detect_configuration_problems(self) -&gt; List[str]:\n    \"\"\"Detect any problems in the current configuration.\n\n    Returns\n    -------\n    List[str]\n        List of detected configuration problems\n    \"\"\"\n    configuration_problems: List[str] = []\n\n    def add_problem(problem: str) -&gt; None:\n        nonlocal configuration_problems\n        configuration_problems.append(f\"{problem}\")\n\n    self.detect_dir_problem(add_problem, \"log_dir\")\n    self.detect_dir_problem(add_problem, \"save_dir\")\n    self.detect_cv_problem(add_problem)\n    self.detect_basis_function_problem(add_problem)\n    self.detect_scaler_problem(add_problem, \"inscaler\")\n    self.detect_scaler_problem(add_problem, \"outscaler\")\n\n    return configuration_problems\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_conf.NormConf.detect_cv_problem","title":"<code>detect_cv_problem(add_problem: Callable[[str], None]) -&gt; None</code>","text":"<p>Detect problems with cross-validation configuration.</p> <p>Validates that: - perform_cv is a boolean - cv_folds is an integer - If perform_cv is True, cv_folds must be &gt;= 2</p> <p>Parameters:</p> Name Type Description Default <code>add_problem</code> <code>Callable[[str], None]</code> <p>Function to add problem description to the list of configuration problems</p> required Source code in <code>pcntoolkit/normative_model/norm_conf.py</code> <pre><code>def detect_cv_problem(self, add_problem: Callable[[str], None]) -&gt; None:\n    \"\"\"Detect problems with cross-validation configuration.\n\n    Validates that:\n    - perform_cv is a boolean\n    - cv_folds is an integer\n    - If perform_cv is True, cv_folds must be &gt;= 2\n\n    Parameters\n    ----------\n    add_problem : Callable[[str], None]\n        Function to add problem description to the list of configuration problems\n    \"\"\"\n    performisbool: bool = isinstance(self.perform_cv, bool)\n    foldsisint: bool = isinstance(self.cv_folds, int)\n    if not performisbool:\n        add_problem(\n            f\"perform_cv is not a boolean, but {type(self.perform_cv).__name__}\"\n        )\n    if not foldsisint:\n        add_problem(\n            f\"cv_folds is not an integer, but {type(self.cv_folds).__name__}\"\n        )\n    if performisbool and foldsisint:\n        if self.perform_cv and self.cv_folds &lt; 2:\n            add_problem(f\"cv_folds must be at least 2, but is {self.cv_folds}\")\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_conf.NormConf.detect_dir_problem","title":"<code>detect_dir_problem(add_problem: Callable[[str], None], dir_attr_str: str) -&gt; None</code>","text":"<p>Detect problems with directory configuration.</p> <p>Parameters:</p> Name Type Description Default <code>add_problem</code> <code>Callable[[str], None]</code> <p>Function to add problem description</p> required <code>dir_attr_str</code> <code>str</code> <p>Name of directory attribute to check</p> required Source code in <code>pcntoolkit/normative_model/norm_conf.py</code> <pre><code>def detect_dir_problem(\n    self, add_problem: Callable[[str], None], dir_attr_str: str\n) -&gt; None:\n    \"\"\"Detect problems with directory configuration.\n\n    Parameters\n    ----------\n    add_problem : Callable[[str], None]\n        Function to add problem description\n    dir_attr_str : str\n        Name of directory attribute to check\n    \"\"\"\n    dir_attr = getattr(self, dir_attr_str)\n    if not isinstance(dir_attr, str):\n        add_problem(\n            f\"{dir_attr_str} is not a string, but {type(dir_attr).__name__}\"\n        )\n    else:\n        if os.path.exists(dir_attr):\n            if not os.path.isdir(dir_attr):\n                add_problem(\n                    f\"{dir_attr_str} is not a directory, but {get_type_of_object(dir_attr)}\"\n                )\n        else:\n            warnings.warn(\n                f\"{dir_attr_str} ({dir_attr}) does not exist, creating it for you\"\n            )\n            os.makedirs(dir_attr)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_conf.NormConf.detect_polynomial_basis_expansion_problem","title":"<code>detect_polynomial_basis_expansion_problem(add_problem: Callable[[str], None]) -&gt; None</code>","text":"<p>Detect problems with polynomial basis expansion configuration.</p> <p>Validates that: - order is an integer &gt;= 1</p> <p>Parameters:</p> Name Type Description Default <code>add_problem</code> <code>Callable[[str], None]</code> <p>Function to add problem description to the list of configuration problems</p> required Source code in <code>pcntoolkit/normative_model/norm_conf.py</code> <pre><code>def detect_polynomial_basis_expansion_problem(\n    self, add_problem: Callable[[str], None]\n) -&gt; None:\n    \"\"\"Detect problems with polynomial basis expansion configuration.\n\n    Validates that:\n    - order is an integer &gt;= 1\n\n    Parameters\n    ----------\n    add_problem : Callable[[str], None]\n        Function to add problem description to the list of configuration problems\n    \"\"\"\n    orderisint = isinstance(self.order, int)\n    if not orderisint:\n        add_problem(f\"order is not an integer, but {type(self.order).__name__}\")\n    else:\n        if self.order &lt; 1:\n            add_problem(f\"order must be at least 1, but is {self.order}\")\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_conf.NormConf.detect_scaler_problem","title":"<code>detect_scaler_problem(add_problem: Callable[[str], None], scaler_attr_str: str) -&gt; None</code>","text":"<p>Detect problems with data scaling configuration.</p> <pre><code>    Validates that the specified scaler is one of:\n    - 'none'\n    - 'standardize'\n    - 'minmax'\n</code></pre> <p>??</p> <pre><code>    Parameters\n</code></pre> <pre><code>    add_problem : Callable[[str], None]\n        Function to add problem description to the list of configuration problems\n    scaler_attr_str : str\n        Name of the scaler attribute to check ('inscaler' or 'outscaler')\n</code></pre> Source code in <code>pcntoolkit/normative_model/norm_conf.py</code> <pre><code>def detect_scaler_problem(\n    self, add_problem: Callable[[str], None], scaler_attr_str: str\n) -&gt; None:\n    \"\"\"Detect problems with data scaling configuration.\n\n            Validates that the specified scaler is one of:\n            - 'none'\n            - 'standardize'\n            - 'minmax'\n    ??\n            Parameters\n            ----------\n            add_problem : Callable[[str], None]\n                Function to add problem description to the list of configuration problems\n            scaler_attr_str : str\n                Name of the scaler attribute to check ('inscaler' or 'outscaler')\n    \"\"\"\n    acceptable_scalers = [\"none\", \"standardize\", \"minmax\"]\n    scaler_attr = getattr(self, scaler_attr_str)\n    if not isinstance(scaler_attr, str):\n        add_problem(\n            f\"{scaler_attr_str} is not a string, but {type(scaler_attr).__name__}\"\n        )\n    else:\n        if scaler_attr not in acceptable_scalers:\n            add_problem(\n                f\"{scaler_attr_str} is not one of the possible values: {acceptable_scalers}\"\n            )\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_conf.NormConf.from_args","title":"<code>from_args(args: Dict[str, Any]) -&gt; NormConf</code>  <code>classmethod</code>","text":"<p>Create configuration from command line arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Dict[str, Any]</code> <p>Dictionary of argument names and values</p> required <p>Returns:</p> Type Description <code>NormConf</code> <p>New configuration instance</p> Source code in <code>pcntoolkit/normative_model/norm_conf.py</code> <pre><code>@classmethod\ndef from_args(cls, args: Dict[str, Any]) -&gt; \"NormConf\":\n    \"\"\"Create configuration from command line arguments.\n\n    Parameters\n    ----------\n    args : Dict[str, Any]\n        Dictionary of argument names and values\n\n    Returns\n    -------\n    NormConf\n        New configuration instance\n    \"\"\"\n    norm_args:dict[str, Any] = {k: v for k, v in args.items() if k in fields(cls)}\n    return cls(**norm_args)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_conf.NormConf.from_dict","title":"<code>from_dict(dct: Dict[str, Any]) -&gt; NormConf</code>  <code>classmethod</code>","text":"<p>Create configuration from dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dct</code> <code>Dict[str, Any]</code> <p>Dictionary of configuration parameters</p> required <p>Returns:</p> Type Description <code>NormConf</code> <p>New configuration instance</p> Source code in <code>pcntoolkit/normative_model/norm_conf.py</code> <pre><code>@classmethod\ndef from_dict(cls, dct: Dict[str, Any]) -&gt; \"NormConf\":\n    \"\"\"Create configuration from dictionary.\n\n    Parameters\n    ----------\n    dct : Dict[str, Any]\n        Dictionary of configuration parameters\n\n    Returns\n    -------\n    NormConf\n        New configuration instance\n    \"\"\"\n    return cls.from_args(dct)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_conf.NormConf.set_log_dir","title":"<code>set_log_dir(path: str) -&gt; None</code>","text":"<p>Set the log directory path.</p> <p>Since this is a frozen dataclass, uses object.setattr to modify the log_dir attribute.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>New path for logging output</p> required Source code in <code>pcntoolkit/normative_model/norm_conf.py</code> <pre><code>def set_log_dir(self, path: str) -&gt; None:\n    \"\"\"Set the log directory path.\n\n    Since this is a frozen dataclass, uses object.__setattr__ to modify\n    the log_dir attribute.\n\n    Parameters\n    ----------\n    path : str\n        New path for logging output\n    \"\"\"\n    object.__setattr__(self, \"log_dir\", path)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_conf.NormConf.set_save_dir","title":"<code>set_save_dir(path: str) -&gt; None</code>","text":"<p>Set the save directory path.</p> <p>Since this is a frozen dataclass, uses object.setattr to modify the save_dir attribute.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>New path for saving models and results</p> required Source code in <code>pcntoolkit/normative_model/norm_conf.py</code> <pre><code>def set_save_dir(self, path: str) -&gt; None:\n    \"\"\"Set the save directory path.\n\n    Since this is a frozen dataclass, uses object.__setattr__ to modify\n    the save_dir attribute.\n\n    Parameters\n    ----------\n    path : str\n        New path for saving models and results\n    \"\"\"\n    object.__setattr__(self, \"save_dir\", path)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_conf.NormConf.to_dict","title":"<code>to_dict() -&gt; Dict[str, Any]</code>","text":"<p>Convert configuration to dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary representation of configuration</p> Source code in <code>pcntoolkit/normative_model/norm_conf.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert configuration to dictionary.\n\n    Returns\n    -------\n    Dict[str, Any]\n        Dictionary representation of configuration\n    \"\"\"\n    return self.__dict__\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_factory","title":"<code>norm_factory</code>","text":"<p>Factory methods for creating and loading normative models.</p> <p>This module provides functions to create and load different types of normative models based on given configurations or command line arguments.</p> <p>Functions:</p> Name Description <code>create_normative_model</code> <p>Create a normative model based on the given configuration.</p> <code>load_normative_model</code> <p>Load a normative model from a specified path.</p> <code>create_normative_model_from_args</code> <p>Create a normative model from command line arguments.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; norm_conf = NormConf()\n&gt;&gt;&gt; reg_conf = HBRConf()\n&gt;&gt;&gt; model = create_normative_model(norm_conf, reg_conf)\n&gt;&gt;&gt; isinstance(model, NormHBR)\nTrue\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_factory.create_normative_model","title":"<code>create_normative_model(norm_conf: NormConf, reg_conf: RegConf) -&gt; NormBase</code>","text":"<p>Create a normative model based on the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>norm_conf</code> <code>NormConf</code> <p>The normative model configuration.</p> required <code>reg_conf</code> <code>RegConf</code> <p>The regression model configuration.</p> required <p>Returns:</p> Type Description <code>NormBase</code> <p>An instance of a normative model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the regression model configuration is unknown.</p> Source code in <code>pcntoolkit/normative_model/norm_factory.py</code> <pre><code>def create_normative_model(norm_conf: NormConf, reg_conf: RegConf) -&gt; NormBase:\n    \"\"\"\n    Create a normative model based on the given configuration.\n\n    Parameters\n    ----------\n    norm_conf : NormConf\n        The normative model configuration.\n    reg_conf : RegConf\n        The regression model configuration.\n\n    Returns\n    -------\n    NormBase\n        An instance of a normative model.\n\n    Raises\n    ------\n    ValueError\n        If the regression model configuration is unknown.\n    \"\"\"\n    if isinstance(reg_conf, HBRConf):\n        return NormHBR(norm_conf, reg_conf)\n    elif isinstance(reg_conf, BLRConf):\n        return NormBLR(norm_conf, reg_conf)\n    # elif isinstance(reg_conf, GPRConf):\n    #     return NormGPR(norm_conf, reg_conf)\n    else:\n        raise ValueError(f\"Unknown regression model configuration: {reg_conf.__class__.__name__}\")\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_factory.create_normative_model_from_args","title":"<code>create_normative_model_from_args(args: dict[str, str]) -&gt; NormBase</code>","text":"<p>Create a normative model from command line arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>dict[str, str]</code> <p>A dictionary of command line arguments.</p> required <p>Returns:</p> Type Description <code>NormBase</code> <p>An instance of a normative model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the regression model specified in the arguments is unknown.</p> Source code in <code>pcntoolkit/normative_model/norm_factory.py</code> <pre><code>def create_normative_model_from_args(args: dict[str, str]) -&gt; NormBase:\n    \"\"\"\n    Create a normative model from command line arguments.\n\n    Parameters\n    ----------\n    args : dict[str, str]\n        A dictionary of command line arguments.\n\n    Returns\n    -------\n    NormBase\n        An instance of a normative model.\n\n    Raises\n    ------\n    ValueError\n        If the regression model specified in the arguments is unknown.\n    \"\"\"\n    norm_conf = NormConf.from_args(args)\n    if args[\"alg\"] == \"hbr\":\n        reg_conf = HBRConf.from_args(args)\n    elif args[\"alg\"] == \"blr\":\n        reg_conf = BLRConf.from_args(args)\n    # elif args[\"alg\"] == \"gpr\":\n    #     reg_conf = GPRConf.from_args(args)\n    else:\n        raise ValueError(f\"Unknown regression model: {args['alg']}\")\n    return create_normative_model(norm_conf, reg_conf)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_factory.load_normative_model","title":"<code>load_normative_model(path: str) -&gt; NormBase</code>","text":"<p>Load a normative model from a specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path to load the normative model from.</p> required <p>Returns:</p> Type Description <code>NormBase</code> <p>An instance of a loaded normative model.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified path does not exist.</p> <code>ValueError</code> <p>If the model name is not recognized.</p> Source code in <code>pcntoolkit/normative_model/norm_factory.py</code> <pre><code>def load_normative_model(path: str) -&gt; NormBase:\n    \"\"\"\n    Load a normative model from a specified path.\n\n    Parameters\n    ----------\n    path : str\n        The file path to load the normative model from.\n\n    Returns\n    -------\n    NormBase\n        An instance of a loaded normative model.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified path does not exist.\n    ValueError\n        If the model name is not recognized.\n    \"\"\"\n    try:\n        with open(os.path.join(path, \"normative_model_dict.json\"), mode=\"r\", encoding='utf-8') as f:\n            metadata = json.load(f)\n    except FileNotFoundError as exc:\n        raise FileNotFoundError(f\"Path {path} does not exist.\") from exc\n\n    norm_conf = NormConf.from_dict(metadata[\"norm_conf\"])\n    model_name = norm_conf.normative_model_name\n\n    if model_name == \"NormHBR\":\n        return NormHBR.load(path)\n    elif model_name == \"NormBLR\":\n        return NormBLR.load(path)\n    # elif model_name == \"NormGPR\":\n    #     return NormGPR.load(path)\n    else:\n        raise ValueError(f\"Model name {model_name} not recognized.\")\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_hbr","title":"<code>norm_hbr</code>","text":"<p>norm_hbr.py</p> <p>This module provides the implementation of the NormHBR class, which is a specialized normative model using Hierarchical Bayesian Regression (HBR). It extends the NormBase class and provides methods for fitting, predicting, and transferring models, as well as computing centiles and z-scores.</p> <p>Classes:</p> Name Description <code>NormHBR : NormBase</code> <p>A class for creating and managing a normative model using Hierarchical Bayesian Regression.</p> Example <p>from pcntoolkit.normative_model.norm_conf import NormConf from pcntoolkit.regression_model.hbr.hbr_conf import HBRConf from pcntoolkit.dataio.norm_data import NormData norm_conf = NormConf(...) hbr_conf = HBRConf(...) model = NormHBR(norm_conf, hbr_conf) data = NormData(...) model._fit(data) predictions = model._predict(data)</p> Notes <ul> <li>The NormHBR class assumes that the data provided is compatible with the HBR model.</li> <li>The class provides several methods that are not implemented and will raise   NotImplementedError if called. These methods are placeholders for future extensions.</li> </ul>"},{"location":"api/#pcntoolkit.normative_model.norm_hbr.NormHBR","title":"<code>NormHBR</code>","text":"<p>               Bases: <code>NormBase</code></p> <p>A class for creating and managing a normative model using Hierarchical Bayesian Regression (HBR).</p> <p>This class extends the NormBase class and provides methods for fitting, predicting, and transferring models, as well as computing centiles and z-scores. It is designed to work with data compatible with the HBR model.</p> <p>Parameters:</p> Name Type Description Default <code>norm_conf</code> <code>NormConf</code> <p>Configuration object for the normative model.</p> required <code>reg_conf</code> <code>Optional[HBRConf]</code> <p>Configuration object for the regression model. If not provided, a default HBRConf is used.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>default_reg_conf</code> <code>HBRConf</code> <p>The default configuration for the regression model.</p> <code>regression_model_type</code> <code>Type[HBR]</code> <p>The type of regression model used, which is HBR.</p> <code>current_regression_model</code> <code>HBR</code> <p>The current instance of the regression model being used.</p> <p>Methods:</p> Name Description <code>_fit</code> <p>Fits the HBR model to the provided data.</p> <code>_predict</code> <p>Predicts outcomes using the fitted HBR model.</p> <code>_fit_predict</code> <p>Fits the model to the fit_data and predicts outcomes for predict_data.</p> <code>_transfer</code> <p>Transfers the model to new data with optional freedom parameter.</p> <code>_centiles</code> <p>Computes centiles for the given data and cumulative distribution function.</p> <code>_zscores</code> <p>Computes z-scores for the given data.</p> <code>n_params</code> <p>Returns the number of parameters in the model.</p> <code>from_args</code> <p>Creates a NormHBR instance from command line arguments.</p> <code>normdata_to_hbrdata</code> <p>Converts NormData to HBRData format.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the model is not fitted before calling transfer.</p> <code>NotImplementedError</code> <p>If extend, tune, or merge methods are called.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; norm_conf = NormConf(...)\n&gt;&gt;&gt; hbr_conf = HBRConf(...)\n&gt;&gt;&gt; model = NormHBR(norm_conf, hbr_conf)\n&gt;&gt;&gt; model._fit(data)\n&gt;&gt;&gt; predictions = model._predict(data)\n</code></pre> Source code in <code>pcntoolkit/normative_model/norm_hbr.py</code> <pre><code>class NormHBR(NormBase):\n    \"\"\"\n    A class for creating and managing a normative model using Hierarchical Bayesian Regression (HBR).\n\n    This class extends the NormBase class and provides methods for fitting, predicting, and transferring\n    models, as well as computing centiles and z-scores. It is designed to work with data compatible with\n    the HBR model.\n\n    Parameters\n    ----------\n    norm_conf : NormConf\n        Configuration object for the normative model.\n    reg_conf : Optional[HBRConf], optional\n        Configuration object for the regression model. If not provided, a default HBRConf is used.\n\n    Attributes\n    ----------\n    default_reg_conf : HBRConf\n        The default configuration for the regression model.\n    regression_model_type : Type[HBR]\n        The type of regression model used, which is HBR.\n    current_regression_model : HBR\n        The current instance of the regression model being used.\n\n    Methods\n    -------\n    _fit(data: NormData, make_new_model: bool = False) -&gt; None\n        Fits the HBR model to the provided data.\n\n    _predict(data: NormData) -&gt; None\n        Predicts outcomes using the fitted HBR model.\n\n    _fit_predict(fit_data: NormData, predict_data: NormData) -&gt; None\n        Fits the model to the fit_data and predicts outcomes for predict_data.\n\n    _transfer(data: NormData, **kwargs: Any) -&gt; HBR\n        Transfers the model to new data with optional freedom parameter.\n\n    _centiles(data: NormData, cdf: np.ndarray, **kwargs: Any) -&gt; xr.DataArray\n        Computes centiles for the given data and cumulative distribution function.\n\n    _zscores(data: NormData, resample: bool = False) -&gt; xr.DataArray\n        Computes z-scores for the given data.\n\n    n_params() -&gt; int\n        Returns the number of parameters in the model.\n\n    from_args(cls, args: Any) -&gt; \"NormHBR\"\n        Creates a NormHBR instance from command line arguments.\n\n    normdata_to_hbrdata(data: NormData) -&gt; hbr_data.HBRData\n        Converts NormData to HBRData format.\n\n    Raises\n    ------\n    RuntimeError\n        If the model is not fitted before calling transfer.\n\n    NotImplementedError\n        If extend, tune, or merge methods are called.\n\n    Examples\n    --------\n    &gt;&gt;&gt; norm_conf = NormConf(...)\n    &gt;&gt;&gt; hbr_conf = HBRConf(...)\n    &gt;&gt;&gt; model = NormHBR(norm_conf, hbr_conf)\n    &gt;&gt;&gt; model._fit(data)\n    &gt;&gt;&gt; predictions = model._predict(data)\n    \"\"\"\n\n    def __init__(self, norm_conf: NormConf, reg_conf: HBRConf = None): # type: ignore\n        super().__init__(norm_conf)\n        if reg_conf is None:\n            reg_conf = HBRConf()\n        self.default_reg_conf: HBRConf = reg_conf\n        self.regression_model_type = HBR\n        self.current_regression_model: HBR = None  # type: ignore\n\n    def _fit(self, data: NormData, make_new_model: bool = False) -&gt; None:\n        hbrdata = self.normdata_to_hbrdata(data)\n        self.current_regression_model.fit(hbrdata, make_new_model)\n\n    def _predict(self, data: NormData) -&gt; None:\n        hbrdata = self.normdata_to_hbrdata(data)\n        self.current_regression_model.predict(hbrdata)\n\n    def _fit_predict(self, fit_data: NormData, predict_data: NormData) -&gt; None:\n        fit_hbrdata = self.normdata_to_hbrdata(fit_data)\n        predict_hbrdata = self.normdata_to_hbrdata(predict_data)\n        self.current_regression_model.fit_predict(fit_hbrdata, predict_hbrdata)\n\n    def _transfer(self, data: NormData, **kwargs: Any) -&gt; HBR:\n        freedom = kwargs.get(\"freedom\", 1)\n        transferdata = self.normdata_to_hbrdata(data)\n        if not self.current_regression_model.is_fitted:\n            raise RuntimeError(\"Model needs to be fitted before it can be transferred\")\n        new_hbr_model = self.current_regression_model.transfer(\n            self.default_reg_conf, transferdata, freedom\n        )\n        return new_hbr_model\n\n    def _extend(self, data: NormData) -&gt; NormHBR:\n        raise NotImplementedError(\n            f\"Extend method not implemented for {self.__class__.__name__}\"\n        )\n\n    def _tune(self, data: NormData) -&gt; NormHBR:\n        raise NotImplementedError(\n            f\"Tune method not implemented for {self.__class__.__name__}\"\n        )\n\n    def _merge(self, other: NormBase) -&gt; NormHBR:\n        raise NotImplementedError(\n            f\"Merge method not implemented for {self.__class__.__name__}\"\n        )\n\n    def _centiles(self, data: NormData, cdf: np.ndarray, **kwargs: Any) -&gt; xr.DataArray:\n        resample = kwargs.get(\"resample\", False)\n        hbrdata = self.normdata_to_hbrdata(data)\n\n        return self.current_regression_model.centiles(hbrdata, cdf, resample)\n\n    def _zscores(self, data: NormData, resample: bool = False) -&gt; xr.DataArray:\n        hbrdata = self.normdata_to_hbrdata(data)\n        return self.current_regression_model.zscores(hbrdata, resample)\n\n    def n_params(self) -&gt; int:\n        return sum(\n            [i.size.eval() for i in self.current_regression_model.pymc_model.free_RVs]\n        )\n\n    @classmethod\n    def from_args(cls, args: Any) -&gt; \"NormHBR\":\n        norm_conf = NormConf.from_args(args)\n        hbrconf = HBRConf.from_args(args)\n        self = cls(norm_conf, hbrconf)\n        return self\n\n    @staticmethod\n    def normdata_to_hbrdata(data: NormData) -&gt; hbr_data.HBRData:\n        \"\"\"\n        Converts NormData to HBRData format.\n\n        This method extracts the necessary components from a NormData object and\n        constructs an HBRData object, which is used for Hierarchical Bayesian Regression.\n\n        Parameters\n        ----------\n        data : NormData\n            The NormData object containing the input data, including covariates,\n            response variables, and batch effects.\n\n        Returns\n        -------\n        HBRData\n            An HBRData object containing the converted data, ready for use in\n            Hierarchical Bayesian Regression.\n\n        Raises\n        ------\n        AssertionError\n            If the response variable in the data has more than one dimension.\n\n        Examples\n        --------\n        &gt;&gt;&gt; norm_data = NormData(...)\n        &gt;&gt;&gt; hbr_data = NormHBR.normdata_to_hbrdata(norm_data)\n        \"\"\"\n        if hasattr(data, \"Phi\") and data.Phi is not None:\n            this_X = data.Phi.to_numpy()\n            this_covariate_dims = data.basis_functions.to_numpy()\n        elif hasattr(data, \"scaled_X\") and data.scaled_X is not None:\n            this_X = data.scaled_X.to_numpy()\n            this_covariate_dims = data.covariates.to_numpy()\n        else:\n            this_X = data.X.to_numpy()\n            this_covariate_dims = data.covariates.to_numpy()\n\n        if hasattr(data, \"scaled_y\") and data.scaled_y is not None:\n            this_y = data.scaled_y.to_numpy()\n        else:\n            this_y = data.y.to_numpy()\n\n        assert (len(data.y.shape) == 1) or (\n            data.y.shape[1] == 1\n        ), \"Only one response variable is supported for HBRdata\"\n\n        hbrdata = hbr_data.HBRData(\n            X=this_X,\n            y=this_y,\n            batch_effects=data.batch_effects.to_numpy(),\n            response_var=data.response_vars.to_numpy().item(),\n            covariate_dims=this_covariate_dims,\n            batch_effect_dims=data.batch_effect_dims.to_numpy().tolist(),\n            datapoint_coords=data.datapoints.to_numpy().tolist(),\n        )\n        hbrdata.set_batch_effects_maps(data.attrs[\"batch_effects_maps\"])\n        return hbrdata\n</code></pre>"},{"location":"api/#pcntoolkit.normative_model.norm_hbr.NormHBR.normdata_to_hbrdata","title":"<code>normdata_to_hbrdata(data: NormData) -&gt; hbr_data.HBRData</code>  <code>staticmethod</code>","text":"<p>Converts NormData to HBRData format.</p> <p>This method extracts the necessary components from a NormData object and constructs an HBRData object, which is used for Hierarchical Bayesian Regression.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NormData</code> <p>The NormData object containing the input data, including covariates, response variables, and batch effects.</p> required <p>Returns:</p> Type Description <code>HBRData</code> <p>An HBRData object containing the converted data, ready for use in Hierarchical Bayesian Regression.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the response variable in the data has more than one dimension.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; norm_data = NormData(...)\n&gt;&gt;&gt; hbr_data = NormHBR.normdata_to_hbrdata(norm_data)\n</code></pre> Source code in <code>pcntoolkit/normative_model/norm_hbr.py</code> <pre><code>@staticmethod\ndef normdata_to_hbrdata(data: NormData) -&gt; hbr_data.HBRData:\n    \"\"\"\n    Converts NormData to HBRData format.\n\n    This method extracts the necessary components from a NormData object and\n    constructs an HBRData object, which is used for Hierarchical Bayesian Regression.\n\n    Parameters\n    ----------\n    data : NormData\n        The NormData object containing the input data, including covariates,\n        response variables, and batch effects.\n\n    Returns\n    -------\n    HBRData\n        An HBRData object containing the converted data, ready for use in\n        Hierarchical Bayesian Regression.\n\n    Raises\n    ------\n    AssertionError\n        If the response variable in the data has more than one dimension.\n\n    Examples\n    --------\n    &gt;&gt;&gt; norm_data = NormData(...)\n    &gt;&gt;&gt; hbr_data = NormHBR.normdata_to_hbrdata(norm_data)\n    \"\"\"\n    if hasattr(data, \"Phi\") and data.Phi is not None:\n        this_X = data.Phi.to_numpy()\n        this_covariate_dims = data.basis_functions.to_numpy()\n    elif hasattr(data, \"scaled_X\") and data.scaled_X is not None:\n        this_X = data.scaled_X.to_numpy()\n        this_covariate_dims = data.covariates.to_numpy()\n    else:\n        this_X = data.X.to_numpy()\n        this_covariate_dims = data.covariates.to_numpy()\n\n    if hasattr(data, \"scaled_y\") and data.scaled_y is not None:\n        this_y = data.scaled_y.to_numpy()\n    else:\n        this_y = data.y.to_numpy()\n\n    assert (len(data.y.shape) == 1) or (\n        data.y.shape[1] == 1\n    ), \"Only one response variable is supported for HBRdata\"\n\n    hbrdata = hbr_data.HBRData(\n        X=this_X,\n        y=this_y,\n        batch_effects=data.batch_effects.to_numpy(),\n        response_var=data.response_vars.to_numpy().item(),\n        covariate_dims=this_covariate_dims,\n        batch_effect_dims=data.batch_effect_dims.to_numpy().tolist(),\n        datapoint_coords=data.datapoints.to_numpy().tolist(),\n    )\n    hbrdata.set_batch_effects_maps(data.attrs[\"batch_effects_maps\"])\n    return hbrdata\n</code></pre>"},{"location":"api/#pcntoolkit.normative_parallel","title":"<code>normative_parallel</code>","text":""},{"location":"api/#pcntoolkit.normative_parallel.bashwrap_nm","title":"<code>bashwrap_nm(processing_dir, python_path, normative_path, job_name, covfile_path, respfile_path, func='estimate', **kwargs)</code>","text":"<p>This function wraps normative modelling into a bash script to run it on a torque cluster system.</p> <p>Basic usage::</p> <pre><code>bashwrap_nm(processing_dir, python_path, normative_path, job_name, covfile_path, respfile_path)\n</code></pre> <p>:param processing_dir: Full path to the processing dir :param python_path: Full path to the python distribution :param normative_path: Full path to the normative.py :param job_name: Name for the bash script that is the output of this function :param covfile_path: Full path to a .txt file that contains all covariates (subjects x covariates) for the responsefile :param respfile_path: Full path to a .txt that contains all features (subjects x features) :param cv_folds: Number of cross validations :param testcovfile_path: Full path to a .txt file that contains all covariates (subjects x covariates) for the testresponse file :param testrespfile_path: Full path to a .txt file that contains all test features :param alg: which algorithm to use :param configparam: configuration parameters for this algorithm</p> <p>:outputs: A bash.sh file containing the commands for normative modelling saved to the processing directory (written to disk).</p> <p>written by (primarily) T Wolfers, (adapted) S Rutherford.</p> Source code in <code>pcntoolkit/normative_parallel.py</code> <pre><code>def bashwrap_nm(\n    processing_dir,\n    python_path,\n    normative_path,\n    job_name,\n    covfile_path,\n    respfile_path,\n    func=\"estimate\",\n    **kwargs\n):\n    \"\"\"This function wraps normative modelling into a bash script to run it\n    on a torque cluster system.\n\n    Basic usage::\n\n        bashwrap_nm(processing_dir, python_path, normative_path, job_name, covfile_path, respfile_path)\n\n    :param processing_dir: Full path to the processing dir\n    :param python_path: Full path to the python distribution\n    :param normative_path: Full path to the normative.py\n    :param job_name: Name for the bash script that is the output of this function\n    :param covfile_path: Full path to a .txt file that contains all covariates (subjects x covariates) for the responsefile\n    :param respfile_path: Full path to a .txt that contains all features (subjects x features)\n    :param cv_folds: Number of cross validations\n    :param testcovfile_path: Full path to a .txt file that contains all covariates (subjects x covariates) for the testresponse file\n    :param testrespfile_path: Full path to a .txt file that contains all test features\n    :param alg: which algorithm to use\n    :param configparam: configuration parameters for this algorithm\n\n    :outputs: A bash.sh file containing the commands for normative modelling saved to the processing directory (written to disk).\n\n    written by (primarily) T Wolfers, (adapted) S Rutherford.\n    \"\"\"\n\n    # here we use pop not get to remove the arguments as they used\n    cv_folds = kwargs.pop(\"cv_folds\", None)\n    testcovfile_path = kwargs.pop(\"testcovfile_path\", None)\n    testrespfile_path = kwargs.pop(\"testrespfile_path\", None)\n    alg = kwargs.pop(\"alg\", None)\n    configparam = kwargs.pop(\"configparam\", None)\n    # change to processing dir\n    os.chdir(processing_dir)\n    output_changedir = [\"cd \" + processing_dir + \"\\n\"]\n\n    bash_lines = \"#!/bin/bash\\n\"\n    bash_cores = \"export OMP_NUM_THREADS=1\\n\"\n    bash_environment = [bash_lines + bash_cores]\n\n    # creates call of function for normative modelling\n    if (testrespfile_path is not None) and (testcovfile_path is not None):\n        job_call = [\n            python_path\n            + \" \"\n            + normative_path\n            + \" -c \"\n            + covfile_path\n            + \" -t \"\n            + testcovfile_path\n            + \" -r \"\n            + testrespfile_path\n            + \" -f \"\n            + func\n        ]\n    elif (testrespfile_path is None) and (testcovfile_path is not None):\n        job_call = [\n            python_path\n            + \" \"\n            + normative_path\n            + \" -c \"\n            + covfile_path\n            + \" -t \"\n            + testcovfile_path\n            + \" -f \"\n            + func\n        ]\n    elif cv_folds is not None:\n        job_call = [\n            python_path\n            + \" \"\n            + normative_path\n            + \" -c \"\n            + covfile_path\n            + \" -k \"\n            + str(cv_folds)\n            + \" -f \"\n            + func\n        ]\n    elif func != \"estimate\":\n        job_call = [\n            python_path + \" \" + normative_path + \" -c \" + covfile_path + \" -f \" + func\n        ]\n    else:\n        raise ValueError(\n            \"\"\"For 'estimate' function either testcov or cvfold\n              must be specified.\"\"\"\n        )\n\n    # add algorithm-specific parameters\n    if alg is not None:\n        job_call = [job_call[0] + \" -a \" + alg]\n        if configparam is not None:\n            job_call = [job_call[0] + \" -x \" + str(configparam)]\n\n    # add standardization flag if it is false\n    # if not standardize:\n    #     job_call = [job_call[0] + ' -s']\n\n    # add responses file\n    job_call = [job_call[0] + \" \" + respfile_path]\n\n    # add in optional arguments.\n    for k in kwargs:\n        job_call = [job_call[0] + \" \" + k + \"=\" + str(kwargs[k])]\n\n    # writes bash file into processing dir\n    with open(processing_dir + job_name, \"w\") as bash_file:\n        bash_file.writelines(bash_environment + output_changedir + job_call + [\"\\n\"])\n\n    # changes permissoins for bash.sh file\n    os.chmod(processing_dir + job_name, 0o770)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_parallel.check_job_status","title":"<code>check_job_status(jobs)</code>","text":"<p>A utility function to count the tasks with different status.</p> <p>:param jobs: List of job ids. :return: returns the number of taks athat are queued, running, completed etc</p> Source code in <code>pcntoolkit/normative_parallel.py</code> <pre><code>def check_job_status(jobs):\n    \"\"\"\n    A utility function to count the tasks with different status.\n\n    :param jobs: List of job ids.\n    :return: returns the number of taks athat are queued, running, completed etc\n\n    \"\"\"\n    running_jobs = retrieve_jobs()\n\n    r = 0\n    c = 0\n    q = 0\n    u = 0\n    for job in jobs:\n        try:\n            if running_jobs[job][\"status\"] == \"C\":\n                c += 1\n            elif running_jobs[job][\"status\"] == \"Q\":\n                q += 1\n            elif running_jobs[job][\"status\"] == \"R\":\n                r += 1\n            else:\n                u += 1\n        except:  # probably meanwhile the job is finished.\n            c += 1\n            continue\n\n    print(\n        \"Total Jobs:%d, Queued:%d, Running:%d, Completed:%d, Unknown:%d\"\n        % (len(jobs), q, r, c, u)\n    )\n    return q, r, c, u\n</code></pre>"},{"location":"api/#pcntoolkit.normative_parallel.check_jobs","title":"<code>check_jobs(jobs, delay=60)</code>","text":"<p>A utility function for chacking the status of submitted jobs.</p> <p>:param jobs: list of job ids. :param delay: the delay (in sec) between two consequative checks, defaults to 60.</p> Source code in <code>pcntoolkit/normative_parallel.py</code> <pre><code>def check_jobs(jobs, delay=60):\n    \"\"\"\n    A utility function for chacking the status of submitted jobs.\n\n    :param jobs: list of job ids.\n    :param delay: the delay (in sec) between two consequative checks, defaults to 60.\n\n    \"\"\"\n\n    n = len(jobs)\n\n    while True:\n        q, r, c, u = check_job_status(jobs)\n        if c == n:\n            print(\"All jobs are completed!\")\n            break\n        time.sleep(delay)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_parallel.collect_nm","title":"<code>collect_nm(processing_dir, job_name, func='estimate', collect=False, binary=False, batch_size=None, outputsuffix='_estimate')</code>","text":"<p>Function to checks and collects all batches.</p> <p>Basic usage::</p> <pre><code>collect_nm(processing_dir, job_name)\n</code></pre> <p>:param processing_dir: Full path to the processing directory :param collect: If True data is checked for failed batches and collected; if False data is just checked :param binary: Results in pkl format</p> <p>:outputs: Text or pkl files containing all results accross all batches the combined output (written to disk).</p> <p>:returns 0: if batches fail :returns 1: if bathches complete successfully</p> <p>written by (primarily) T Wolfers, (adapted) SM Kia, (adapted) S Rutherford.</p> Source code in <code>pcntoolkit/normative_parallel.py</code> <pre><code>def collect_nm(\n    processing_dir,\n    job_name,\n    func=\"estimate\",\n    collect=False,\n    binary=False,\n    batch_size=None,\n    outputsuffix=\"_estimate\",\n):\n    \"\"\"Function to checks and collects all batches.\n\n    Basic usage::\n\n        collect_nm(processing_dir, job_name)\n\n\n    :param processing_dir: Full path to the processing directory\n    :param collect: If True data is checked for failed batches and collected; if False data is just checked\n    :param binary: Results in pkl format\n\n    :outputs: Text or pkl files containing all results accross all batches the combined output (written to disk).\n\n    :returns 0: if batches fail\n    :returns 1: if bathches complete successfully\n\n    written by (primarily) T Wolfers, (adapted) SM Kia, (adapted) S Rutherford.\n    \"\"\"\n\n    if binary:\n        file_extentions = \".pkl\"\n    else:\n        file_extentions = \".txt\"\n\n    # detect number of subjects, batches, hyperparameters and CV\n    batches = glob.glob(processing_dir + \"batch_*/\")\n\n    count = 0\n    batch_fail = []\n\n    if func != \"fit\" and func != \"extend\" and func != \"merge\" and func != \"tune\":\n        file_example = []\n        # TODO: Collect_nm only depends on yhat, thus does not work when no\n        # prediction is made (when test cov is not specified).\n        for batch in batches:\n            if file_example == []:\n                file_example = glob.glob(\n                    batch + \"yhat\" + outputsuffix + file_extentions\n                )\n            else:\n                break\n        if binary is False:\n            file_example = fileio.load(file_example[0])\n        else:\n            file_example = pd.read_pickle(file_example[0])\n        numsubjects = file_example.shape[0]\n        try:\n            # doesn't exist if size=1, and txt file\n            batch_size = file_example.shape[1]\n        except:\n            batch_size = 1\n\n        # artificially creates files for batches that were not executed\n        batch_dirs = glob.glob(processing_dir + \"batch_*/\")\n        batch_dirs = fileio.sort_nicely(batch_dirs)\n        for batch in batch_dirs:\n            filepath = glob.glob(batch + \"yhat\" + outputsuffix + \"*\")\n            if filepath == []:\n                count = count + 1\n                batch1 = glob.glob(batch + \"/\" + job_name + \"*.sh\")\n                print(batch1)\n                batch_fail.append(batch1)\n                if collect is True:\n                    pRho = np.ones(batch_size)\n                    pRho = pRho.transpose()\n                    pRho = pd.Series(pRho)\n                    fileio.save(pRho, batch + \"pRho\" + outputsuffix + file_extentions)\n\n                    Rho = np.zeros(batch_size)\n                    Rho = Rho.transpose()\n                    Rho = pd.Series(Rho)\n                    fileio.save(Rho, batch + \"Rho\" + outputsuffix + file_extentions)\n\n                    rmse = np.zeros(batch_size)\n                    rmse = rmse.transpose()\n                    rmse = pd.Series(rmse)\n                    fileio.save(rmse, batch + \"RMSE\" + outputsuffix + file_extentions)\n\n                    smse = np.zeros(batch_size)\n                    smse = smse.transpose()\n                    smse = pd.Series(smse)\n                    fileio.save(smse, batch + \"SMSE\" + outputsuffix + file_extentions)\n\n                    expv = np.zeros(batch_size)\n                    expv = expv.transpose()\n                    expv = pd.Series(expv)\n                    fileio.save(expv, batch + \"EXPV\" + outputsuffix + file_extentions)\n\n                    msll = np.zeros(batch_size)\n                    msll = msll.transpose()\n                    msll = pd.Series(msll)\n                    fileio.save(msll, batch + \"MSLL\" + outputsuffix + file_extentions)\n\n                    yhat = np.zeros([numsubjects, batch_size])\n                    yhat = pd.DataFrame(yhat)\n                    fileio.save(yhat, batch + \"yhat\" + outputsuffix + file_extentions)\n\n                    ys2 = np.zeros([numsubjects, batch_size])\n                    ys2 = pd.DataFrame(ys2)\n                    fileio.save(ys2, batch + \"ys2\" + outputsuffix + file_extentions)\n\n                    Z = np.zeros([numsubjects, batch_size])\n                    Z = pd.DataFrame(Z)\n                    fileio.save(Z, batch + \"Z\" + outputsuffix + file_extentions)\n\n                    nll = np.zeros(batch_size)\n                    nll = nll.transpose()\n                    nll = pd.Series(nll)\n                    fileio.save(nll, batch + \"NLL\" + outputsuffix + file_extentions)\n\n                    bic = np.zeros(batch_size)\n                    bic = bic.transpose()\n                    bic = pd.Series(bic)\n                    fileio.save(bic, batch + \"BIC\" + outputsuffix + file_extentions)\n\n                    if not os.path.isdir(batch + \"Models\"):\n                        os.mkdir(\"Models\")\n\n            else:  # if more than 10% of yhat is nan then it is a failed batch\n                yhat = fileio.load(filepath[0])\n                if np.count_nonzero(~np.isnan(yhat)) / (np.prod(yhat.shape)) &lt; 0.9:\n                    count = count + 1\n                    batch1 = glob.glob(batch + \"/\" + job_name + \"*.sh\")\n                    print(\"More than 10% nans in \" + batch1[0])\n                    batch_fail.append(batch1)\n\n    else:\n        batch_dirs = glob.glob(processing_dir + \"batch_*/\")\n        batch_dirs = fileio.sort_nicely(batch_dirs)\n        for batch in batch_dirs:\n            filepath = glob.glob(batch + \"Models/\" + \"NM_\" + \"*\" + outputsuffix + \"*\")\n            if len(filepath) &lt; batch_size:\n                count = count + 1\n                batch1 = glob.glob(batch + \"/\" + job_name + \"*.sh\")\n                print(batch1)\n                batch_fail.append(batch1)\n\n    # combines all output files across batches\n    if collect is True:\n        pRho_filenames = glob.glob(\n            processing_dir + \"batch_*/\" + \"pRho\" + outputsuffix + \"*\"\n        )\n        if pRho_filenames:\n            pRho_filenames = fileio.sort_nicely(pRho_filenames)\n            pRho_dfs = []\n            for pRho_filename in pRho_filenames:\n                if (\n                    batch_size == 1 and binary is False\n                ):  # if batch size = 1 and .txt file\n                    # from 0d (scalar) to 1d-array\n                    pRho_dfs.append(\n                        pd.DataFrame(fileio.load(pRho_filename)[np.newaxis,])\n                    )\n                else:\n                    pRho_dfs.append(pd.DataFrame(fileio.load(pRho_filename)))\n            pRho_dfs = pd.concat(pRho_dfs, ignore_index=True, axis=0)\n            fileio.save(\n                pRho_dfs, processing_dir + \"pRho\" + outputsuffix + file_extentions\n            )\n            del pRho_dfs\n\n        Rho_filenames = glob.glob(\n            processing_dir + \"batch_*/\" + \"Rho\" + outputsuffix + \"*\"\n        )\n        if Rho_filenames:\n            Rho_filenames = fileio.sort_nicely(Rho_filenames)\n            Rho_dfs = []\n            for Rho_filename in Rho_filenames:\n                if (\n                    batch_size == 1 and binary is False\n                ):  # if batch size = 1 and .txt file\n                    # from 0d (scalar) to 1d-array\n                    Rho_dfs.append(pd.DataFrame(fileio.load(Rho_filename)[np.newaxis,]))\n                else:\n                    Rho_dfs.append(pd.DataFrame(fileio.load(Rho_filename)))\n            Rho_dfs = pd.concat(Rho_dfs, ignore_index=True, axis=0)\n            fileio.save(\n                Rho_dfs, processing_dir + \"Rho\" + outputsuffix + file_extentions\n            )\n            del Rho_dfs\n\n        Z_filenames = glob.glob(processing_dir + \"batch_*/\" + \"Z\" + outputsuffix + \"*\")\n        if Z_filenames:\n            Z_filenames = fileio.sort_nicely(Z_filenames)\n            Z_dfs = []\n            for Z_filename in Z_filenames:\n                Z_dfs.append(pd.DataFrame(fileio.load(Z_filename)))\n            Z_dfs = pd.concat(Z_dfs, ignore_index=True, axis=1)\n            fileio.save(Z_dfs, processing_dir + \"Z\" + outputsuffix + file_extentions)\n            del Z_dfs\n\n        yhat_filenames = glob.glob(\n            processing_dir + \"batch_*/\" + \"yhat\" + outputsuffix + \"*\"\n        )\n        if yhat_filenames:\n            yhat_filenames = fileio.sort_nicely(yhat_filenames)\n            yhat_dfs = []\n            for yhat_filename in yhat_filenames:\n                yhat_dfs.append(pd.DataFrame(fileio.load(yhat_filename)))\n            yhat_dfs = pd.concat(yhat_dfs, ignore_index=True, axis=1)\n            fileio.save(\n                yhat_dfs, processing_dir + \"yhat\" + outputsuffix + file_extentions\n            )\n            del yhat_dfs\n\n        ys2_filenames = glob.glob(\n            processing_dir + \"batch_*/\" + \"ys2\" + outputsuffix + \"*\"\n        )\n        if ys2_filenames:\n            ys2_filenames = fileio.sort_nicely(ys2_filenames)\n            ys2_dfs = []\n            for ys2_filename in ys2_filenames:\n                ys2_dfs.append(pd.DataFrame(fileio.load(ys2_filename)))\n            ys2_dfs = pd.concat(ys2_dfs, ignore_index=True, axis=1)\n            fileio.save(\n                ys2_dfs, processing_dir + \"ys2\" + outputsuffix + file_extentions\n            )\n            del ys2_dfs\n\n        rmse_filenames = glob.glob(\n            processing_dir + \"batch_*/\" + \"RMSE\" + outputsuffix + \"*\"\n        )\n        if rmse_filenames:\n            rmse_filenames = fileio.sort_nicely(rmse_filenames)\n            rmse_dfs = []\n            for rmse_filename in rmse_filenames:\n                if (\n                    batch_size == 1 and binary is False\n                ):  # if batch size = 1 and .txt file\n                    # from 0d (scalar) to 1d-array\n                    rmse_dfs.append(\n                        pd.DataFrame(fileio.load(rmse_filename)[np.newaxis,])\n                    )\n                else:\n                    rmse_dfs.append(pd.DataFrame(fileio.load(rmse_filename)))\n            rmse_dfs = pd.concat(rmse_dfs, ignore_index=True, axis=0)\n            fileio.save(\n                rmse_dfs, processing_dir + \"RMSE\" + outputsuffix + file_extentions\n            )\n            del rmse_dfs\n\n        smse_filenames = glob.glob(\n            processing_dir + \"batch_*/\" + \"SMSE\" + outputsuffix + \"*\"\n        )\n        if smse_filenames:\n            smse_filenames = fileio.sort_nicely(smse_filenames)\n            smse_dfs = []\n            for smse_filename in smse_filenames:\n                if (\n                    batch_size == 1 and binary is False\n                ):  # if batch size = 1 and .txt file\n                    # from 0d (scalar) to 1d-array\n                    smse_dfs.append(\n                        pd.DataFrame(fileio.load(smse_filename)[np.newaxis,])\n                    )\n                else:\n                    smse_dfs.append(pd.DataFrame(fileio.load(smse_filename)))\n            smse_dfs = pd.concat(smse_dfs, ignore_index=True, axis=0)\n            fileio.save(\n                smse_dfs, processing_dir + \"SMSE\" + outputsuffix + file_extentions\n            )\n            del smse_dfs\n\n        expv_filenames = glob.glob(\n            processing_dir + \"batch_*/\" + \"EXPV\" + outputsuffix + \"*\"\n        )\n        if expv_filenames:\n            expv_filenames = fileio.sort_nicely(expv_filenames)\n            expv_dfs = []\n            for expv_filename in expv_filenames:\n                if (\n                    batch_size == 1 and binary is False\n                ):  # if batch size = 1 and .txt file\n                    # from 0d (scalar) to 1d-array\n                    expv_dfs.append(\n                        pd.DataFrame(fileio.load(expv_filename)[np.newaxis,])\n                    )\n                else:\n                    expv_dfs.append(pd.DataFrame(fileio.load(expv_filename)))\n            expv_dfs = pd.concat(expv_dfs, ignore_index=True, axis=0)\n            fileio.save(\n                expv_dfs, processing_dir + \"EXPV\" + outputsuffix + file_extentions\n            )\n            del expv_dfs\n\n        msll_filenames = glob.glob(\n            processing_dir + \"batch_*/\" + \"MSLL\" + outputsuffix + \"*\"\n        )\n        if msll_filenames:\n            msll_filenames = fileio.sort_nicely(msll_filenames)\n            msll_dfs = []\n            for msll_filename in msll_filenames:\n                if (\n                    batch_size == 1 and binary is False\n                ):  # if batch size = 1 and .txt file\n                    # from 0d (scalar) to 1d-array\n                    msll_dfs.append(\n                        pd.DataFrame(fileio.load(msll_filename)[np.newaxis,])\n                    )\n                else:\n                    msll_dfs.append(pd.DataFrame(fileio.load(msll_filename)))\n            msll_dfs = pd.concat(msll_dfs, ignore_index=True, axis=0)\n            fileio.save(\n                msll_dfs, processing_dir + \"MSLL\" + outputsuffix + file_extentions\n            )\n            del msll_dfs\n\n        nll_filenames = glob.glob(\n            processing_dir + \"batch_*/\" + \"NLL\" + outputsuffix + \"*\"\n        )\n        if nll_filenames:\n            nll_filenames = fileio.sort_nicely(nll_filenames)\n            nll_dfs = []\n            for nll_filename in nll_filenames:\n                if (\n                    batch_size == 1 and binary is False\n                ):  # if batch size = 1 and .txt file\n                    # from 0d (scalar) to 1d-array\n                    nll_dfs.append(pd.DataFrame(fileio.load(nll_filename)[np.newaxis,]))\n                else:\n                    nll_dfs.append(pd.DataFrame(fileio.load(nll_filename)))\n            nll_dfs = pd.concat(nll_dfs, ignore_index=True, axis=0)\n            fileio.save(\n                nll_dfs, processing_dir + \"NLL\" + outputsuffix + file_extentions\n            )\n            del nll_dfs\n\n        bic_filenames = glob.glob(\n            processing_dir + \"batch_*/\" + \"BIC\" + outputsuffix + \"*\"\n        )\n        if bic_filenames:\n            bic_filenames = fileio.sort_nicely(bic_filenames)\n            bic_dfs = []\n            for bic_filename in bic_filenames:\n                if (\n                    batch_size == 1 and binary is False\n                ):  # if batch size = 1 and .txt file\n                    # from 0d (scalar) to 1d-array\n                    bic_dfs.append(pd.DataFrame(fileio.load(bic_filename)[np.newaxis,]))\n                else:\n                    bic_dfs.append(pd.DataFrame(fileio.load(bic_filename)))\n            bic_dfs = pd.concat(bic_dfs, ignore_index=True, axis=0)\n            fileio.save(\n                bic_dfs, processing_dir + \"BIC\" + outputsuffix + file_extentions\n            )\n            del bic_dfs\n\n        if (\n            func != \"predict\"\n            and func != \"extend\"\n            and func != \"merge\"\n            and func != \"tune\"\n        ):\n            if not os.path.isdir(processing_dir + \"Models\") and os.path.exists(\n                os.path.join(batches[0], \"Models\")\n            ):\n                os.mkdir(processing_dir + \"Models\")\n\n            meta_filenames = glob.glob(\n                processing_dir + \"batch_*/Models/\" + \"meta_data.md\"\n            )\n            mY = []\n            sY = []\n            X_scalers = []\n            Y_scalers = []\n            if meta_filenames:\n                meta_filenames = fileio.sort_nicely(meta_filenames)\n                with open(meta_filenames[0], \"rb\") as file:\n                    meta_data = pickle.load(file)\n\n                for meta_filename in meta_filenames:\n                    with open(meta_filename, \"rb\") as file:\n                        meta_data = pickle.load(file)\n                    mY.append(meta_data[\"mean_resp\"])\n                    sY.append(meta_data[\"std_resp\"])\n                    if meta_data[\"inscaler\"] in [\"standardize\", \"minmax\", \"robminmax\"]:\n                        X_scalers.append(meta_data[\"scaler_cov\"])\n                    if meta_data[\"outscaler\"] in [\"standardize\", \"minmax\", \"robminmax\"]:\n                        Y_scalers.append(meta_data[\"scaler_resp\"])\n                meta_data[\"mean_resp\"] = np.squeeze(np.column_stack(mY))\n                meta_data[\"std_resp\"] = np.squeeze(np.column_stack(sY))\n                meta_data[\"scaler_cov\"] = X_scalers\n                meta_data[\"scaler_resp\"] = Y_scalers\n\n                with open(\n                    os.path.join(processing_dir, \"Models\", \"meta_data.md\"), \"wb\"\n                ) as file:\n                    pickle.dump(meta_data, file, protocol=PICKLE_PROTOCOL)\n\n            batch_dirs = glob.glob(processing_dir + \"batch_*/\")\n            if batch_dirs:\n                batch_dirs = fileio.sort_nicely(batch_dirs)\n                for b, batch_dir in enumerate(batch_dirs):\n                    src_files = glob.glob(\n                        batch_dir + \"Models/NM*\" + outputsuffix + \".pkl\"\n                    )\n                    if src_files:\n                        src_files = fileio.sort_nicely(src_files)\n                        for f, full_file_name in enumerate(src_files):\n                            if os.path.isfile(full_file_name):\n                                file_name = full_file_name.split(\"/\")[-1]\n                                n = file_name.split(\"_\")\n                                n[-2] = str(b * batch_size + f)\n                                n = \"_\".join(n)\n                                shutil.copy(\n                                    full_file_name, processing_dir + \"Models/\" + n\n                                )\n                    elif func == \"fit\":\n                        count = count + 1\n                        batch1 = glob.glob(batch_dir + \"/\" + job_name + \"*.sh\")\n                        print(\"Failed batch: \" + batch1[0])\n                        batch_fail.append(batch1)\n\n    # list batches that were not executed\n    print(\"Number of batches that failed:\" + str(count))\n    batch_fail_df = pd.DataFrame(batch_fail)\n    if file_extentions == \".txt\":\n        fileio.save_pd(\n            batch_fail_df, processing_dir + \"failed_batches\" + file_extentions\n        )\n    else:\n        fileio.save(batch_fail_df, processing_dir + \"failed_batches\" + file_extentions)\n\n    if not batch_fail:\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"api/#pcntoolkit.normative_parallel.delete_nm","title":"<code>delete_nm(processing_dir, binary=False)</code>","text":"<p>This function deletes all processing for normative modelling and just keeps the combined output.</p> <p>Basic usage::</p> <pre><code>collect_nm(processing_dir)\n</code></pre> <p>:param processing_dir: Full path to the processing directory. :param binary: Results in pkl format.</p> <p>written by (primarily) T Wolfers, (adapted) SM Kia, (adapted) S Rutherford.</p> Source code in <code>pcntoolkit/normative_parallel.py</code> <pre><code>def delete_nm(processing_dir, binary=False):\n    \"\"\"This function deletes all processing for normative modelling and just keeps the combined output.\n\n    Basic usage::\n\n        collect_nm(processing_dir)\n\n    :param processing_dir: Full path to the processing directory.\n    :param binary: Results in pkl format.\n\n    written by (primarily) T Wolfers, (adapted) SM Kia, (adapted) S Rutherford.\n    \"\"\"\n\n    if binary:\n        file_extentions = \".pkl\"\n    else:\n        file_extentions = \".txt\"\n    for file in glob.glob(processing_dir + \"batch_*/\"):\n        shutil.rmtree(file)\n    if os.path.exists(processing_dir + \"failed_batches\" + file_extentions):\n        os.remove(processing_dir + \"failed_batches\" + file_extentions)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_parallel.execute_nm","title":"<code>execute_nm(processing_dir, python_path, job_name, covfile_path, respfile_path, batch_size, memory, duration, normative_path=None, func='estimate', interactive=False, **kwargs)</code>","text":"<p>Execute parallel normative models This function is a mother function that executes all parallel normative modelling routines. Different specifications are possible using the sub- functions.</p> <p>Basic usage::</p> <pre><code>execute_nm(processing_dir, python_path, job_name, covfile_path, respfile_path, batch_size, memory, duration)\n</code></pre> <p>:param processing_dir: Full path to the processing dir :param python_path: Full path to the python distribution :param normative_path: Full path to the normative.py. If None (default) then it will automatically retrieves the path from the installed packeage. :param job_name: Name for the bash script that is the output of this function :param covfile_path: Full path to a .txt file that contains all covariats (subjects x covariates) for the responsefile :param respfile_path: Full path to a .txt that contains all features (subjects x features) :param batch_size: Number of features in each batch :param memory: Memory requirements written as string for example 4gb or 500mb :param duation: The approximate duration of the job, a string with HH:MM:SS for example 01:01:01 :param cv_folds: Number of cross validations :param testcovfile_path: Full path to a .txt file that contains all covariates (subjects x covariates) for the test response file :param testrespfile_path: Full path to a .txt file that contains all test features :param log_path: Path for saving log files :param binary: If True uses binary format for response file otherwise it is text :param interactive: If False (default) the user should manually                     rerun the failed jobs or collect the results.                     If 'auto' the job status are checked until all                     jobs are completed then the failed jobs are rerun                     and the results are automaticallu collectted.                     Using 'query' is similar to 'auto' unless it                     asks for user verification thius is immune to                     endless loop in the case of bugs in the code.</p> <p>written by (primarily) T Wolfers, (adapted) SM Kia The documentation is adapated by S Rutherford.</p> Source code in <code>pcntoolkit/normative_parallel.py</code> <pre><code>def execute_nm(\n    processing_dir,\n    python_path,\n    job_name,\n    covfile_path,\n    respfile_path,\n    batch_size,\n    memory,\n    duration,\n    normative_path=None,\n    func=\"estimate\",\n    interactive=False,\n    **kwargs\n):\n    \"\"\"Execute parallel normative models\n    This function is a mother function that executes all parallel normative\n    modelling routines. Different specifications are possible using the sub-\n    functions.\n\n    Basic usage::\n\n        execute_nm(processing_dir, python_path, job_name, covfile_path, respfile_path, batch_size, memory, duration)\n\n    :param processing_dir: Full path to the processing dir\n    :param python_path: Full path to the python distribution\n    :param normative_path: Full path to the normative.py. If None (default) then it will automatically retrieves the path from the installed packeage.\n    :param job_name: Name for the bash script that is the output of this function\n    :param covfile_path: Full path to a .txt file that contains all covariats (subjects x covariates) for the responsefile\n    :param respfile_path: Full path to a .txt that contains all features (subjects x features)\n    :param batch_size: Number of features in each batch\n    :param memory: Memory requirements written as string for example 4gb or 500mb\n    :param duation: The approximate duration of the job, a string with HH:MM:SS for example 01:01:01\n    :param cv_folds: Number of cross validations\n    :param testcovfile_path: Full path to a .txt file that contains all covariates (subjects x covariates) for the test response file\n    :param testrespfile_path: Full path to a .txt file that contains all test features\n    :param log_path: Path for saving log files\n    :param binary: If True uses binary format for response file otherwise it is text\n    :param interactive: If False (default) the user should manually\n                        rerun the failed jobs or collect the results.\n                        If 'auto' the job status are checked until all\n                        jobs are completed then the failed jobs are rerun\n                        and the results are automaticallu collectted.\n                        Using 'query' is similar to 'auto' unless it\n                        asks for user verification thius is immune to\n                        endless loop in the case of bugs in the code.\n\n    written by (primarily) T Wolfers, (adapted) SM Kia\n    The documentation is adapated by S Rutherford.\n    \"\"\"\n\n    if normative_path is None:\n        normative_path = ptkpath + \"/normative.py\"\n\n    cv_folds = kwargs.get(\"cv_folds\", None)\n    testcovfile_path = kwargs.get(\"testcovfile_path\", None)\n    testrespfile_path = kwargs.get(\"testrespfile_path\", None)\n    outputsuffix = kwargs.get(\"outputsuffix\", \"estimate\")\n    cluster_spec = kwargs.pop(\"cluster_spec\", \"torque\")\n    log_path = kwargs.get(\"log_path\", None)\n    binary = kwargs.pop(\"binary\", False)\n\n    split_nm(processing_dir, respfile_path, batch_size, binary, **kwargs)\n\n    batch_dir = glob.glob(processing_dir + \"batch_*\")\n    # print(batch_dir)\n    number_of_batches = len(batch_dir)\n    # print(number_of_batches)\n\n    if binary:\n        file_extentions = \".pkl\"\n    else:\n        file_extentions = \".txt\"\n\n    kwargs.update({\"batch_size\": str(batch_size)})\n    job_ids = []\n    for n in range(1, number_of_batches + 1):\n        kwargs.update({\"job_id\": str(n)})\n        if testrespfile_path is not None:\n            if cv_folds is not None:\n                raise ValueError(\n                    \"\"\"If the response file is specified\n                                     cv_folds must be equal to None\"\"\"\n                )\n            else:\n                # specified train/test split\n                batch_processing_dir = processing_dir + \"batch_\" + str(n) + \"/\"\n                batch_job_name = job_name + \"_\" + str(n) + \".sh\"\n                batch_respfile_path = (\n                    batch_processing_dir + \"resp_batch_\" + str(n) + file_extentions\n                )\n                batch_testrespfile_path = (\n                    batch_processing_dir + \"testresp_batch_\" + str(n) + file_extentions\n                )\n                batch_job_path = batch_processing_dir + batch_job_name\n                if cluster_spec == \"torque\":\n\n                    # update the response file\n                    kwargs.update({\"testrespfile_path\": batch_testrespfile_path})\n                    bashwrap_nm(\n                        batch_processing_dir,\n                        python_path,\n                        normative_path,\n                        batch_job_name,\n                        covfile_path,\n                        batch_respfile_path,\n                        func=func,\n                        **kwargs\n                    )\n                    job_id = qsub_nm(\n                        job_path=batch_job_path,\n                        log_path=log_path,\n                        memory=memory,\n                        duration=duration,\n                    )\n                    job_ids.append(job_id)\n                elif cluster_spec == \"sbatch\":\n                    # update the response file\n                    kwargs.update({\"testrespfile_path\": batch_testrespfile_path})\n                    sbatchwrap_nm(\n                        batch_processing_dir,\n                        python_path,\n                        normative_path,\n                        batch_job_name,\n                        covfile_path,\n                        batch_respfile_path,\n                        func=func,\n                        memory=memory,\n                        duration=duration,\n                        **kwargs\n                    )\n                    sbatch_nm(job_path=batch_job_path, log_path=log_path)\n                elif cluster_spec == \"new\":\n                    # this part requires addition in different environment [\n                    sbatchwrap_nm(\n                        processing_dir=batch_processing_dir, func=func, **kwargs\n                    )\n                    sbatch_nm(processing_dir=batch_processing_dir)\n                    # ]\n        if testrespfile_path is None:\n            if testcovfile_path is not None:\n                # forward model\n                batch_processing_dir = processing_dir + \"batch_\" + str(n) + \"/\"\n                batch_job_name = job_name + \"_\" + str(n) + \".sh\"\n                batch_respfile_path = (\n                    batch_processing_dir + \"resp_batch_\" + str(n) + file_extentions\n                )\n                batch_job_path = batch_processing_dir + batch_job_name\n                if cluster_spec == \"torque\":\n                    bashwrap_nm(\n                        batch_processing_dir,\n                        python_path,\n                        normative_path,\n                        batch_job_name,\n                        covfile_path,\n                        batch_respfile_path,\n                        func=func,\n                        **kwargs\n                    )\n                    job_id = qsub_nm(\n                        job_path=batch_job_path,\n                        log_path=log_path,\n                        memory=memory,\n                        duration=duration,\n                    )\n                    job_ids.append(job_id)\n                elif cluster_spec == \"sbatch\":\n                    sbatchwrap_nm(\n                        batch_processing_dir,\n                        python_path,\n                        normative_path,\n                        batch_job_name,\n                        covfile_path,\n                        batch_respfile_path,\n                        func=func,\n                        memory=memory,\n                        duration=duration,\n                        **kwargs\n                    )\n                    sbatch_nm(job_path=batch_job_path, log_path=log_path)\n                elif cluster_spec == \"new\":\n                    # this part requires addition in different envioronment [\n                    bashwrap_nm(\n                        processing_dir=batch_processing_dir, func=func, **kwargs\n                    )\n                    qsub_nm(processing_dir=batch_processing_dir)\n                    # ]\n            else:\n                # cross-validation\n                batch_processing_dir = processing_dir + \"batch_\" + str(n) + \"/\"\n                batch_job_name = job_name + \"_\" + str(n) + \".sh\"\n                batch_respfile_path = (\n                    batch_processing_dir + \"resp_batch_\" + str(n) + file_extentions\n                )\n                batch_job_path = batch_processing_dir + batch_job_name\n                if cluster_spec == \"torque\":\n                    bashwrap_nm(\n                        batch_processing_dir,\n                        python_path,\n                        normative_path,\n                        batch_job_name,\n                        covfile_path,\n                        batch_respfile_path,\n                        func=func,\n                        **kwargs\n                    )\n                    job_id = qsub_nm(\n                        job_path=batch_job_path,\n                        log_path=log_path,\n                        memory=memory,\n                        duration=duration,\n                    )\n                    job_ids.append(job_id)\n                elif cluster_spec == \"sbatch\":\n                    sbatchwrap_nm(\n                        batch_processing_dir,\n                        python_path,\n                        normative_path,\n                        batch_job_name,\n                        covfile_path,\n                        batch_respfile_path,\n                        func=func,\n                        memory=memory,\n                        duration=duration,\n                        **kwargs\n                    )\n                    sbatch_nm(job_path=batch_job_path, log_path=log_path)\n                elif cluster_spec == \"new\":\n                    # this part requires addition in different envioronment [\n                    bashwrap_nm(\n                        processing_dir=batch_processing_dir, func=func, **kwargs\n                    )\n                    qsub_nm(processing_dir=batch_processing_dir)\n                    # ]\n\n    if interactive:\n\n        check_jobs(job_ids, delay=60)\n\n        success = False\n        while not success:\n            success = collect_nm(\n                processing_dir,\n                job_name,\n                func=func,\n                collect=False,\n                binary=binary,\n                batch_size=batch_size,\n                outputsuffix=outputsuffix,\n            )\n            if success:\n                break\n            else:\n                if interactive == \"query\":\n                    response = yes_or_no(\"Rerun the failed jobs?\")\n                    if response:\n                        rerun_nm(\n                            processing_dir,\n                            log_path=log_path,\n                            memory=memory,\n                            duration=duration,\n                            binary=binary,\n                            interactive=interactive,\n                        )\n                    else:\n                        success = True\n                else:\n                    print(\"Rerunning the failed jobs ...\")\n                    rerun_nm(\n                        processing_dir,\n                        log_path=log_path,\n                        memory=memory,\n                        duration=duration,\n                        binary=binary,\n                        interactive=interactive,\n                    )\n\n        if interactive == \"query\":\n            response = yes_or_no(\"Collect the results?\")\n            if response:\n                success = collect_nm(\n                    processing_dir,\n                    job_name,\n                    func=func,\n                    collect=True,\n                    binary=binary,\n                    batch_size=batch_size,\n                    outputsuffix=outputsuffix,\n                )\n        else:\n            print(\"Collecting the results ...\")\n            success = collect_nm(\n                processing_dir,\n                job_name,\n                func=func,\n                collect=True,\n                binary=binary,\n                batch_size=batch_size,\n                outputsuffix=outputsuffix,\n            )\n</code></pre>"},{"location":"api/#pcntoolkit.normative_parallel.qsub_nm","title":"<code>qsub_nm(job_path, log_path, memory, duration)</code>","text":"<p>This function submits a job.sh scipt to the torque custer using the qsub command.</p> <p>Basic usage::</p> <pre><code>qsub_nm(job_path, log_path, memory, duration)\n</code></pre> <p>:param job_path: Full path to the job.sh file. :param memory: Memory requirements written as string for example 4gb or 500mb. :param duation: The approximate duration of the job, a string with HH:MM:SS for example 01:01:01.</p> <p>:outputs: Submission of the job to the (torque) cluster.</p> <p>written by (primarily) T Wolfers, (adapted) SM Kia, (adapted) S Rutherford.</p> Source code in <code>pcntoolkit/normative_parallel.py</code> <pre><code>def qsub_nm(job_path, log_path, memory, duration):\n    \"\"\"This function submits a job.sh scipt to the torque custer using the qsub command.\n\n    Basic usage::\n\n\n        qsub_nm(job_path, log_path, memory, duration)\n\n    :param job_path: Full path to the job.sh file.\n    :param memory: Memory requirements written as string for example 4gb or 500mb.\n    :param duation: The approximate duration of the job, a string with HH:MM:SS for example 01:01:01.\n\n    :outputs: Submission of the job to the (torque) cluster.\n\n    written by (primarily) T Wolfers, (adapted) SM Kia, (adapted) S Rutherford.\n    \"\"\"\n\n    # created qsub command\n    if log_path is None:\n        qsub_call = [\n            \"echo \"\n            + job_path\n            + \" | qsub -N \"\n            + job_path\n            + \" -l \"\n            + \"procs=1\"\n            + \",mem=\"\n            + memory\n            + \",walltime=\"\n            + duration\n        ]\n    else:\n        qsub_call = [\n            \"echo \"\n            + job_path\n            + \" | qsub -N \"\n            + job_path\n            + \" -l \"\n            + \"procs=1\"\n            + \",mem=\"\n            + memory\n            + \",walltime=\"\n            + duration\n            + \" -o \"\n            + log_path\n            + \" -e \"\n            + log_path\n        ]\n\n    # submits job to cluster\n    # call(qsub_call, shell=True)\n    job_id = (\n        check_output(qsub_call, shell=True)\n        .decode(sys.stdout.encoding)\n        .replace(\"\\n\", \"\")\n    )\n\n    return job_id\n</code></pre>"},{"location":"api/#pcntoolkit.normative_parallel.rerun_nm","title":"<code>rerun_nm(processing_dir, log_path, memory, duration, binary=False, interactive=False)</code>","text":"<p>This function reruns all failed batched in processing_dir after collect_nm has identified the failed batches. Basic usage::</p> <pre><code>rerun_nm(processing_dir, log_path, memory, duration)\n</code></pre> <p>:param processing_dir: Full path to the processing directory :param memory: Memory requirements written as string for example 4gb or 500mb. :param duration: The approximate duration of the job, a string with HH:MM:SS for example 01:01:01.</p> <p>written by (primarily) T Wolfers, (adapted) SM Kia, (adapted) S Rutherford.</p> Source code in <code>pcntoolkit/normative_parallel.py</code> <pre><code>def rerun_nm(\n    processing_dir, log_path, memory, duration, binary=False, interactive=False\n):\n    \"\"\"This function reruns all failed batched in processing_dir after collect_nm has identified the failed batches.\n    Basic usage::\n\n        rerun_nm(processing_dir, log_path, memory, duration)\n\n    :param processing_dir: Full path to the processing directory\n    :param memory: Memory requirements written as string for example 4gb or 500mb.\n    :param duration: The approximate duration of the job, a string with HH:MM:SS for example 01:01:01.\n\n    written by (primarily) T Wolfers, (adapted) SM Kia, (adapted) S Rutherford.\n    \"\"\"\n\n    job_ids = []\n\n    if binary:\n        file_extentions = \".pkl\"\n        failed_batches = fileio.load(\n            processing_dir + \"failed_batches\" + file_extentions\n        )\n        shape = failed_batches.shape\n        for n in range(0, shape[0]):\n            jobpath = failed_batches[n, 0]\n            print(jobpath)\n            job_id = qsub_nm(\n                job_path=jobpath, log_path=log_path, memory=memory, duration=duration\n            )\n            job_ids.append(job_id)\n    else:\n        file_extentions = \".txt\"\n        failed_batches = fileio.load_pd(\n            processing_dir + \"failed_batches\" + file_extentions\n        )\n        shape = failed_batches.shape\n        for n in range(0, shape[0]):\n            jobpath = failed_batches.iloc[n, 0]\n            print(jobpath)\n            job_id = qsub_nm(\n                job_path=jobpath, log_path=log_path, memory=memory, duration=duration\n            )\n            job_ids.append(job_id)\n\n    if interactive:\n        check_jobs(job_ids, delay=60)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_parallel.retrieve_jobs","title":"<code>retrieve_jobs()</code>","text":"<p>A utility function to retrieve task status from the outputs of qstat.</p> <p>:return: a dictionary of jobs.</p> Source code in <code>pcntoolkit/normative_parallel.py</code> <pre><code>def retrieve_jobs():\n    \"\"\"\n    A utility function to retrieve task status from the outputs of qstat.\n\n    :return: a dictionary of jobs.\n\n    \"\"\"\n\n    output = check_output(\"qstat\", shell=True).decode(sys.stdout.encoding)\n    output = output.split(\"\\n\")\n    jobs = dict()\n    for line in output[2:-1]:\n        (Job_ID, Job_Name, User, Wall_Time, Status, Queue) = line.split()\n        jobs[Job_ID] = dict()\n        jobs[Job_ID][\"name\"] = Job_Name\n        jobs[Job_ID][\"walltime\"] = Wall_Time\n        jobs[Job_ID][\"status\"] = Status\n\n    return jobs\n</code></pre>"},{"location":"api/#pcntoolkit.normative_parallel.sbatch_nm","title":"<code>sbatch_nm(job_path, log_path)</code>","text":"<p>This function submits a job.sh scipt to the torque custer using the qsub command.</p> <p>Basic usage::</p> <pre><code>sbatch_nm(job_path, log_path)\n</code></pre> <p>:param job_path: Full path to the job.sh file :param log_path: The logs are currently stored in the working dir</p> <p>:outputs: Submission of the job to the (torque) cluster.</p> <p>written by (primarily) T Wolfers, (adapted) S Rutherford.</p> Source code in <code>pcntoolkit/normative_parallel.py</code> <pre><code>def sbatch_nm(job_path, log_path):\n    \"\"\"This function submits a job.sh scipt to the torque custer using the qsub\n    command.\n\n    Basic usage::\n\n        sbatch_nm(job_path, log_path)\n\n    :param job_path: Full path to the job.sh file\n    :param log_path: The logs are currently stored in the working dir\n\n    :outputs: Submission of the job to the (torque) cluster.\n\n    written by (primarily) T Wolfers, (adapted) S Rutherford.\n    \"\"\"\n\n    # created qsub command\n    sbatch_call = [\"sbatch \" + job_path]\n\n    # submits job to cluster\n    call(sbatch_call, shell=True)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_parallel.sbatchrerun_nm","title":"<code>sbatchrerun_nm(processing_dir, memory, duration, new_memory=False, new_duration=False, binary=False, **kwargs)</code>","text":"<p>This function reruns all failed batched in processing_dir after collect_nm has identified he failed batches.</p> <p>Basic usage::</p> <pre><code>rerun_nm(processing_dir, memory, duration)\n</code></pre> <p>:param processing_dir: Full path to the processing directory. :param memory: Memory requirements written as string, for example 4gb or 500mb. :param duration: The approximate duration of the job, a string with HH:MM:SS for example 01:01:01. :param new_memory: If you want to change the memory you have to indicate it here. :param new_duration: If you want to change the duration you have to indicate it here.</p> <p>:outputs: Re-runs failed batches.</p> <p>written by (primarily) T Wolfers, (adapted) S Rutherford.</p> Source code in <code>pcntoolkit/normative_parallel.py</code> <pre><code>def sbatchrerun_nm(\n    processing_dir,\n    memory,\n    duration,\n    new_memory=False,\n    new_duration=False,\n    binary=False,\n    **kwargs\n):\n    \"\"\"This function reruns all failed batched in processing_dir after collect_nm has identified he failed batches.\n\n    Basic usage::\n\n        rerun_nm(processing_dir, memory, duration)\n\n    :param processing_dir: Full path to the processing directory.\n    :param memory: Memory requirements written as string, for example 4gb or 500mb.\n    :param duration: The approximate duration of the job, a string with HH:MM:SS for example 01:01:01.\n    :param new_memory: If you want to change the memory you have to indicate it here.\n    :param new_duration: If you want to change the duration you have to indicate it here.\n\n    :outputs: Re-runs failed batches.\n\n     written by (primarily) T Wolfers, (adapted) S Rutherford.\n    \"\"\"\n    log_path = kwargs.pop(\"log_path\", None)\n\n    if binary:\n        file_extentions = \".pkl\"\n        failed_batches = fileio.load(\n            processing_dir + \"failed_batches\" + file_extentions\n        )\n        shape = failed_batches.shape\n        for n in range(0, shape[0]):\n            jobpath = failed_batches[n, 0]\n            print(jobpath)\n            if new_duration != False:\n                with fileinput.FileInput(jobpath, inplace=True) as file:\n                    for line in file:\n                        print(line.replace(duration, new_duration), end=\"\")\n                if new_memory != False:\n                    with fileinput.FileInput(jobpath, inplace=True) as file:\n                        for line in file:\n                            print(line.replace(memory, new_memory), end=\"\")\n                sbatch_nm(jobpath, log_path)\n\n    else:\n        file_extentions = \".txt\"\n        failed_batches = fileio.load_pd(\n            processing_dir + \"failed_batches\" + file_extentions\n        )\n        shape = failed_batches.shape\n        for n in range(0, shape[0]):\n            jobpath = failed_batches.iloc[n, 0]\n            print(jobpath)\n            if new_duration != False:\n                with fileinput.FileInput(jobpath, inplace=True) as file:\n                    for line in file:\n                        print(line.replace(duration, new_duration), end=\"\")\n                if new_memory != False:\n                    with fileinput.FileInput(jobpath, inplace=True) as file:\n                        for line in file:\n                            print(line.replace(memory, new_memory), end=\"\")\n                sbatch_nm(jobpath, log_path)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_parallel.sbatchwrap_nm","title":"<code>sbatchwrap_nm(processing_dir, python_path, normative_path, job_name, covfile_path, respfile_path, memory, duration, func='estimate', **kwargs)</code>","text":"<p>This function wraps normative modelling into a bash script to run it on a torque cluster system.</p> <p>Basic usage::</p> <pre><code>sbatchwrap_nm(processing_dir, python_path, normative_path, job_name, covfile_path, respfile_path, memory, duration)\n</code></pre> <p>:param processing_dir: Full path to the processing dir :param python_path: Full path to the python distribution :param normative_path: Full path to the normative.py :param job_name: Name for the bash script that is the output of this function :param covfile_path: Full path to a .txt file that contains all covariates (subjects x covariates) for the responsefile :param respfile_path: Full path to a .txt that contains all features (subjects x features) :param cv_folds: Number of cross validations :param testcovfile_path: Full path to a .txt file that contains all covariates (subjects x covariates) for the testresponse file :param testrespfile_path: Full path to a .txt file that contains all test features :param alg: which algorithm to use :param configparam: configuration parameters for this algorithm</p> <p>:outputs: A bash.sh file containing the commands for normative modelling saved to the processing directory (written to disk).</p> <p>written by (primarily) T Wolfers, (adapted) S Rutherford</p> Source code in <code>pcntoolkit/normative_parallel.py</code> <pre><code>def sbatchwrap_nm(\n    processing_dir,\n    python_path,\n    normative_path,\n    job_name,\n    covfile_path,\n    respfile_path,\n    memory,\n    duration,\n    func=\"estimate\",\n    **kwargs\n):\n    \"\"\"This function wraps normative modelling into a bash script to run it\n    on a torque cluster system.\n\n    Basic usage::\n\n        sbatchwrap_nm(processing_dir, python_path, normative_path, job_name, covfile_path, respfile_path, memory, duration)\n\n    :param processing_dir: Full path to the processing dir\n    :param python_path: Full path to the python distribution\n    :param normative_path: Full path to the normative.py\n    :param job_name: Name for the bash script that is the output of this function\n    :param covfile_path: Full path to a .txt file that contains all covariates (subjects x covariates) for the responsefile\n    :param respfile_path: Full path to a .txt that contains all features (subjects x features)\n    :param cv_folds: Number of cross validations\n    :param testcovfile_path: Full path to a .txt file that contains all covariates (subjects x covariates) for the testresponse file\n    :param testrespfile_path: Full path to a .txt file that contains all test features\n    :param alg: which algorithm to use\n    :param configparam: configuration parameters for this algorithm\n\n    :outputs: A bash.sh file containing the commands for normative modelling saved to the processing directory (written to disk).\n\n    written by (primarily) T Wolfers, (adapted) S Rutherford\n    \"\"\"\n\n    # here we use pop not get to remove the arguments as they used\n    cv_folds = kwargs.pop(\"cv_folds\", None)\n    testcovfile_path = kwargs.pop(\"testcovfile_path\", None)\n    testrespfile_path = kwargs.pop(\"testrespfile_path\", None)\n    alg = kwargs.pop(\"alg\", None)\n    configparam = kwargs.pop(\"configparam\", None)\n\n    # change to processing dir\n    os.chdir(processing_dir)\n    output_changedir = [\"cd \" + processing_dir + \"\\n\"]\n\n    sbatch_init = \"#!/bin/bash\\n\"\n    sbatch_jobname = \"#SBATCH --job-name=\" + processing_dir + \"\\n\"\n    sbatch_account = \"#SBATCH --account=p33_norment\\n\"\n    sbatch_nodes = \"#SBATCH --nodes=1\\n\"\n    sbatch_tasks = \"#SBATCH --ntasks=1\\n\"\n    sbatch_time = \"#SBATCH --time=\" + str(duration) + \"\\n\"\n    sbatch_memory = \"#SBATCH --mem-per-cpu=\" + str(memory) + \"\\n\"\n    sbatch_module = \"module purge\\n\"\n    sbatch_anaconda = \"module load anaconda3\\n\"\n    sbatch_exit = \"set -o errexit\\n\"\n\n    # echo -n \"This script is running on \"\n    # hostname\n\n    bash_environment = [\n        sbatch_init\n        + sbatch_jobname\n        + sbatch_account\n        + sbatch_nodes\n        + sbatch_tasks\n        + sbatch_time\n        + sbatch_module\n        + sbatch_anaconda\n    ]\n\n    # creates call of function for normative modelling\n    if (testrespfile_path is not None) and (testcovfile_path is not None):\n        job_call = [\n            python_path\n            + \" \"\n            + normative_path\n            + \" -c \"\n            + covfile_path\n            + \" -t \"\n            + testcovfile_path\n            + \" -r \"\n            + testrespfile_path\n            + \" -f \"\n            + func\n        ]\n    elif (testrespfile_path is None) and (testcovfile_path is not None):\n        job_call = [\n            python_path\n            + \" \"\n            + normative_path\n            + \" -c \"\n            + covfile_path\n            + \" -t \"\n            + testcovfile_path\n            + \" -f \"\n            + func\n        ]\n    elif cv_folds is not None:\n        job_call = [\n            python_path\n            + \" \"\n            + normative_path\n            + \" -c \"\n            + covfile_path\n            + \" -k \"\n            + str(cv_folds)\n            + \" -f \"\n            + func\n        ]\n    elif func != \"estimate\":\n        job_call = [\n            python_path + \" \" + normative_path + \" -c \" + covfile_path + \" -f \" + func\n        ]\n    else:\n        raise ValueError(\n            \"\"\"For 'estimate' function either testcov or cvfold\n              must be specified.\"\"\"\n        )\n\n    # add algorithm-specific parameters\n    if alg is not None:\n        job_call = [job_call[0] + \" -a \" + alg]\n        if configparam is not None:\n            job_call = [job_call[0] + \" -x \" + str(configparam)]\n\n    # add standardization flag if it is false\n    # if not standardize:\n    #     job_call = [job_call[0] + ' -s']\n\n    # add responses file\n    job_call = [job_call[0] + \" \" + respfile_path]\n\n    # add in optional arguments.\n    for k in kwargs:\n        job_call = [job_call[0] + \" \" + k + \"=\" + kwargs[k]]\n\n    # writes bash file into processing dir\n    with open(processing_dir + job_name, \"w\") as bash_file:\n        bash_file.writelines(\n            bash_environment + output_changedir + job_call + [\"\\n\"] + [sbatch_exit]\n        )\n\n    # changes permissoins for bash.sh file\n    os.chmod(processing_dir + job_name, 0o770)\n</code></pre>"},{"location":"api/#pcntoolkit.normative_parallel.split_nm","title":"<code>split_nm(processing_dir, respfile_path, batch_size, binary, **kwargs)</code>","text":"<p>This function prepares the input files for normative_parallel.</p> <p>Basic usage::</p> <pre><code>split_nm(processing_dir, respfile_path, batch_size, binary, testrespfile_path)\n</code></pre> <p>:param processing_dir: Full path to the processing dir :param respfile_path: Full path to the responsefile.txt (subjects x features) :param batch_size: Number of features in each batch :param testrespfile_path: Full path to the test responsefile.txt (subjects x features) :param binary: If True binary file</p> <p>:outputs: The creation of a folder struture for batch-wise processing.</p> <p>witten by (primarily) T Wolfers (adapted) SM Kia, (adapted) S Rutherford.</p> Source code in <code>pcntoolkit/normative_parallel.py</code> <pre><code>def split_nm(processing_dir, respfile_path, batch_size, binary, **kwargs):\n    \"\"\"This function prepares the input files for normative_parallel.\n\n    Basic usage::\n\n        split_nm(processing_dir, respfile_path, batch_size, binary, testrespfile_path)\n\n    :param processing_dir: Full path to the processing dir\n    :param respfile_path: Full path to the responsefile.txt (subjects x features)\n    :param batch_size: Number of features in each batch\n    :param testrespfile_path: Full path to the test responsefile.txt (subjects x features)\n    :param binary: If True binary file\n\n    :outputs: The creation of a folder struture for batch-wise processing.\n\n    witten by (primarily) T Wolfers (adapted) SM Kia, (adapted) S Rutherford.\n    \"\"\"\n\n    testrespfile_path = kwargs.pop(\"testrespfile_path\", None)\n\n    dummy, respfile_extension = os.path.splitext(respfile_path)\n    if binary and respfile_extension != \".pkl\":\n        raise ValueError(\n            \"\"\"If binary is True the file format for the\n              testrespfile file must be .pkl\"\"\"\n        )\n    elif binary == False and respfile_extension != \".txt\":\n        raise ValueError(\n            \"\"\"If binary is False the file format for the\n              testrespfile file must be .txt\"\"\"\n        )\n\n    # splits response into batches\n    if testrespfile_path is None:\n        if binary == False:\n            respfile = fileio.load_ascii(respfile_path)\n        else:\n            respfile = pd.read_pickle(respfile_path)\n\n        respfile = pd.DataFrame(respfile)\n\n        numsub = respfile.shape[1]\n        batch_vec = np.arange(0, numsub, batch_size)\n        batch_vec = np.append(batch_vec, numsub)\n\n        for n in range(0, (len(batch_vec) - 1)):\n            resp_batch = respfile.iloc[:, (batch_vec[n]) : batch_vec[n + 1]]\n            os.chdir(processing_dir)\n            resp = str(\"resp_batch_\" + str(n + 1))\n            batch = str(\"batch_\" + str(n + 1))\n            if not os.path.exists(processing_dir + batch):\n                os.makedirs(processing_dir + batch)\n                os.makedirs(processing_dir + batch + \"/Models/\")\n            if binary == False:\n                fileio.save_pd(resp_batch, processing_dir + batch + \"/\" + resp + \".txt\")\n            else:\n                resp_batch.to_pickle(\n                    processing_dir + batch + \"/\" + resp + \".pkl\",\n                    protocol=PICKLE_PROTOCOL,\n                )\n\n    # splits response and test responsefile into batches\n    else:\n        dummy, testrespfile_extension = os.path.splitext(testrespfile_path)\n        if binary and testrespfile_extension != \".pkl\":\n            raise ValueError(\n                \"\"\"If binary is True the file format for the\n                  testrespfile file must be .pkl\"\"\"\n            )\n        elif binary == False and testrespfile_extension != \".txt\":\n            raise ValueError(\n                \"\"\"If binary is False the file format for the\n                  testrespfile file must be .txt\"\"\"\n            )\n\n        if binary == False:\n            respfile = fileio.load_ascii(respfile_path)\n            testrespfile = fileio.load_ascii(testrespfile_path)\n        else:\n            respfile = pd.read_pickle(respfile_path)\n            testrespfile = pd.read_pickle(testrespfile_path)\n\n        respfile = pd.DataFrame(respfile)\n        testrespfile = pd.DataFrame(testrespfile)\n\n        numsub = respfile.shape[1]\n        batch_vec = np.arange(0, numsub, batch_size)\n        batch_vec = np.append(batch_vec, numsub)\n        for n in range(0, (len(batch_vec) - 1)):\n            resp_batch = respfile.iloc[:, (batch_vec[n]) : batch_vec[n + 1]]\n            testresp_batch = testrespfile.iloc[:, (batch_vec[n]) : batch_vec[n + 1]]\n            os.chdir(processing_dir)\n            resp = str(\"resp_batch_\" + str(n + 1))\n            testresp = str(\"testresp_batch_\" + str(n + 1))\n            batch = str(\"batch_\" + str(n + 1))\n            if not os.path.exists(processing_dir + batch):\n                os.makedirs(processing_dir + batch)\n                os.makedirs(processing_dir + batch + \"/Models/\")\n            if binary == False:\n                fileio.save_pd(resp_batch, processing_dir + batch + \"/\" + resp + \".txt\")\n                fileio.save_pd(\n                    testresp_batch, processing_dir + batch + \"/\" + testresp + \".txt\"\n                )\n            else:\n                resp_batch.to_pickle(\n                    processing_dir + batch + \"/\" + resp + \".pkl\",\n                    protocol=PICKLE_PROTOCOL,\n                )\n                testresp_batch.to_pickle(\n                    processing_dir + batch + \"/\" + testresp + \".pkl\",\n                    protocol=PICKLE_PROTOCOL,\n                )\n</code></pre>"},{"location":"api/#pcntoolkit.plotting","title":"<code>plotting</code>","text":""},{"location":"api/#pcntoolkit.plotting.plotter","title":"<code>plotter</code>","text":"<p>A module for plotting functions.</p>"},{"location":"api/#pcntoolkit.plotting.plotter.plot_centiles","title":"<code>plot_centiles(model: NormBase, data: NormData, covariate: str | None = None, cummul_densities: list | None = None, batch_effects: Dict[str, List[str]] | None = None, show_data: bool = False, plt_kwargs: dict | None = None, hue_data: str = 'site', markers_data: str = 'sex') -&gt; None</code>","text":"<p>Generate centile plots for response variables with optional data overlay.</p> <p>This function creates visualization of centile curves for all response variables in the dataset. It can optionally show the actual data points overlaid on the centile curves, with customizable styling based on categorical variables.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>NormBase</code> <p>The fitted normative model used to generate centile predictions.</p> required <code>data</code> <code>NormData</code> <p>Dataset containing covariates and response variables to be plotted.</p> required <code>covariate</code> <code>str | None</code> <p>Name of the covariate to plot on x-axis. If None, uses the first covariate in the dataset, by default None.</p> <code>None</code> <code>cummul_densities</code> <code>list | None</code> <p>List of cumulative density function values to plot as centiles. If None, uses model's default values, by default None.</p> <code>None</code> <code>batch_effects</code> <code>Dict[str, List[str]] | None</code> <p>Specification of batch effects for plotting. Format: {'batch_effect_name': ['value1', 'value2', ...]} For models with random effects, specifies which batch effect values to use for centile computation (first value in each list). For data visualization, specifies which batch effects to highlight. By default None.</p> <code>None</code> <code>show_data</code> <code>bool</code> <p>If True, overlays actual data points on the centile curves. Points matching batch_effects are highlighted, others are shown in light gray, by default False.</p> <code>False</code> <code>plt_kwargs</code> <code>dict | None</code> <p>Additional keyword arguments passed to plt.figure(), by default None.</p> <code>None</code> <code>hue_data</code> <code>str</code> <p>Column name in data used for color-coding points when show_data=True, by default \"site\".</p> <code>'site'</code> <code>markers_data</code> <code>str</code> <p>Column name in data used for marker styles when show_data=True, by default \"sex\".</p> <code>'sex'</code> <p>Returns:</p> Type Description <code>None</code> <p>Displays the plot using matplotlib.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If batch_effects dictionary contains invalid value types.</p> Notes <ul> <li>Centile lines are styled differently based on their distance from the median:</li> <li>Solid lines for centiles close to median (|cdf - 0.5| &lt; 0.25)</li> <li>Dashed lines for intermediate centiles (0.25 \u2264 |cdf - 0.5| &lt; 0.475)</li> <li>Dotted lines for extreme centiles (|cdf - 0.5| \u2265 0.475)</li> <li>CDF values are displayed at both ends of each centile line</li> <li>When showing data with batch effects, matching points are highlighted   while others are shown in gray with reduced opacity</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic centile plot\n&gt;&gt;&gt; plot_centiles(model, data, covariate='age')\n</code></pre> <pre><code>&gt;&gt;&gt; # With data overlay and batch effects\n&gt;&gt;&gt; plot_centiles(\n...     model, \n...     data,\n...     covariate='age',\n...     batch_effects={'site': ['site1', 'site2']},\n...     show_data=True,\n...     hue_data='diagnosis',\n...     markers_data='sex'\n... )\n</code></pre> Source code in <code>pcntoolkit/plotting/plotter.py</code> <pre><code>def plot_centiles(\n    model: NormBase,\n    data: NormData,\n    covariate: str | None = None,\n    cummul_densities: list | None = None,\n    batch_effects: Dict[str, List[str]] | None = None,\n    show_data: bool = False,\n    plt_kwargs: dict | None = None,\n    hue_data: str = \"site\",\n    markers_data: str = \"sex\",\n) -&gt; None:\n    \"\"\"Generate centile plots for response variables with optional data overlay.\n\n    This function creates visualization of centile curves for all response variables\n    in the dataset. It can optionally show the actual data points overlaid on the\n    centile curves, with customizable styling based on categorical variables.\n\n    Parameters\n    ----------\n    model : NormBase\n        The fitted normative model used to generate centile predictions.\n    data : NormData\n        Dataset containing covariates and response variables to be plotted.\n    covariate : str | None, optional\n        Name of the covariate to plot on x-axis. If None, uses the first\n        covariate in the dataset, by default None.\n    cummul_densities : list | None, optional\n        List of cumulative density function values to plot as centiles.\n        If None, uses model's default values, by default None.\n    batch_effects : Dict[str, List[str]] | None, optional\n        Specification of batch effects for plotting. Format:\n        {'batch_effect_name': ['value1', 'value2', ...]}\n        For models with random effects, specifies which batch effect values\n        to use for centile computation (first value in each list).\n        For data visualization, specifies which batch effects to highlight.\n        By default None.\n    show_data : bool, optional\n        If True, overlays actual data points on the centile curves.\n        Points matching batch_effects are highlighted, others are shown\n        in light gray, by default False.\n    plt_kwargs : dict | None, optional\n        Additional keyword arguments passed to plt.figure(),\n        by default None.\n    hue_data : str, optional\n        Column name in data used for color-coding points when show_data=True,\n        by default \"site\".\n    markers_data : str, optional\n        Column name in data used for marker styles when show_data=True,\n        by default \"sex\".\n\n    Returns\n    -------\n    None\n        Displays the plot using matplotlib.\n\n    Raises\n    ------\n    ValueError\n        If batch_effects dictionary contains invalid value types.\n\n    Notes\n    -----\n    - Centile lines are styled differently based on their distance from the median:\n      - Solid lines for centiles close to median (|cdf - 0.5| &lt; 0.25)\n      - Dashed lines for intermediate centiles (0.25 \u2264 |cdf - 0.5| &lt; 0.475)\n      - Dotted lines for extreme centiles (|cdf - 0.5| \u2265 0.475)\n    - CDF values are displayed at both ends of each centile line\n    - When showing data with batch effects, matching points are highlighted\n      while others are shown in gray with reduced opacity\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Basic centile plot\n    &gt;&gt;&gt; plot_centiles(model, data, covariate='age')\n\n    &gt;&gt;&gt; # With data overlay and batch effects\n    &gt;&gt;&gt; plot_centiles(\n    ...     model, \n    ...     data,\n    ...     covariate='age',\n    ...     batch_effects={'site': ['site1', 'site2']},\n    ...     show_data=True,\n    ...     hue_data='diagnosis',\n    ...     markers_data='sex'\n    ... )\n    \"\"\"\n    if covariate is None:\n        covariate = data.covariates[0].to_numpy().item()\n        assert isinstance(covariate, str)\n\n    if batch_effects is None:\n        if model.has_random_effect:\n            batch_effects = data.get_single_batch_effect()\n        else:\n            batch_effects = {}\n\n    # Ensure that the batch effects are in the correct format\n    if batch_effects:\n        for k, v in batch_effects.items():\n            if isinstance(v, str):\n                batch_effects[k] = [v]\n            elif not isinstance(v, list):\n                raise ValueError(\n                    f\"Items of the batch_effect dict be a list or a string, not {type(v)}\"\n                )\n\n    if plt_kwargs is None:\n        plt_kwargs = {}\n    palette = plt_kwargs.pop(\"cmap\", \"viridis\")\n    synth_data = data.create_synthetic_data(\n        n_datapoints=150,\n        range_dim=covariate,\n        batch_effects_to_sample={k: [v[0]] for k, v in batch_effects.items()}\n        if batch_effects\n        else None,\n    )\n    model.compute_centiles(synth_data, cdf=cummul_densities)\n    for response_var in data.coords[\"response_vars\"].to_numpy():\n        _plot_centiles(\n            data=data,\n            synth_data=synth_data,\n            response_var=response_var,\n            covariate=covariate,\n            batch_effects=batch_effects,\n            show_data=show_data,\n            plt_kwargs=plt_kwargs,\n            hue_data=hue_data,\n            markers_data=markers_data,\n            palette=palette,\n        )\n</code></pre>"},{"location":"api/#pcntoolkit.plotting.plotter.plot_qq","title":"<code>plot_qq(data: NormData, plt_kwargs: dict | None = None, bound: int | float = 0, plot_id_line: bool = False, hue_data: str | None = None, markers_data: str | None = None, split_data: str | None = None, seed: int = 42) -&gt; None</code>","text":"<p>Plot QQ plots for each response variable in the data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NormData</code> <p>Data containing the response variables.</p> required <code>plt_kwargs</code> <code>dict or None</code> <p>Additional keyword arguments for the plot. Defaults to None.</p> <code>None</code> <code>bound</code> <code>int or float</code> <p>Axis limits for the plot. Defaults to 0.</p> <code>0</code> <code>plot_id_line</code> <code>bool</code> <p>Whether to plot the identity line. Defaults to False.</p> <code>False</code> <code>hue_data</code> <code>str or None</code> <p>Column to use for coloring. Defaults to None.</p> <code>None</code> <code>markers_data</code> <code>str or None</code> <p>Column to use for marker styling. Defaults to None.</p> <code>None</code> <code>split_data</code> <code>str or None</code> <p>Column to use for splitting data. Defaults to None.</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; plot_qq(data, plt_kwargs={'figsize': (10, 6)}, bound=3)\n</code></pre> Source code in <code>pcntoolkit/plotting/plotter.py</code> <pre><code>def plot_qq(\n    data: NormData,\n    plt_kwargs: dict | None = None,\n    bound: int | float = 0,\n    plot_id_line: bool = False,\n    hue_data: str | None = None,\n    markers_data: str | None = None,\n    split_data: str | None = None,\n    seed: int = 42,\n) -&gt; None:\n    \"\"\"\n    Plot QQ plots for each response variable in the data.\n\n    Parameters\n    ----------\n    data : NormData\n        Data containing the response variables.\n    plt_kwargs : dict or None, optional\n        Additional keyword arguments for the plot. Defaults to None.\n    bound : int or float, optional\n        Axis limits for the plot. Defaults to 0.\n    plot_id_line : bool, optional\n        Whether to plot the identity line. Defaults to False.\n    hue_data : str or None, optional\n        Column to use for coloring. Defaults to None.\n    markers_data : str or None, optional\n        Column to use for marker styling. Defaults to None.\n    split_data : str or None, optional\n        Column to use for splitting data. Defaults to None.\n    seed : int, optional\n        Random seed for reproducibility. Defaults to 42.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; plot_qq(data, plt_kwargs={'figsize': (10, 6)}, bound=3)\n    \"\"\"\n    plt_kwargs = plt_kwargs or {}\n    for response_var in data.coords[\"response_vars\"].to_numpy():\n        _plot_qq(\n            data,\n            response_var,\n            plt_kwargs,\n            bound,\n            plot_id_line,\n            hue_data,\n            markers_data,\n            split_data,\n            seed,\n        )\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model","title":"<code>regression_model</code>","text":""},{"location":"api/#pcntoolkit.regression_model.blr","title":"<code>blr</code>","text":""},{"location":"api/#pcntoolkit.regression_model.blr.blr","title":"<code>blr</code>","text":"<p>Bayesian Linear Regression (BLR) implementation.</p> <p>This module implements Bayesian Linear Regression with support for: - L1/L2 regularization - Automatic Relevance Determination (ARD) - Heteroskedastic noise modeling - Multiple optimization methods (CG, Powell, Nelder-Mead, L-BFGS-B)</p> <p>The implementation follows standard Bayesian formulation with Gaussian priors and supports both homoskedastic and heteroskedastic noise models.</p>"},{"location":"api/#pcntoolkit.regression_model.blr.blr.BLR","title":"<code>BLR</code>","text":"<p>               Bases: <code>RegressionModel</code></p> <p>Bayesian Linear Regression model implementation.</p> <p>This class implements Bayesian Linear Regression with various features including automatic relevance determination (ARD), heteroskedastic noise modeling, and multiple optimization methods.</p> <p>Attributes:</p> Name Type Description <code>hyp</code> <code>ndarray</code> <p>Model hyperparameters</p> <code>nlZ</code> <code>float</code> <p>Negative log marginal likelihood</p> <code>N</code> <code>int</code> <p>Number of samples</p> <code>D</code> <code>int</code> <p>Number of features</p> <code>lambda_n_vec</code> <code>ndarray</code> <p>Precision matrix</p> <code>Sigma_a</code> <code>ndarray</code> <p>Prior covariance</p> <code>Lambda_a</code> <code>ndarray</code> <p>Prior precision</p> <code>warp</code> <code>bool</code> <p>Whether to use warping</p> <code>hyp0</code> <code>ndarray</code> <p>Initial hyperparameters</p> <code>n_hyp</code> <code>int</code> <p>Number of hyperparameters</p> Source code in <code>pcntoolkit/regression_model/blr/blr.py</code> <pre><code>class BLR(RegressionModel):\n    \"\"\"\n    Bayesian Linear Regression model implementation.\n\n    This class implements Bayesian Linear Regression with various features including\n    automatic relevance determination (ARD), heteroskedastic noise modeling, and\n    multiple optimization methods.\n\n    Attributes\n    ----------\n    hyp : np.ndarray\n        Model hyperparameters\n    nlZ : float\n        Negative log marginal likelihood\n    N : int\n        Number of samples\n    D : int\n        Number of features\n    lambda_n_vec : np.ndarray\n        Precision matrix\n    Sigma_a : np.ndarray\n        Prior covariance\n    Lambda_a : np.ndarray\n        Prior precision\n    warp : bool\n        Whether to use warping\n    hyp0 : np.ndarray\n        Initial hyperparameters\n    n_hyp : int\n        Number of hyperparameters\n    \"\"\"\n\n    @property\n    def blr_conf(self) -&gt; BLRConf:\n        \"\"\"Rewturn the configuration object for the BLR model.\n\n        Returns\n        -------\n        BLRConf\n            BLRConf\n        \"\"\"\n        return cast(BLRConf, self.reg_conf)\n\n    def __init__(\n        self,\n        name: str,\n        reg_conf: BLRConf,\n        is_fitted: bool = False,\n        is_from_dict: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the BLR model.\n\n        Parameters\n        ----------\n        name : str\n            Model name identifier\n        reg_conf : BLRConf\n            Model configuration object\n        is_fitted : bool, optional\n            Whether the model is already fitted, by default False\n        is_from_dict : bool, optional\n            Whether the model is being loaded from dictionary, by default False\n        \"\"\"\n        super().__init__(name, reg_conf, is_fitted, is_from_dict)\n\n        self.hyp: np.ndarray = None  # type: ignore\n        self.nlZ: np.ndarray = np.nan  # type: ignore\n        self.N: int = None  # type: ignore  # Number of samples\n        self.D: int = None  # type: ignore # Number of features\n        self.lambda_n_vec: np.ndarray = None  # type: ignore  # precision matrix\n        self.Sigma_a: np.ndarray = None  # type: ignore  # prior covariance\n        self.Lambda_a: np.ndarray = None  # type: ignore # prior precision\n        self.warp: bool = None  # type: ignore\n        self.hyp0: np.ndarray = None  # type: ignore\n        self.n_hyp: int = 0\n        self.var_D: int = 0\n        self.alpha: np.ndarray = None  # type: ignore\n        self.beta: np.ndarray = None  # type: ignore\n        self.gamma: np.ndarray = None  # type: ignore\n        self.m: np.ndarray = None  # type: ignore\n        self.A: np.ndarray = None  # type: ignore\n        self.dnlZ: np.ndarray = None  # type: ignore\n        # ? Do we need ys and s2?\n        self.ys: np.ndarray = None  # type: ignore\n        self.s2: np.ndarray = None  # type: ignore\n\n        # self.gamma = None # Not used if warp is not used\n\n    def init_hyp(self, data: BLRData) -&gt; np.ndarray:  # type:ignore\n        \"\"\"\n        Initialize model hyperparameters.\n\n        Parameters\n        ----------\n        data : BLRData\n            Training data containing features and targets\n\n        Returns\n        -------\n        np.ndarray\n            Initialized hyperparameter vector\n        \"\"\"\n        # TODO check if this is correct\n        # Model order\n        if self.hyp0:\n            return self.hyp0\n\n        if self.models_variance:\n            n_beta = self.var_D\n        else:\n            n_beta = 1\n\n        n_alpha = self.D\n        n_gamma = 0\n        self.n_hyp = n_beta + n_alpha + n_gamma  # type: ignore\n        return np.zeros(self.n_hyp)\n\n    def fit(self, data: BLRData) -&gt; None:\n        \"\"\"\n        Fit the Bayesian Linear Regression model to the data.\n\n        Parameters\n        ----------\n        data : BLRData\n            Data object containing features and target.\n        \"\"\"\n        self.D = data.X.shape[1]\n        self.var_D = data.var_X.shape[1]\n\n        # Initialize hyperparameters if not provided\n        hyp0 = self.init_hyp(data)\n\n        args = (data.X, data.y, data.var_X)\n\n        match self.blr_conf.optimizer.lower():\n            case \"cg\":\n                out = optimize.fmin_cg(\n                    f=self.loglik,\n                    x0=hyp0,\n                    fprime=self.dloglik,\n                    args=args,\n                    gtol=self.tol,\n                    maxiter=self.n_iter,\n                    full_output=1,\n                )\n            case \"powell\":\n                out = optimize.fmin_powell(\n                    func=self.loglik, x0=hyp0, args=args, full_output=1\n                )\n            case \"nelder-mead\":\n                out = optimize.fmin(func=self.loglik, x0=hyp0, args=args, full_output=1)\n            case \"l-bfgs-b\":\n                all_hyp_i = [hyp0]\n\n                def store(X: np.ndarray) -&gt; None:\n                    hyp = X\n                    all_hyp_i.append(hyp)\n\n                try:\n                    out = optimize.fmin_l_bfgs_b(\n                        func=self.penalized_loglik,\n                        x0=hyp0,\n                        args=(*args, self.l_bfgs_b_l, self.norm),\n                        approx_grad=True,\n                        epsilon=self.epsilon,\n                        callback=store,\n                    )\n                except np.linalg.LinAlgError as e:\n                    print(\n                        f\"Restarting estimation at hyp = {all_hyp_i[-1]}, due to *** numpy.linalg.LinAlgError: Matrix is singular.\\n{e}\"\n                    )\n                    out = optimize.fmin_l_bfgs_b(\n                        func=self.penalized_loglik,\n                        x0=all_hyp_i[-1],\n                        args=(*args, self.l_bfgs_b_l, self.norm),\n                        approx_grad=True,\n                        epsilon=self.epsilon,\n                    )\n\n            case _:\n                raise ValueError(f\"Optimizer {self.blr_conf.optimizer} not recognized.\")\n        self.hyp = out[0]\n        self.nlZ = out[1]\n        _, self.beta = self.parse_hyps(self.hyp, data.X, data.var_X)\n        self.is_fitted = True\n\n    def predict(self, data: BLRData) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Make predictions using the fitted model.\n\n        Parameters\n        ----------\n        data : BLRData\n            Data object containing features for prediction.\n\n        Returns\n        -------\n        tuple[np.ndarray, np.ndarray]\n            Predictive mean and variance.\n        \"\"\"\n        _, beta = self.parse_hyps(self.hyp, data.X, data.var_X)\n        ys = data.X.dot(self.m)\n        s2n = 1 / beta\n        s2 = s2n + np.sum(data.X * linalg.solve(self.A, data.X.T).T, axis=1)\n        # ! These need to be stored for the centiles and zscores methods\n        self.ys = ys\n        self.s2 = s2\n\n        return ys, s2\n\n    def parse_hyps(\n        self, hyp: np.ndarray, X: np.ndarray, var_X: np.ndarray\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Parse hyperparameters into model parameters.\n\n        Parameters\n        ----------\n        hyp : np.ndarray\n            Hyperparameter vector.\n        X : np.ndarray\n            Covariates.\n        var_X : np.ndarray\n            Variance of covariates.\n\n        Returns\n        -------\n        tuple[np.ndarray, np.ndarray]\n            Parsed alpha and beta parameters.\n        \"\"\"\n        N = X.shape[0]\n        beta: np.ndarray = None  # type: ignore\n        # Noise precision\n        if self.models_variance:\n            Dv = var_X.shape[1]\n            w_d = np.asarray(hyp[0:Dv])\n            beta = np.exp(var_X.dot(w_d))\n            n_lik_param = len(w_d)\n            self.lambda_n_vec = beta\n        else:\n            beta = np.asarray([np.exp(hyp[0])])\n            n_lik_param = len(beta)\n            self.lambda_n_vec = np.ones(N) * beta\n\n        # Coefficients precision\n        if isinstance(beta, list) or isinstance(beta, np.ndarray):\n            alpha = np.exp(hyp[n_lik_param:])\n        else:\n            alpha = np.exp(hyp[1:])\n\n        return alpha, beta\n\n    def post(\n        self, hyp: np.ndarray, X: np.ndarray, y: np.ndarray, var_X: np.ndarray\n    ) -&gt; None:\n        \"\"\"\n        Compute the posterior distribution.\n\n        Parameters\n        ----------\n        hyp : np.ndarray\n            Hyperparameter vector.\n        X : np.ndarray\n            Covariates.\n        y : np.ndarray\n            Responses.\n        var_X : np.ndarray\n            Variance of covariates.\n        \"\"\"\n        # Store the number of samples and features\n        self.N = X.shape[0]\n        if len(X.shape) == 1:\n            self.D = 1\n        else:\n            self.D = X.shape[1]\n\n        # Check if hyperparameters have changed\n        if (hyp == self.hyp).all() and hasattr(self, \"N\"):\n            print(\"hyperparameters have not changed, exiting\")\n            return\n        else:\n            self.hyp = hyp\n\n        # Parse hyperparameters\n        alpha, _ = self.parse_hyps(self.hyp, X, var_X)\n\n        # prior variance\n        if len(alpha) == 1 or len(alpha) == self.D:\n            self.Sigma_a = np.diag(np.ones(self.D)) / alpha\n            self.Lambda_a = np.diag(np.ones(self.D)) * alpha\n        else:\n            raise ValueError(\"hyperparameter vector has invalid length\")\n\n        # Compute the posterior precision and mean\n        XtLambda_n = X.T * self.lambda_n_vec\n        self.A = XtLambda_n.dot(X) + self.Lambda_a\n        invAXt: np.ndarray = linalg.solve(self.A, X.T, check_finite=False)\n        self.m = (invAXt * self.lambda_n_vec).dot(y)\n\n    def loglik(\n        self, hyp: np.ndarray, X: np.ndarray, y: np.ndarray, var_X: np.ndarray\n    ) -&gt; float:\n        \"\"\"\n        Compute the negative log likelihood.\n\n        Parameters\n        ----------\n        hyp : np.ndarray\n            Hyperparameter vector.\n        X : np.ndarray\n            Covariates.\n        y : np.ndarray\n            Responses.\n        var_X : np.ndarray\n            Variance of covariates.\n\n        Returns\n        -------\n        float\n            Negative log likelihood.\n        \"\"\"\n        _, _ = self.parse_hyps(hyp, X, var_X)\n\n        something_big: float = float(np.finfo(np.float64).max)\n\n        # load posterior and prior covariance\n        if (hyp != self.hyp).any() or not hasattr(self, \"A\"):\n            try:\n                self.post(hyp, X, y, var_X)\n            except ValueError:\n                print(\"Warning: Estimation of posterior distribution failed\")\n                nlZ = something_big\n                return nlZ\n\n        try:\n            # compute the log determinants in a numerically stable way\n            logdetA = 2 * np.sum(np.log(np.diag(np.linalg.cholesky(self.A))))\n        except (ValueError, LinAlgError):\n            print(\"Warning: Estimation of posterior distribution failed\")\n            nlZ = something_big\n            return nlZ\n\n        logdetSigma_a = np.sum(np.log(np.diag(self.Sigma_a)))  # diagonal\n        logdetSigma_n = -np.sum(np.log(self.lambda_n_vec))\n\n        # compute negative marginal log likelihood\n        X_y_t_sLambda_n = (y - X.dot(self.m)) * np.sqrt(self.lambda_n_vec)\n        nlZ = -0.5 * (\n            -self.N * np.log(2 * np.pi)\n            - logdetSigma_n\n            - logdetSigma_a\n            - X_y_t_sLambda_n.T.dot(X_y_t_sLambda_n)\n            - self.m.T.dot(self.Lambda_a).dot(self.m)\n            - logdetA\n        )\n\n        # make sure the output is finite to stop the minimizer getting upset\n        if not np.isfinite(nlZ):\n            nlZ = something_big\n\n        self.nlZ = nlZ  # type: ignore\n        return nlZ\n\n    def penalized_loglik(\n        self,\n        hyp: np.ndarray,\n        X: np.ndarray,\n        y: np.ndarray,\n        var_X: np.ndarray,\n        regularizer_strength: float = 0.1,\n        norm: Literal[\"L1\", \"L2\"] = \"L1\",\n    ) -&gt; float:\n        \"\"\"\n        Compute the penalized log likelihood with L1 or L2 regularization.\n\n        Parameters\n        ----------\n        hyp : np.ndarray\n            Hyperparameter vector\n        X : np.ndarray\n            Feature matrix\n        y : np.ndarray\n            Target vector\n        var_X : np.ndarray\n            Variance of features\n        regularizer_strength : float, optional\n            Regularization strength, by default 0.1\n        norm : {\"L1\", \"L2\"}, optional\n            Type of regularization norm, by default \"L1\"\n\n        Returns\n        -------\n        float\n            Penalized negative log likelihood value\n\n        Raises\n        ------\n        ValueError\n            If norm is not \"L1\" or \"L2\"\n        \"\"\"\n        if norm.upper() == \"L1\":\n            return self.loglik(hyp, X, y, var_X) + regularizer_strength * np.sum(\n                np.abs(hyp)\n            )\n        elif norm.upper() == \"L2\":\n            return self.loglik(hyp, X, y, var_X) + regularizer_strength * np.sum(\n                np.square(hyp)\n            )\n        else:\n            raise ValueError(\n                \"Requested penalty not recognized, choose between 'L1' or 'L2'.\"\n            )\n\n    def dloglik(\n        self, hyp: np.ndarray, X: np.ndarray, y: np.ndarray, var_X: np.ndarray\n    ) -&gt; np.ndarray:\n        \"\"\"Function to compute derivatives\"\"\"\n\n        # hyperparameters\n        alpha, beta = self.parse_hyps(hyp, X, var_X)\n\n        # load posterior and prior covariance\n        if (hyp != self.hyp).any() or not hasattr(self, \"A\"):\n            try:\n                self.post(hyp, X, y, var_X)\n            except ValueError:\n                print(\"Warning: Estimation of posterior distribution failed\")\n                if self.dnlZ is not None:\n                    dnlZ = np.sign(self.dnlZ) / np.finfo(float).eps\n                    return dnlZ\n                return np.array(1 / np.finfo(float).eps)\n        # precompute re-used quantities to maximise speed\n        # todo: revise implementation to use Cholesky throughout\n        #       that would remove the need to explicitly compute the inverse\n        S = np.linalg.inv(self.A)  # posterior covariance\n        SX = S.dot(X.T)\n        XLn = X.T * self.lambda_n_vec  # = X.T.dot(self.Lambda_n)\n        XLny = XLn.dot(y)\n        SXLny = S.dot(XLny)\n        XLnXm = XLn.dot(X).dot(self.m)\n\n        # initialise derivatives\n        dnlZ = np.zeros(hyp.shape)\n        # dnl2 = np.zeros(hyp.shape)\n\n        # noise precision parameter(s)\n        for i, _ in enumerate(beta):\n            # first compute derivative of Lambda_n with respect to beta\n            dL_n_vec = np.ones(self.N)\n            dLambda_n = np.diag(dL_n_vec)\n\n            # compute quantities used multiple times\n            XdLnX = X.T.dot(dLambda_n).dot(X)\n            dA = XdLnX\n\n            # derivative of posterior parameters with respect to beta\n            b = -S.dot(dA).dot(SXLny) + SX.dot(dLambda_n).dot(y)\n\n            # compute np.trace(self.Sigma_n.dot(dLambda_n)) efficiently\n            trSigma_ndLambda_n = sum((1 / self.lambda_n_vec) * np.diag(dLambda_n))\n\n            # compute  y.T.dot(Lambda_n) efficiently\n            ytLn = (y * self.lambda_n_vec).T\n\n            # compute derivatives\n            dnlZ[i] = (\n                -(\n                    0.5 * trSigma_ndLambda_n\n                    - 0.5 * y.dot(dLambda_n).dot(y)\n                    + y.dot(dLambda_n).dot(X).dot(self.m)\n                    + ytLn.dot(X).dot(b)\n                    - 0.5 * self.m.T.dot(XdLnX).dot(self.m)\n                    - b.T.dot(XLnXm)\n                    - b.T.dot(self.Lambda_a).dot(self.m)\n                    - 0.5 * np.trace(S.dot(dA))\n                )\n                * beta[i]\n            )\n\n        # scaling parameter(s)\n        for i, _ in enumerate(beta):\n            # first compute derivatives with respect to alpha\n            if len(alpha) == self.D:  # are we using ARD?\n                dLambda_a = np.zeros((self.D, self.D))\n                dLambda_a[i, i] = 1\n            else:\n                dLambda_a = np.eye(self.D)\n\n            F = dLambda_a\n            c = -S.dot(F).dot(SXLny)\n\n            # compute np.trace(self.Sigma_a.dot(dLambda_a)) efficiently\n            trSigma_adLambda_a = sum(np.diag(self.Sigma_a) * np.diag(dLambda_a))\n\n            dnlZ[i + len(beta)] = (\n                -(\n                    0.5 * trSigma_adLambda_a\n                    + XLny.T.dot(c)\n                    - c.T.dot(XLnXm)\n                    - c.T.dot(self.Lambda_a).dot(self.m)\n                    - 0.5 * self.m.T.dot(F).dot(self.m)\n                    - 0.5 * np.trace(linalg.solve(self.A, F))\n                )\n                * alpha[i]\n            )\n\n        # make sure the gradient is finite to stop the minimizer getting upset\n        if not all(np.isfinite(dnlZ)):\n            bad = np.where(np.logical_not(np.isfinite(dnlZ)))\n            for b in bad:\n                dnlZ[b] = np.sign(self.dnlZ[b]) / np.finfo(float).eps  # type: ignore\n\n        self.dnlZ = dnlZ\n        return dnlZ\n\n    def centiles(\n        self, data: BLRData, cdf: np.ndarray, resample: bool = True\n    ) -&gt; np.ndarray:\n        \"\"\"Calculate prediction centiles for given cumulative distribution function values.\n\n        Parameters\n        ----------\n        data : BLRData\n            Data object containing features for prediction\n        cdf : np.ndarray\n            Array of cumulative distribution function values to compute centiles for\n        resample : bool, optional\n            Whether to recompute predictions before calculating centiles, by default True\n\n        Returns\n        -------\n        np.ndarray\n            Array of shape (len(cdf), n_samples) containing the predicted centile \n            values for each CDF value and sample\n        \"\"\"\n        if resample:\n            self.predict(data)\n        centiles = np.zeros((cdf.shape[0], data.X.shape[0]))\n        for i, cdf in enumerate(cdf):\n            centiles[i, :] = self.ys + stats.norm.ppf(cdf) * np.sqrt(self.s2)\n        return centiles\n\n    def zscores(self, data: BLRData, resample: bool = True) -&gt; np.ndarray:\n        \"\"\"Calculate z-scores for observed values relative to predictions.\n\n        Parameters\n        ----------\n        data : BLRData\n            Data object containing features and observed values\n        resample : bool, optional\n            Whether to recompute predictions before calculating z-scores, by default True\n\n        Returns\n        -------\n        np.ndarray\n            Array of z-scores for each observation\n        \"\"\"\n        if resample:\n            self.predict(data)\n        return (data.y - self.ys) / np.sqrt(self.s2)\n\n    def to_dict(self, path: str | None = None) -&gt; dict:\n        my_dict = super().to_dict()\n        my_dict[\"hyp\"] = self.hyp.tolist()\n        my_dict[\"nlZ\"] = self.nlZ\n        my_dict[\"N\"] = self.N\n        my_dict[\"D\"] = self.D\n        my_dict[\"lambda_n_vec\"] = self.lambda_n_vec.tolist()\n        my_dict[\"Sigma_a\"] = self.Sigma_a.tolist()\n        my_dict[\"Lambda_a\"] = self.Lambda_a.tolist()\n        my_dict[\"beta\"] = self.beta.tolist()\n        my_dict[\"m\"] = self.m.tolist()\n        my_dict[\"A\"] = self.A.tolist()\n        return my_dict\n\n    @classmethod\n    def from_dict(cls, my_dict: dict, path: str | None = None) -&gt; \"BLR\":\n        \"\"\"\n        Creates a configuration from a dictionary.\n        \"\"\"\n        name = my_dict[\"name\"]\n        conf = BLRConf.from_dict(my_dict[\"reg_conf\"])\n        is_fitted = my_dict[\"is_fitted\"]\n        is_from_dict = True\n        self = cls(name, conf, is_fitted, is_from_dict)\n        self.hyp = np.array(my_dict[\"hyp\"])\n        self.nlZ = my_dict[\"nlZ\"]\n        self.N = my_dict[\"N\"]\n        self.D = my_dict[\"D\"]\n        self.lambda_n_vec = np.array(my_dict[\"lambda_n_vec\"])\n        self.Sigma_a = np.array(my_dict[\"Sigma_a\"])\n        self.Lambda_a = np.array(my_dict[\"Lambda_a\"])\n        self.beta = np.array(my_dict[\"beta\"])\n        self.m = np.array(my_dict[\"m\"])\n        self.A = np.array(my_dict[\"A\"])\n        return self\n\n    @classmethod\n    def from_args(cls, name: str, args: dict) -&gt; \"BLR\":\n        \"\"\"\n        Creates a configuration from command line arguments\n        \"\"\"\n        conf = BLRConf.from_args(args)\n        is_fitted = args.get(\"is_fitted\", False)\n        is_from_dict = True\n        self = cls(name, conf, is_fitted, is_from_dict)\n        self.hyp = np.array(args.get(\"hyp\", None))\n        self.nlZ = args.get(\"nlZ\", None)\n        self.N = args.get(\"N\", None)\n        self.D = args.get(\"D\", None)\n        self.lambda_n_vec = np.array(args.get(\"lambda_n_vec\", None))\n        self.Sigma_a = np.array(args.get(\"Sigma_a\", None))\n        self.Lambda_a = np.array(args.get(\"Lambda_a\", None))\n        self.beta = np.array(args.get(\"beta\", None))\n        self.m = np.array(args.get(\"m\", None))\n        self.A = np.array(args.get(\"A\", None))\n        return self\n\n    @property\n    def tol(self) -&gt; float:\n        \"\"\"Optimization convergence tolerance.\n\n        Returns\n        -------\n        float\n            Tolerance value for optimization convergence\n        \"\"\"\n        return self.blr_conf.tol\n\n    @property\n    def n_iter(self) -&gt; int:\n        \"\"\"Maximum number of optimization iterations.\n\n        Returns\n        -------\n        int\n            Maximum number of iterations for optimization\n        \"\"\"\n        return self.blr_conf.n_iter\n\n    @property\n    def optimizer(self) -&gt; str:\n        \"\"\"Optimization method to use.\n\n        Returns\n        -------\n        str\n            Name of optimization method ('cg', 'powell', 'nelder-mead', or 'l-bfgs-b')\n        \"\"\"\n        return self.blr_conf.optimizer\n\n    @property\n    def ard(self) -&gt; bool:\n        \"\"\"Whether to use Automatic Relevance Determination.\n\n        Returns\n        -------\n        bool\n            True if using ARD, False otherwise\n        \"\"\"\n        return self.blr_conf.ard\n\n    @property\n    def l_bfgs_b_l(self) -&gt; float:\n        \"\"\"L-BFGS-B regularization strength.\n\n        Returns\n        -------\n        float\n            Regularization strength parameter for L-BFGS-B optimizer\n        \"\"\"\n        return self.blr_conf.l_bfgs_b_l\n\n    @property\n    def epsilon(self) -&gt; float:\n        \"\"\"Step size for gradient approximation in L-BFGS-B.\n\n        Returns\n        -------\n        float\n            Step size for finite difference gradient approximation\n        \"\"\"\n        return self.blr_conf.l_bfgs_b_epsilon\n\n    @property\n    def norm(self) -&gt; str:\n        \"\"\"Type of regularization norm for L-BFGS-B.\n\n        Returns\n        -------\n        str\n            Regularization norm type ('L1' or 'L2')\n        \"\"\"\n        return self.blr_conf.l_bfgs_b_norm\n\n    @property\n    def intercept(self) -&gt; bool:\n        \"\"\"Whether to include an intercept term.\n\n        Returns\n        -------\n        bool\n            True if model includes intercept, False otherwise\n        \"\"\"\n        return self.blr_conf.intercept\n\n    @property\n    def random_intercept(self) -&gt; bool:\n        \"\"\"Whether to include a random intercept.\n\n        Returns\n        -------\n        bool\n            True if model includes random intercept, False otherwise\n        \"\"\"\n        return self.blr_conf.random_intercept\n\n    @property\n    def heteroskedastic(self) -&gt; bool:\n        \"\"\"Whether to model heteroskedastic noise.\n\n        Returns\n        -------\n        bool\n            True if modeling heteroskedastic noise, False otherwise\n        \"\"\"\n        return self.blr_conf.heteroskedastic\n\n    @property\n    def random_intercept_var(self) -&gt; bool:\n        \"\"\"Whether to model random intercept variance.\n\n        Returns\n        -------\n        bool\n            True if modeling random intercept variance, False otherwise\n        \"\"\"\n        return self.blr_conf.random_intercept_var\n\n    @property\n    def intercept_var(self) -&gt; bool:\n        \"\"\"Whether to model intercept variance.\n\n        Returns\n        -------\n        bool\n            True if modeling intercept variance, False otherwise\n        \"\"\"\n        return self.blr_conf.intercept_var\n\n    @property\n    def models_variance(self) -&gt; bool:\n        \"\"\"Whether the model includes any variance components.\n\n        Returns\n        -------\n        bool\n            True if model includes heteroskedastic noise, random intercept variance,\n            or intercept variance components\n        \"\"\"\n        return (\n            self.blr_conf.heteroskedastic\n            or self.blr_conf.random_intercept_var\n            or self.blr_conf.intercept_var\n        )\n</code></pre> <code>ard: bool</code> <code>property</code> \u00b6 <p>Whether to use Automatic Relevance Determination.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if using ARD, False otherwise</p> <code>blr_conf: BLRConf</code> <code>property</code> \u00b6 <p>Rewturn the configuration object for the BLR model.</p> <p>Returns:</p> Type Description <code>BLRConf</code> <p>BLRConf</p> <code>epsilon: float</code> <code>property</code> \u00b6 <p>Step size for gradient approximation in L-BFGS-B.</p> <p>Returns:</p> Type Description <code>float</code> <p>Step size for finite difference gradient approximation</p> <code>heteroskedastic: bool</code> <code>property</code> \u00b6 <p>Whether to model heteroskedastic noise.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if modeling heteroskedastic noise, False otherwise</p> <code>intercept: bool</code> <code>property</code> \u00b6 <p>Whether to include an intercept term.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if model includes intercept, False otherwise</p> <code>intercept_var: bool</code> <code>property</code> \u00b6 <p>Whether to model intercept variance.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if modeling intercept variance, False otherwise</p> <code>l_bfgs_b_l: float</code> <code>property</code> \u00b6 <p>L-BFGS-B regularization strength.</p> <p>Returns:</p> Type Description <code>float</code> <p>Regularization strength parameter for L-BFGS-B optimizer</p> <code>models_variance: bool</code> <code>property</code> \u00b6 <p>Whether the model includes any variance components.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if model includes heteroskedastic noise, random intercept variance, or intercept variance components</p> <code>n_iter: int</code> <code>property</code> \u00b6 <p>Maximum number of optimization iterations.</p> <p>Returns:</p> Type Description <code>int</code> <p>Maximum number of iterations for optimization</p> <code>norm: str</code> <code>property</code> \u00b6 <p>Type of regularization norm for L-BFGS-B.</p> <p>Returns:</p> Type Description <code>str</code> <p>Regularization norm type ('L1' or 'L2')</p> <code>optimizer: str</code> <code>property</code> \u00b6 <p>Optimization method to use.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of optimization method ('cg', 'powell', 'nelder-mead', or 'l-bfgs-b')</p> <code>random_intercept: bool</code> <code>property</code> \u00b6 <p>Whether to include a random intercept.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if model includes random intercept, False otherwise</p> <code>random_intercept_var: bool</code> <code>property</code> \u00b6 <p>Whether to model random intercept variance.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if modeling random intercept variance, False otherwise</p> <code>tol: float</code> <code>property</code> \u00b6 <p>Optimization convergence tolerance.</p> <p>Returns:</p> Type Description <code>float</code> <p>Tolerance value for optimization convergence</p> <code>__init__(name: str, reg_conf: BLRConf, is_fitted: bool = False, is_from_dict: bool = False) -&gt; None</code> \u00b6 <p>Initialize the BLR model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Model name identifier</p> required <code>reg_conf</code> <code>BLRConf</code> <p>Model configuration object</p> required <code>is_fitted</code> <code>bool</code> <p>Whether the model is already fitted, by default False</p> <code>False</code> <code>is_from_dict</code> <code>bool</code> <p>Whether the model is being loaded from dictionary, by default False</p> <code>False</code> Source code in <code>pcntoolkit/regression_model/blr/blr.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    reg_conf: BLRConf,\n    is_fitted: bool = False,\n    is_from_dict: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the BLR model.\n\n    Parameters\n    ----------\n    name : str\n        Model name identifier\n    reg_conf : BLRConf\n        Model configuration object\n    is_fitted : bool, optional\n        Whether the model is already fitted, by default False\n    is_from_dict : bool, optional\n        Whether the model is being loaded from dictionary, by default False\n    \"\"\"\n    super().__init__(name, reg_conf, is_fitted, is_from_dict)\n\n    self.hyp: np.ndarray = None  # type: ignore\n    self.nlZ: np.ndarray = np.nan  # type: ignore\n    self.N: int = None  # type: ignore  # Number of samples\n    self.D: int = None  # type: ignore # Number of features\n    self.lambda_n_vec: np.ndarray = None  # type: ignore  # precision matrix\n    self.Sigma_a: np.ndarray = None  # type: ignore  # prior covariance\n    self.Lambda_a: np.ndarray = None  # type: ignore # prior precision\n    self.warp: bool = None  # type: ignore\n    self.hyp0: np.ndarray = None  # type: ignore\n    self.n_hyp: int = 0\n    self.var_D: int = 0\n    self.alpha: np.ndarray = None  # type: ignore\n    self.beta: np.ndarray = None  # type: ignore\n    self.gamma: np.ndarray = None  # type: ignore\n    self.m: np.ndarray = None  # type: ignore\n    self.A: np.ndarray = None  # type: ignore\n    self.dnlZ: np.ndarray = None  # type: ignore\n    # ? Do we need ys and s2?\n    self.ys: np.ndarray = None  # type: ignore\n    self.s2: np.ndarray = None  # type: ignore\n</code></pre> <code>centiles(data: BLRData, cdf: np.ndarray, resample: bool = True) -&gt; np.ndarray</code> \u00b6 <p>Calculate prediction centiles for given cumulative distribution function values.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>BLRData</code> <p>Data object containing features for prediction</p> required <code>cdf</code> <code>ndarray</code> <p>Array of cumulative distribution function values to compute centiles for</p> required <code>resample</code> <code>bool</code> <p>Whether to recompute predictions before calculating centiles, by default True</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape (len(cdf), n_samples) containing the predicted centile  values for each CDF value and sample</p> Source code in <code>pcntoolkit/regression_model/blr/blr.py</code> <pre><code>def centiles(\n    self, data: BLRData, cdf: np.ndarray, resample: bool = True\n) -&gt; np.ndarray:\n    \"\"\"Calculate prediction centiles for given cumulative distribution function values.\n\n    Parameters\n    ----------\n    data : BLRData\n        Data object containing features for prediction\n    cdf : np.ndarray\n        Array of cumulative distribution function values to compute centiles for\n    resample : bool, optional\n        Whether to recompute predictions before calculating centiles, by default True\n\n    Returns\n    -------\n    np.ndarray\n        Array of shape (len(cdf), n_samples) containing the predicted centile \n        values for each CDF value and sample\n    \"\"\"\n    if resample:\n        self.predict(data)\n    centiles = np.zeros((cdf.shape[0], data.X.shape[0]))\n    for i, cdf in enumerate(cdf):\n        centiles[i, :] = self.ys + stats.norm.ppf(cdf) * np.sqrt(self.s2)\n    return centiles\n</code></pre> <code>dloglik(hyp: np.ndarray, X: np.ndarray, y: np.ndarray, var_X: np.ndarray) -&gt; np.ndarray</code> \u00b6 <p>Function to compute derivatives</p> Source code in <code>pcntoolkit/regression_model/blr/blr.py</code> <pre><code>def dloglik(\n    self, hyp: np.ndarray, X: np.ndarray, y: np.ndarray, var_X: np.ndarray\n) -&gt; np.ndarray:\n    \"\"\"Function to compute derivatives\"\"\"\n\n    # hyperparameters\n    alpha, beta = self.parse_hyps(hyp, X, var_X)\n\n    # load posterior and prior covariance\n    if (hyp != self.hyp).any() or not hasattr(self, \"A\"):\n        try:\n            self.post(hyp, X, y, var_X)\n        except ValueError:\n            print(\"Warning: Estimation of posterior distribution failed\")\n            if self.dnlZ is not None:\n                dnlZ = np.sign(self.dnlZ) / np.finfo(float).eps\n                return dnlZ\n            return np.array(1 / np.finfo(float).eps)\n    # precompute re-used quantities to maximise speed\n    # todo: revise implementation to use Cholesky throughout\n    #       that would remove the need to explicitly compute the inverse\n    S = np.linalg.inv(self.A)  # posterior covariance\n    SX = S.dot(X.T)\n    XLn = X.T * self.lambda_n_vec  # = X.T.dot(self.Lambda_n)\n    XLny = XLn.dot(y)\n    SXLny = S.dot(XLny)\n    XLnXm = XLn.dot(X).dot(self.m)\n\n    # initialise derivatives\n    dnlZ = np.zeros(hyp.shape)\n    # dnl2 = np.zeros(hyp.shape)\n\n    # noise precision parameter(s)\n    for i, _ in enumerate(beta):\n        # first compute derivative of Lambda_n with respect to beta\n        dL_n_vec = np.ones(self.N)\n        dLambda_n = np.diag(dL_n_vec)\n\n        # compute quantities used multiple times\n        XdLnX = X.T.dot(dLambda_n).dot(X)\n        dA = XdLnX\n\n        # derivative of posterior parameters with respect to beta\n        b = -S.dot(dA).dot(SXLny) + SX.dot(dLambda_n).dot(y)\n\n        # compute np.trace(self.Sigma_n.dot(dLambda_n)) efficiently\n        trSigma_ndLambda_n = sum((1 / self.lambda_n_vec) * np.diag(dLambda_n))\n\n        # compute  y.T.dot(Lambda_n) efficiently\n        ytLn = (y * self.lambda_n_vec).T\n\n        # compute derivatives\n        dnlZ[i] = (\n            -(\n                0.5 * trSigma_ndLambda_n\n                - 0.5 * y.dot(dLambda_n).dot(y)\n                + y.dot(dLambda_n).dot(X).dot(self.m)\n                + ytLn.dot(X).dot(b)\n                - 0.5 * self.m.T.dot(XdLnX).dot(self.m)\n                - b.T.dot(XLnXm)\n                - b.T.dot(self.Lambda_a).dot(self.m)\n                - 0.5 * np.trace(S.dot(dA))\n            )\n            * beta[i]\n        )\n\n    # scaling parameter(s)\n    for i, _ in enumerate(beta):\n        # first compute derivatives with respect to alpha\n        if len(alpha) == self.D:  # are we using ARD?\n            dLambda_a = np.zeros((self.D, self.D))\n            dLambda_a[i, i] = 1\n        else:\n            dLambda_a = np.eye(self.D)\n\n        F = dLambda_a\n        c = -S.dot(F).dot(SXLny)\n\n        # compute np.trace(self.Sigma_a.dot(dLambda_a)) efficiently\n        trSigma_adLambda_a = sum(np.diag(self.Sigma_a) * np.diag(dLambda_a))\n\n        dnlZ[i + len(beta)] = (\n            -(\n                0.5 * trSigma_adLambda_a\n                + XLny.T.dot(c)\n                - c.T.dot(XLnXm)\n                - c.T.dot(self.Lambda_a).dot(self.m)\n                - 0.5 * self.m.T.dot(F).dot(self.m)\n                - 0.5 * np.trace(linalg.solve(self.A, F))\n            )\n            * alpha[i]\n        )\n\n    # make sure the gradient is finite to stop the minimizer getting upset\n    if not all(np.isfinite(dnlZ)):\n        bad = np.where(np.logical_not(np.isfinite(dnlZ)))\n        for b in bad:\n            dnlZ[b] = np.sign(self.dnlZ[b]) / np.finfo(float).eps  # type: ignore\n\n    self.dnlZ = dnlZ\n    return dnlZ\n</code></pre> <code>fit(data: BLRData) -&gt; None</code> \u00b6 <p>Fit the Bayesian Linear Regression model to the data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>BLRData</code> <p>Data object containing features and target.</p> required Source code in <code>pcntoolkit/regression_model/blr/blr.py</code> <pre><code>def fit(self, data: BLRData) -&gt; None:\n    \"\"\"\n    Fit the Bayesian Linear Regression model to the data.\n\n    Parameters\n    ----------\n    data : BLRData\n        Data object containing features and target.\n    \"\"\"\n    self.D = data.X.shape[1]\n    self.var_D = data.var_X.shape[1]\n\n    # Initialize hyperparameters if not provided\n    hyp0 = self.init_hyp(data)\n\n    args = (data.X, data.y, data.var_X)\n\n    match self.blr_conf.optimizer.lower():\n        case \"cg\":\n            out = optimize.fmin_cg(\n                f=self.loglik,\n                x0=hyp0,\n                fprime=self.dloglik,\n                args=args,\n                gtol=self.tol,\n                maxiter=self.n_iter,\n                full_output=1,\n            )\n        case \"powell\":\n            out = optimize.fmin_powell(\n                func=self.loglik, x0=hyp0, args=args, full_output=1\n            )\n        case \"nelder-mead\":\n            out = optimize.fmin(func=self.loglik, x0=hyp0, args=args, full_output=1)\n        case \"l-bfgs-b\":\n            all_hyp_i = [hyp0]\n\n            def store(X: np.ndarray) -&gt; None:\n                hyp = X\n                all_hyp_i.append(hyp)\n\n            try:\n                out = optimize.fmin_l_bfgs_b(\n                    func=self.penalized_loglik,\n                    x0=hyp0,\n                    args=(*args, self.l_bfgs_b_l, self.norm),\n                    approx_grad=True,\n                    epsilon=self.epsilon,\n                    callback=store,\n                )\n            except np.linalg.LinAlgError as e:\n                print(\n                    f\"Restarting estimation at hyp = {all_hyp_i[-1]}, due to *** numpy.linalg.LinAlgError: Matrix is singular.\\n{e}\"\n                )\n                out = optimize.fmin_l_bfgs_b(\n                    func=self.penalized_loglik,\n                    x0=all_hyp_i[-1],\n                    args=(*args, self.l_bfgs_b_l, self.norm),\n                    approx_grad=True,\n                    epsilon=self.epsilon,\n                )\n\n        case _:\n            raise ValueError(f\"Optimizer {self.blr_conf.optimizer} not recognized.\")\n    self.hyp = out[0]\n    self.nlZ = out[1]\n    _, self.beta = self.parse_hyps(self.hyp, data.X, data.var_X)\n    self.is_fitted = True\n</code></pre> <code>from_args(name: str, args: dict) -&gt; 'BLR'</code> <code>classmethod</code> \u00b6 <p>Creates a configuration from command line arguments</p> Source code in <code>pcntoolkit/regression_model/blr/blr.py</code> <pre><code>@classmethod\ndef from_args(cls, name: str, args: dict) -&gt; \"BLR\":\n    \"\"\"\n    Creates a configuration from command line arguments\n    \"\"\"\n    conf = BLRConf.from_args(args)\n    is_fitted = args.get(\"is_fitted\", False)\n    is_from_dict = True\n    self = cls(name, conf, is_fitted, is_from_dict)\n    self.hyp = np.array(args.get(\"hyp\", None))\n    self.nlZ = args.get(\"nlZ\", None)\n    self.N = args.get(\"N\", None)\n    self.D = args.get(\"D\", None)\n    self.lambda_n_vec = np.array(args.get(\"lambda_n_vec\", None))\n    self.Sigma_a = np.array(args.get(\"Sigma_a\", None))\n    self.Lambda_a = np.array(args.get(\"Lambda_a\", None))\n    self.beta = np.array(args.get(\"beta\", None))\n    self.m = np.array(args.get(\"m\", None))\n    self.A = np.array(args.get(\"A\", None))\n    return self\n</code></pre> <code>from_dict(my_dict: dict, path: str | None = None) -&gt; 'BLR'</code> <code>classmethod</code> \u00b6 <p>Creates a configuration from a dictionary.</p> Source code in <code>pcntoolkit/regression_model/blr/blr.py</code> <pre><code>@classmethod\ndef from_dict(cls, my_dict: dict, path: str | None = None) -&gt; \"BLR\":\n    \"\"\"\n    Creates a configuration from a dictionary.\n    \"\"\"\n    name = my_dict[\"name\"]\n    conf = BLRConf.from_dict(my_dict[\"reg_conf\"])\n    is_fitted = my_dict[\"is_fitted\"]\n    is_from_dict = True\n    self = cls(name, conf, is_fitted, is_from_dict)\n    self.hyp = np.array(my_dict[\"hyp\"])\n    self.nlZ = my_dict[\"nlZ\"]\n    self.N = my_dict[\"N\"]\n    self.D = my_dict[\"D\"]\n    self.lambda_n_vec = np.array(my_dict[\"lambda_n_vec\"])\n    self.Sigma_a = np.array(my_dict[\"Sigma_a\"])\n    self.Lambda_a = np.array(my_dict[\"Lambda_a\"])\n    self.beta = np.array(my_dict[\"beta\"])\n    self.m = np.array(my_dict[\"m\"])\n    self.A = np.array(my_dict[\"A\"])\n    return self\n</code></pre> <code>init_hyp(data: BLRData) -&gt; np.ndarray</code> \u00b6 <p>Initialize model hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>BLRData</code> <p>Training data containing features and targets</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Initialized hyperparameter vector</p> Source code in <code>pcntoolkit/regression_model/blr/blr.py</code> <pre><code>def init_hyp(self, data: BLRData) -&gt; np.ndarray:  # type:ignore\n    \"\"\"\n    Initialize model hyperparameters.\n\n    Parameters\n    ----------\n    data : BLRData\n        Training data containing features and targets\n\n    Returns\n    -------\n    np.ndarray\n        Initialized hyperparameter vector\n    \"\"\"\n    # TODO check if this is correct\n    # Model order\n    if self.hyp0:\n        return self.hyp0\n\n    if self.models_variance:\n        n_beta = self.var_D\n    else:\n        n_beta = 1\n\n    n_alpha = self.D\n    n_gamma = 0\n    self.n_hyp = n_beta + n_alpha + n_gamma  # type: ignore\n    return np.zeros(self.n_hyp)\n</code></pre> <code>loglik(hyp: np.ndarray, X: np.ndarray, y: np.ndarray, var_X: np.ndarray) -&gt; float</code> \u00b6 <p>Compute the negative log likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>hyp</code> <code>ndarray</code> <p>Hyperparameter vector.</p> required <code>X</code> <code>ndarray</code> <p>Covariates.</p> required <code>y</code> <code>ndarray</code> <p>Responses.</p> required <code>var_X</code> <code>ndarray</code> <p>Variance of covariates.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Negative log likelihood.</p> Source code in <code>pcntoolkit/regression_model/blr/blr.py</code> <pre><code>def loglik(\n    self, hyp: np.ndarray, X: np.ndarray, y: np.ndarray, var_X: np.ndarray\n) -&gt; float:\n    \"\"\"\n    Compute the negative log likelihood.\n\n    Parameters\n    ----------\n    hyp : np.ndarray\n        Hyperparameter vector.\n    X : np.ndarray\n        Covariates.\n    y : np.ndarray\n        Responses.\n    var_X : np.ndarray\n        Variance of covariates.\n\n    Returns\n    -------\n    float\n        Negative log likelihood.\n    \"\"\"\n    _, _ = self.parse_hyps(hyp, X, var_X)\n\n    something_big: float = float(np.finfo(np.float64).max)\n\n    # load posterior and prior covariance\n    if (hyp != self.hyp).any() or not hasattr(self, \"A\"):\n        try:\n            self.post(hyp, X, y, var_X)\n        except ValueError:\n            print(\"Warning: Estimation of posterior distribution failed\")\n            nlZ = something_big\n            return nlZ\n\n    try:\n        # compute the log determinants in a numerically stable way\n        logdetA = 2 * np.sum(np.log(np.diag(np.linalg.cholesky(self.A))))\n    except (ValueError, LinAlgError):\n        print(\"Warning: Estimation of posterior distribution failed\")\n        nlZ = something_big\n        return nlZ\n\n    logdetSigma_a = np.sum(np.log(np.diag(self.Sigma_a)))  # diagonal\n    logdetSigma_n = -np.sum(np.log(self.lambda_n_vec))\n\n    # compute negative marginal log likelihood\n    X_y_t_sLambda_n = (y - X.dot(self.m)) * np.sqrt(self.lambda_n_vec)\n    nlZ = -0.5 * (\n        -self.N * np.log(2 * np.pi)\n        - logdetSigma_n\n        - logdetSigma_a\n        - X_y_t_sLambda_n.T.dot(X_y_t_sLambda_n)\n        - self.m.T.dot(self.Lambda_a).dot(self.m)\n        - logdetA\n    )\n\n    # make sure the output is finite to stop the minimizer getting upset\n    if not np.isfinite(nlZ):\n        nlZ = something_big\n\n    self.nlZ = nlZ  # type: ignore\n    return nlZ\n</code></pre> <code>parse_hyps(hyp: np.ndarray, X: np.ndarray, var_X: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]</code> \u00b6 <p>Parse hyperparameters into model parameters.</p> <p>Parameters:</p> Name Type Description Default <code>hyp</code> <code>ndarray</code> <p>Hyperparameter vector.</p> required <code>X</code> <code>ndarray</code> <p>Covariates.</p> required <code>var_X</code> <code>ndarray</code> <p>Variance of covariates.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>Parsed alpha and beta parameters.</p> Source code in <code>pcntoolkit/regression_model/blr/blr.py</code> <pre><code>def parse_hyps(\n    self, hyp: np.ndarray, X: np.ndarray, var_X: np.ndarray\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Parse hyperparameters into model parameters.\n\n    Parameters\n    ----------\n    hyp : np.ndarray\n        Hyperparameter vector.\n    X : np.ndarray\n        Covariates.\n    var_X : np.ndarray\n        Variance of covariates.\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray]\n        Parsed alpha and beta parameters.\n    \"\"\"\n    N = X.shape[0]\n    beta: np.ndarray = None  # type: ignore\n    # Noise precision\n    if self.models_variance:\n        Dv = var_X.shape[1]\n        w_d = np.asarray(hyp[0:Dv])\n        beta = np.exp(var_X.dot(w_d))\n        n_lik_param = len(w_d)\n        self.lambda_n_vec = beta\n    else:\n        beta = np.asarray([np.exp(hyp[0])])\n        n_lik_param = len(beta)\n        self.lambda_n_vec = np.ones(N) * beta\n\n    # Coefficients precision\n    if isinstance(beta, list) or isinstance(beta, np.ndarray):\n        alpha = np.exp(hyp[n_lik_param:])\n    else:\n        alpha = np.exp(hyp[1:])\n\n    return alpha, beta\n</code></pre> <code>penalized_loglik(hyp: np.ndarray, X: np.ndarray, y: np.ndarray, var_X: np.ndarray, regularizer_strength: float = 0.1, norm: Literal['L1', 'L2'] = 'L1') -&gt; float</code> \u00b6 <p>Compute the penalized log likelihood with L1 or L2 regularization.</p> <p>Parameters:</p> Name Type Description Default <code>hyp</code> <code>ndarray</code> <p>Hyperparameter vector</p> required <code>X</code> <code>ndarray</code> <p>Feature matrix</p> required <code>y</code> <code>ndarray</code> <p>Target vector</p> required <code>var_X</code> <code>ndarray</code> <p>Variance of features</p> required <code>regularizer_strength</code> <code>float</code> <p>Regularization strength, by default 0.1</p> <code>0.1</code> <code>norm</code> <code>('L1', 'L2')</code> <p>Type of regularization norm, by default \"L1\"</p> <code>\"L1\"</code> <p>Returns:</p> Type Description <code>float</code> <p>Penalized negative log likelihood value</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If norm is not \"L1\" or \"L2\"</p> Source code in <code>pcntoolkit/regression_model/blr/blr.py</code> <pre><code>def penalized_loglik(\n    self,\n    hyp: np.ndarray,\n    X: np.ndarray,\n    y: np.ndarray,\n    var_X: np.ndarray,\n    regularizer_strength: float = 0.1,\n    norm: Literal[\"L1\", \"L2\"] = \"L1\",\n) -&gt; float:\n    \"\"\"\n    Compute the penalized log likelihood with L1 or L2 regularization.\n\n    Parameters\n    ----------\n    hyp : np.ndarray\n        Hyperparameter vector\n    X : np.ndarray\n        Feature matrix\n    y : np.ndarray\n        Target vector\n    var_X : np.ndarray\n        Variance of features\n    regularizer_strength : float, optional\n        Regularization strength, by default 0.1\n    norm : {\"L1\", \"L2\"}, optional\n        Type of regularization norm, by default \"L1\"\n\n    Returns\n    -------\n    float\n        Penalized negative log likelihood value\n\n    Raises\n    ------\n    ValueError\n        If norm is not \"L1\" or \"L2\"\n    \"\"\"\n    if norm.upper() == \"L1\":\n        return self.loglik(hyp, X, y, var_X) + regularizer_strength * np.sum(\n            np.abs(hyp)\n        )\n    elif norm.upper() == \"L2\":\n        return self.loglik(hyp, X, y, var_X) + regularizer_strength * np.sum(\n            np.square(hyp)\n        )\n    else:\n        raise ValueError(\n            \"Requested penalty not recognized, choose between 'L1' or 'L2'.\"\n        )\n</code></pre> <code>post(hyp: np.ndarray, X: np.ndarray, y: np.ndarray, var_X: np.ndarray) -&gt; None</code> \u00b6 <p>Compute the posterior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>hyp</code> <code>ndarray</code> <p>Hyperparameter vector.</p> required <code>X</code> <code>ndarray</code> <p>Covariates.</p> required <code>y</code> <code>ndarray</code> <p>Responses.</p> required <code>var_X</code> <code>ndarray</code> <p>Variance of covariates.</p> required Source code in <code>pcntoolkit/regression_model/blr/blr.py</code> <pre><code>def post(\n    self, hyp: np.ndarray, X: np.ndarray, y: np.ndarray, var_X: np.ndarray\n) -&gt; None:\n    \"\"\"\n    Compute the posterior distribution.\n\n    Parameters\n    ----------\n    hyp : np.ndarray\n        Hyperparameter vector.\n    X : np.ndarray\n        Covariates.\n    y : np.ndarray\n        Responses.\n    var_X : np.ndarray\n        Variance of covariates.\n    \"\"\"\n    # Store the number of samples and features\n    self.N = X.shape[0]\n    if len(X.shape) == 1:\n        self.D = 1\n    else:\n        self.D = X.shape[1]\n\n    # Check if hyperparameters have changed\n    if (hyp == self.hyp).all() and hasattr(self, \"N\"):\n        print(\"hyperparameters have not changed, exiting\")\n        return\n    else:\n        self.hyp = hyp\n\n    # Parse hyperparameters\n    alpha, _ = self.parse_hyps(self.hyp, X, var_X)\n\n    # prior variance\n    if len(alpha) == 1 or len(alpha) == self.D:\n        self.Sigma_a = np.diag(np.ones(self.D)) / alpha\n        self.Lambda_a = np.diag(np.ones(self.D)) * alpha\n    else:\n        raise ValueError(\"hyperparameter vector has invalid length\")\n\n    # Compute the posterior precision and mean\n    XtLambda_n = X.T * self.lambda_n_vec\n    self.A = XtLambda_n.dot(X) + self.Lambda_a\n    invAXt: np.ndarray = linalg.solve(self.A, X.T, check_finite=False)\n    self.m = (invAXt * self.lambda_n_vec).dot(y)\n</code></pre> <code>predict(data: BLRData) -&gt; tuple[np.ndarray, np.ndarray]</code> \u00b6 <p>Make predictions using the fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>BLRData</code> <p>Data object containing features for prediction.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>Predictive mean and variance.</p> Source code in <code>pcntoolkit/regression_model/blr/blr.py</code> <pre><code>def predict(self, data: BLRData) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Make predictions using the fitted model.\n\n    Parameters\n    ----------\n    data : BLRData\n        Data object containing features for prediction.\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray]\n        Predictive mean and variance.\n    \"\"\"\n    _, beta = self.parse_hyps(self.hyp, data.X, data.var_X)\n    ys = data.X.dot(self.m)\n    s2n = 1 / beta\n    s2 = s2n + np.sum(data.X * linalg.solve(self.A, data.X.T).T, axis=1)\n    # ! These need to be stored for the centiles and zscores methods\n    self.ys = ys\n    self.s2 = s2\n\n    return ys, s2\n</code></pre> <code>zscores(data: BLRData, resample: bool = True) -&gt; np.ndarray</code> \u00b6 <p>Calculate z-scores for observed values relative to predictions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>BLRData</code> <p>Data object containing features and observed values</p> required <code>resample</code> <code>bool</code> <p>Whether to recompute predictions before calculating z-scores, by default True</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of z-scores for each observation</p> Source code in <code>pcntoolkit/regression_model/blr/blr.py</code> <pre><code>def zscores(self, data: BLRData, resample: bool = True) -&gt; np.ndarray:\n    \"\"\"Calculate z-scores for observed values relative to predictions.\n\n    Parameters\n    ----------\n    data : BLRData\n        Data object containing features and observed values\n    resample : bool, optional\n        Whether to recompute predictions before calculating z-scores, by default True\n\n    Returns\n    -------\n    np.ndarray\n        Array of z-scores for each observation\n    \"\"\"\n    if resample:\n        self.predict(data)\n    return (data.y - self.ys) / np.sqrt(self.s2)\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.blr.blr_conf","title":"<code>blr_conf</code>","text":"<p>Module: blr_conf</p> <p>This module defines the configuration class for Bayesian Linear Regression (BLR) models. The <code>BLRConf</code> class is a dataclass that encapsulates configuration parameters for setting up and running BLR models. It provides a convenient method to initialize its instances from a dictionary of arguments.</p> <p>Classes:</p> Name Description <code>BLRConf</code> <p>A dataclass that holds configuration parameters for a BLR model.</p> <p>Functions:</p> Name Description <code>BLRConf.from_args</code> <p>Class method to create an instance of <code>BLRConf</code> from a dictionary of arguments.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; args = {'param1': value1, 'param2': value2}\n&gt;&gt;&gt; config = BLRConf.from_args(args)\n&gt;&gt;&gt; print(config)\nBLRConf(param1=value1, param2=value2)\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.blr.blr_conf.BLRConf","title":"<code>BLRConf</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegConf</code></p> <p>Class: BLRConf</p> <p>A dataclass for configuring Bayesian Linear Regression (BLR) models. This class encapsulates the parameters required to set up and execute a BLR model, providing a structured way to manage configuration data.</p> <p>Parameters:</p> Name Type Description Default <code>n_iter</code> <code>int</code> <p>The number of iterations for the optimization algorithm.</p> <code>N_ITER</code> <code>tol</code> <code>float</code> <p>The tolerance for the optimization algorithm.</p> <code>TOL</code> <code>ard</code> <code>bool</code> <p>Whether to use Automatic Relevance Determination (ARD).</p> <code>ARD</code> <code>optimizer</code> <code>str</code> <p>The optimization algorithm to use. Options are: \"l-bfgs-b\", \"cg\", \"powell\", \"nelder-mead\".</p> <code>OPTIMIZER</code> <code>l_bfgs_b_l</code> <code>float</code> <p>The L2 regularization parameter for the \"l-bfgs-b\" optimizer.</p> <code>L_BFGS_B_L</code> <code>l_bfgs_b_epsilon</code> <code>float</code> <p>The epsilon parameter for the \"l-bfgs-b\" optimizer.</p> <code>L_BFGS_B_EPSILON</code> <code>l_bfgs_b_norm</code> <code>str</code> <p>The norm to use for the \"l-bfgs-b\" optimizer. Options are: \"l1\", \"l2\".</p> <code>L_BFGS_B_NORM</code> <code>intercept</code> <code>bool</code> <p>Whether to include an intercept in the model.</p> <code>INTERCEPT</code> <code>random_intercept</code> <code>bool</code> <p>Whether to include a random intercept in the model.</p> <code>RANDOM_INTERCEPT</code> <code>heteroskedastic</code> <code>bool</code> <p>Whether to model heteroskedasticity in the data.</p> <code>HETEROSKEDASTIC</code> <code>intercept_var</code> <code>bool</code> <p>Whether the variance has an intercept (a fixed effect).</p> <code>INTERCEPT_VAR</code> <code>random_intercept_var</code> <code>bool</code> <p>Whether the variance has a random intercept for each group.</p> <code>RANDOM_INTERCEPT_VAR</code> <p>Methods:</p> Name Description <code>from_args</code> <p>Class method to create an instance of <code>BLRConf</code> from a dictionary of arguments.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; args = {'param1': value1, 'param2': value2}\n&gt;&gt;&gt; config = BLRConf.from_args(args)\n&gt;&gt;&gt; print(config)\nBLRConf(param1=value1, param2=value2)\n</code></pre> Source code in <code>pcntoolkit/regression_model/blr/blr_conf.py</code> <pre><code>@dataclass(frozen=True)\nclass BLRConf(RegConf):\n    \"\"\"\n    Class: BLRConf\n\n    A dataclass for configuring Bayesian Linear Regression (BLR) models. This class encapsulates\n    the parameters required to set up and execute a BLR model, providing a structured way to\n    manage configuration data.\n\n    Parameters\n    ----------\n    n_iter: int = N_ITER\n        The number of iterations for the optimization algorithm.\n    tol: float = TOL\n        The tolerance for the optimization algorithm.\n    ard: bool = ARD\n        Whether to use Automatic Relevance Determination (ARD).\n    optimizer: str = OPTIMIZER\n        The optimization algorithm to use. Options are: \"l-bfgs-b\", \"cg\", \"powell\", \"nelder-mead\".\n    l_bfgs_b_l: float = L_BFGS_B_L\n        The L2 regularization parameter for the \"l-bfgs-b\" optimizer.\n    l_bfgs_b_epsilon: float = L_BFGS_B_EPSILON\n        The epsilon parameter for the \"l-bfgs-b\" optimizer.\n    l_bfgs_b_norm: str = L_BFGS_B_NORM\n        The norm to use for the \"l-bfgs-b\" optimizer. Options are: \"l1\", \"l2\".\n    intercept: bool = INTERCEPT\n        Whether to include an intercept in the model.\n    random_intercept: bool = RANDOM_INTERCEPT\n        Whether to include a random intercept in the model.\n    heteroskedastic: bool = HETEROSKEDASTIC\n        Whether to model heteroskedasticity in the data.\n    intercept_var: bool = INTERCEPT_VAR\n        Whether the variance has an intercept (a fixed effect).\n    random_intercept_var: bool = RANDOM_INTERCEPT_VAR\n        Whether the variance has a random intercept for each group.\n\n    Methods\n    -------\n    from_args(args: dict[str, Any]) -&gt; BLRConf\n        Class method to create an instance of `BLRConf` from a dictionary of arguments.\n\n    Examples\n    --------\n    &gt;&gt;&gt; args = {'param1': value1, 'param2': value2}\n    &gt;&gt;&gt; config = BLRConf.from_args(args)\n    &gt;&gt;&gt; print(config)\n    BLRConf(param1=value1, param2=value2)\n    \"\"\"\n    # some configuration parameters\n    n_iter: int = N_ITER\n    tol: float = TOL\n\n    # use ard\n    ard: bool = ARD\n\n    # optimization parameters\n    optimizer: str = OPTIMIZER  # options: \"l-bfgs-b\", \"cg\", \"powell\", \" nelder-mead\"\n    l_bfgs_b_l: float = L_BFGS_B_L\n    l_bfgs_b_epsilon: float = L_BFGS_B_EPSILON\n    l_bfgs_b_norm: str = L_BFGS_B_NORM\n\n    # Design matrix configuration\n    intercept: bool = INTERCEPT\n    random_intercept: bool = RANDOM_INTERCEPT\n    heteroskedastic: bool = HETEROSKEDASTIC\n    intercept_var: bool = INTERCEPT_VAR\n    random_intercept_var: bool = RANDOM_INTERCEPT_VAR\n\n    # TODO implement warp\n    # warp: WarpBase = None\n    # warp_reparam: bool = Falses\n\n    def detect_configuration_problems(self) -&gt; list[str]:\n        \"\"\"\n        Detects problems in the configuration and returns them as a list of strings.\n        The super class will throw an exception if the configuration is invalid, and show the problems.\n        \"\"\"\n\n        configuration_problems = []\n\n        def add_problem(problem: str) -&gt; None:\n            nonlocal configuration_problems\n            configuration_problems.append(f\"{problem}\")\n\n        if self.n_iter &lt; 1:\n            add_problem(\"n_iter must be greater than 0.\")\n\n        if self.tol &lt;= 0:\n            add_problem(\"tol must be greater than 0.\")\n\n        if self.optimizer not in [\"l-bfgs-b\", \"cg\", \"powell\", \"nelder-mead\"]:\n            add_problem(f\"Optimizer {self.optimizer} not recognized.\")\n\n        return configuration_problems\n\n    @classmethod\n    def from_args(cls, args:dict[str, Any]) -&gt; \"BLRConf\":\n        args_filt:dict[str, Any] = {k: v for k, v in args.items() if k in fields(cls)}\n\n        return cls(\n            n_iter=args_filt.get(\"n_iter\", N_ITER),\n            tol=args_filt.get(\"tol\", TOL),\n            ard=args_filt.get(\"ard\", ARD),\n            optimizer=args_filt.get(\"optimizer\", OPTIMIZER),\n            l_bfgs_b_l=args_filt.get(\"l_bfgs_b_l\", L_BFGS_B_L),\n            l_bfgs_b_epsilon=args_filt.get(\"l_bfgs_b_epsilon\", L_BFGS_B_EPSILON),\n            l_bfgs_b_norm=args_filt.get(\"l_bfgs_b_norm\", L_BFGS_B_NORM),\n            intercept=args_filt.get(\"intercept\", INTERCEPT),\n            random_intercept=args_filt.get(\"random_intercept\", RANDOM_INTERCEPT),\n            heteroskedastic=args_filt.get(\"heteroskedastic\", HETEROSKEDASTIC),\n            intercept_var=args_filt.get(\"intercept_var\", INTERCEPT_VAR),\n            random_intercept_var=args_filt.get(\n                \"random_intercept_var\", RANDOM_INTERCEPT_VAR\n            ),\n        )\n\n    @classmethod\n    def from_dict(cls, dct:dict[str, Any]) -&gt; \"BLRConf\":\n        return cls(\n            n_iter=dct[\"n_iter\"],\n            tol=dct[\"tol\"],\n            ard=dct[\"ard\"],\n            optimizer=dct[\"optimizer\"],\n            l_bfgs_b_l=dct[\"l_bfgs_b_l\"],\n            l_bfgs_b_epsilon=dct[\"l_bfgs_b_epsilon\"],\n            l_bfgs_b_norm=dct[\"l_bfgs_b_norm\"],\n            intercept=dct[\"intercept\"],\n            random_intercept=dct[\"random_intercept\"],\n            heteroskedastic=dct[\"heteroskedastic\"],\n            intercept_var=dct[\"intercept_var\"],\n            random_intercept_var=dct[\"random_intercept_var\"],\n        )\n    def to_dict(self, path:str|None=\"\") -&gt; dict[str, Any]:\n        return {\n            \"n_iter\": self.n_iter,\n            \"tol\": self.tol,\n            \"ard\": self.ard,\n            \"optimizer\": self.optimizer,\n            \"l_bfgs_b_l\": self.l_bfgs_b_l,\n            \"l_bfgs_b_epsilon\": self.l_bfgs_b_epsilon,\n            \"l_bfgs_b_norm\": self.l_bfgs_b_norm,\n            \"intercept\": self.intercept,\n            \"random_intercept\": self.random_intercept,\n            \"heteroskedastic\": self.heteroskedastic,\n            \"intercept_var\": self.intercept_var,\n            \"random_intercept_var\": self.random_intercept_var,\n        }\n\n    @property\n    def has_random_effect(self) -&gt; bool:\n        return self.random_intercept or self.random_intercept_var\n</code></pre> <code>detect_configuration_problems() -&gt; list[str]</code> \u00b6 <p>Detects problems in the configuration and returns them as a list of strings. The super class will throw an exception if the configuration is invalid, and show the problems.</p> Source code in <code>pcntoolkit/regression_model/blr/blr_conf.py</code> <pre><code>def detect_configuration_problems(self) -&gt; list[str]:\n    \"\"\"\n    Detects problems in the configuration and returns them as a list of strings.\n    The super class will throw an exception if the configuration is invalid, and show the problems.\n    \"\"\"\n\n    configuration_problems = []\n\n    def add_problem(problem: str) -&gt; None:\n        nonlocal configuration_problems\n        configuration_problems.append(f\"{problem}\")\n\n    if self.n_iter &lt; 1:\n        add_problem(\"n_iter must be greater than 0.\")\n\n    if self.tol &lt;= 0:\n        add_problem(\"tol must be greater than 0.\")\n\n    if self.optimizer not in [\"l-bfgs-b\", \"cg\", \"powell\", \"nelder-mead\"]:\n        add_problem(f\"Optimizer {self.optimizer} not recognized.\")\n\n    return configuration_problems\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.blr.blr_data","title":"<code>blr_data</code>","text":"<p>Module for handling data in Bayesian linear regression.</p> <p>This module provides the BLRData class, which is used to store and manage data for Bayesian linear regression models. It includes functionality for validating and expanding data arrays, setting batch effects, and managing covariate and data point counts.</p> <p>Classes:</p> Name Description <code>BLRData</code> <p>An object to store the data used in Bayesian linear regression.</p>"},{"location":"api/#pcntoolkit.regression_model.blr.blr_data.BLRData","title":"<code>BLRData</code>","text":"<p>An object to store the data used in Bayesian linear regression.</p> Source code in <code>pcntoolkit/regression_model/blr/blr_data.py</code> <pre><code>class BLRData:\n    \"\"\"An object to store the data used in Bayesian linear regression.\"\"\"\n\n    def __init__(\n        self,\n        X: np.ndarray,\n        y: Optional[np.ndarray] = None,\n        var_X: Optional[np.ndarray] = None,\n        batch_effects: Optional[np.ndarray] = None,\n        response_var: Optional[str] = None,\n    ):\n        \"\"\"\n        Initializes the BLRData object.\n        Parameters\n        ----------\n        X : np.ndarray\n            The input data matrix.\n        y : np.ndarray, optional\n            The response variable.\n        var_X : np.ndarray, optional\n            The variance of the input data.\n        batch_effects : np.ndarray, optional\n            The batch effects data.\n        response_var : str, optional\n            The name of the response variable.\n        \"\"\"\n        self._batch_effects_maps = {}\n        self.check_and_set_data(X, y, var_X, batch_effects)\n        self.response_var = response_var\n        self._n_covariates = self.X.shape[1]\n        self._n_datapoints = self.X.shape[0]\n        self._n_batch_effect_columns = self.batch_effects.shape[1]\n\n    def check_and_set_data(\n        self, X: np.ndarray, y: Optional[np.ndarray], var_X: Optional[np.ndarray], batch_effects: Optional[np.ndarray]\n    ) -&gt; None:\n        \"\"\"\n        Checks that the data is valid and sets the data attributes.\n\n        Parameters\n        ----------\n        X : np.ndarray\n            The input data matrix.\n        y : np.ndarray, optional\n            The response variable.\n        var_X : np.ndarray, optional\n            The variance of the input data.\n        batch_effects : np.ndarray, optional\n            The batch effects data.\n\n        Raises\n        ------\n        ValueError\n            If X is not provided.\n        \"\"\"\n        if X is None:\n            raise ValueError(\"X must be provided\")\n        else:\n            self.X = X\n\n        if y is None:\n            warnings.warn(\"y is not provided, setting self.y to zeros\")\n            self.y = np.zeros((X.shape[0], 1))\n        else:\n            self.y = y\n\n        if var_X is None:\n            warnings.warn(\"var_X is not provided, setting self.var_X to zeros\")\n            self.var_X = np.zeros((X.shape[0], 1))\n        else:\n            self.var_X = var_X\n\n        if batch_effects is None:\n            warnings.warn(\n                \"batch_effects is not provided, setting self.batch_effects to zeros\"\n            )\n            self.batch_effects = np.zeros((X.shape[0], 1))\n        else:\n            self.batch_effects = batch_effects\n\n        self.X, self.var_X, self.batch_effects = self.expand_all(\n            \"X\", \"var_X\", \"batch_effects\"\n        )\n\n        assert (\n            self.X.shape[0]\n            == self.y.shape[0]\n            == self.var_X.shape[0]\n            == self.batch_effects.shape[0]\n        ), \"X, var_X, y and batch_effects must have the same number of rows\"\n\n        if len(self.y.shape) &gt; 1:\n            assert (\n                self.y.shape[1] == 1\n            ), \"y can only have one column, or it must be a 1D array\"\n            self.y = np.squeeze(self.y)\n\n    def expand_all(self, *args: str) -&gt; Tuple[np.ndarray, ...]:\n        \"\"\"\n        Expands all data attributes.\n\n        Parameters\n        ----------\n        *args : str\n            The names of the data attributes to expand.\n\n        Returns\n        -------\n        tuple of np.ndarray\n            The expanded data attributes.\n        \"\"\"\n        return tuple(self.expand(arg) for arg in args)\n\n    def expand(self, data_attr_str: str) -&gt; np.ndarray:\n        \"\"\"\n        Expands a 1D array to a 2D array if necessary.\n\n        Parameters\n        ----------\n        data_attr_str : str\n            The name of the data attribute to expand.\n\n        Returns\n        -------\n        np.ndarray\n            The expanded data attribute.\n\n        Raises\n        ------\n        AssertionError\n            If the array is not 1D or 2D.\n        \"\"\"\n        data_attr: np.ndarray = getattr(self, data_attr_str)\n        if len(data_attr.shape) == 1:\n            data_attr = data_attr.reshape(-1, 1)\n        assert len(data_attr.shape) == 2, f\"{data_attr_str} must be a 1D or 2D array\"\n        return data_attr\n\n    def set_batch_effects_maps(self, batch_effects_maps: dict[str, dict[Any, int]]) -&gt; None:\n        \"\"\"\n        Sets the batch effects map.\n\n        Parameters\n        ----------\n        batch_effects_maps : dict of str to dict of Any to int\n            The batch effects map.\n        \"\"\"\n        self._batch_effects_maps = batch_effects_maps\n\n    @property\n    def n_covariates(self) -&gt; int:\n        \"\"\"int: The number of covariates.\"\"\"\n        return self._n_covariates\n\n    @property\n    def n_datapoints(self) -&gt; int:\n        \"\"\"int: The number of data points.\"\"\"\n        return self._n_datapoints\n\n    @property\n    def n_batch_effect_columns(self) -&gt; int:\n        \"\"\"int: The number of batch effect columns.\"\"\"\n        return self._n_batch_effect_columns\n\n    @property\n    def batch_effects_maps(self) -&gt; dict[str, dict[Any, int]]:\n        \"\"\"dict of str to dict of Any to int: The batch effects map.\"\"\"\n        return self._batch_effects_maps\n</code></pre> <code>batch_effects_maps: dict[str, dict[Any, int]]</code> <code>property</code> \u00b6 <p>dict of str to dict of Any to int: The batch effects map.</p> <code>n_batch_effect_columns: int</code> <code>property</code> \u00b6 <p>int: The number of batch effect columns.</p> <code>n_covariates: int</code> <code>property</code> \u00b6 <p>int: The number of covariates.</p> <code>n_datapoints: int</code> <code>property</code> \u00b6 <p>int: The number of data points.</p> <code>__init__(X: np.ndarray, y: Optional[np.ndarray] = None, var_X: Optional[np.ndarray] = None, batch_effects: Optional[np.ndarray] = None, response_var: Optional[str] = None)</code> \u00b6 <p>Initializes the BLRData object.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input data matrix.</p> required <code>y</code> <code>ndarray</code> <p>The response variable.</p> <code>None</code> <code>var_X</code> <code>ndarray</code> <p>The variance of the input data.</p> <code>None</code> <code>batch_effects</code> <code>ndarray</code> <p>The batch effects data.</p> <code>None</code> <code>response_var</code> <code>str</code> <p>The name of the response variable.</p> <code>None</code> Source code in <code>pcntoolkit/regression_model/blr/blr_data.py</code> <pre><code>def __init__(\n    self,\n    X: np.ndarray,\n    y: Optional[np.ndarray] = None,\n    var_X: Optional[np.ndarray] = None,\n    batch_effects: Optional[np.ndarray] = None,\n    response_var: Optional[str] = None,\n):\n    \"\"\"\n    Initializes the BLRData object.\n    Parameters\n    ----------\n    X : np.ndarray\n        The input data matrix.\n    y : np.ndarray, optional\n        The response variable.\n    var_X : np.ndarray, optional\n        The variance of the input data.\n    batch_effects : np.ndarray, optional\n        The batch effects data.\n    response_var : str, optional\n        The name of the response variable.\n    \"\"\"\n    self._batch_effects_maps = {}\n    self.check_and_set_data(X, y, var_X, batch_effects)\n    self.response_var = response_var\n    self._n_covariates = self.X.shape[1]\n    self._n_datapoints = self.X.shape[0]\n    self._n_batch_effect_columns = self.batch_effects.shape[1]\n</code></pre> <code>check_and_set_data(X: np.ndarray, y: Optional[np.ndarray], var_X: Optional[np.ndarray], batch_effects: Optional[np.ndarray]) -&gt; None</code> \u00b6 <p>Checks that the data is valid and sets the data attributes.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input data matrix.</p> required <code>y</code> <code>ndarray</code> <p>The response variable.</p> required <code>var_X</code> <code>ndarray</code> <p>The variance of the input data.</p> required <code>batch_effects</code> <code>ndarray</code> <p>The batch effects data.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If X is not provided.</p> Source code in <code>pcntoolkit/regression_model/blr/blr_data.py</code> <pre><code>def check_and_set_data(\n    self, X: np.ndarray, y: Optional[np.ndarray], var_X: Optional[np.ndarray], batch_effects: Optional[np.ndarray]\n) -&gt; None:\n    \"\"\"\n    Checks that the data is valid and sets the data attributes.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input data matrix.\n    y : np.ndarray, optional\n        The response variable.\n    var_X : np.ndarray, optional\n        The variance of the input data.\n    batch_effects : np.ndarray, optional\n        The batch effects data.\n\n    Raises\n    ------\n    ValueError\n        If X is not provided.\n    \"\"\"\n    if X is None:\n        raise ValueError(\"X must be provided\")\n    else:\n        self.X = X\n\n    if y is None:\n        warnings.warn(\"y is not provided, setting self.y to zeros\")\n        self.y = np.zeros((X.shape[0], 1))\n    else:\n        self.y = y\n\n    if var_X is None:\n        warnings.warn(\"var_X is not provided, setting self.var_X to zeros\")\n        self.var_X = np.zeros((X.shape[0], 1))\n    else:\n        self.var_X = var_X\n\n    if batch_effects is None:\n        warnings.warn(\n            \"batch_effects is not provided, setting self.batch_effects to zeros\"\n        )\n        self.batch_effects = np.zeros((X.shape[0], 1))\n    else:\n        self.batch_effects = batch_effects\n\n    self.X, self.var_X, self.batch_effects = self.expand_all(\n        \"X\", \"var_X\", \"batch_effects\"\n    )\n\n    assert (\n        self.X.shape[0]\n        == self.y.shape[0]\n        == self.var_X.shape[0]\n        == self.batch_effects.shape[0]\n    ), \"X, var_X, y and batch_effects must have the same number of rows\"\n\n    if len(self.y.shape) &gt; 1:\n        assert (\n            self.y.shape[1] == 1\n        ), \"y can only have one column, or it must be a 1D array\"\n        self.y = np.squeeze(self.y)\n</code></pre> <code>expand(data_attr_str: str) -&gt; np.ndarray</code> \u00b6 <p>Expands a 1D array to a 2D array if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>data_attr_str</code> <code>str</code> <p>The name of the data attribute to expand.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The expanded data attribute.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the array is not 1D or 2D.</p> Source code in <code>pcntoolkit/regression_model/blr/blr_data.py</code> <pre><code>def expand(self, data_attr_str: str) -&gt; np.ndarray:\n    \"\"\"\n    Expands a 1D array to a 2D array if necessary.\n\n    Parameters\n    ----------\n    data_attr_str : str\n        The name of the data attribute to expand.\n\n    Returns\n    -------\n    np.ndarray\n        The expanded data attribute.\n\n    Raises\n    ------\n    AssertionError\n        If the array is not 1D or 2D.\n    \"\"\"\n    data_attr: np.ndarray = getattr(self, data_attr_str)\n    if len(data_attr.shape) == 1:\n        data_attr = data_attr.reshape(-1, 1)\n    assert len(data_attr.shape) == 2, f\"{data_attr_str} must be a 1D or 2D array\"\n    return data_attr\n</code></pre> <code>expand_all(*args: str) -&gt; Tuple[np.ndarray, ...]</code> \u00b6 <p>Expands all data attributes.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>The names of the data attributes to expand.</p> <code>()</code> <p>Returns:</p> Type Description <code>tuple of np.ndarray</code> <p>The expanded data attributes.</p> Source code in <code>pcntoolkit/regression_model/blr/blr_data.py</code> <pre><code>def expand_all(self, *args: str) -&gt; Tuple[np.ndarray, ...]:\n    \"\"\"\n    Expands all data attributes.\n\n    Parameters\n    ----------\n    *args : str\n        The names of the data attributes to expand.\n\n    Returns\n    -------\n    tuple of np.ndarray\n        The expanded data attributes.\n    \"\"\"\n    return tuple(self.expand(arg) for arg in args)\n</code></pre> <code>set_batch_effects_maps(batch_effects_maps: dict[str, dict[Any, int]]) -&gt; None</code> \u00b6 <p>Sets the batch effects map.</p> <p>Parameters:</p> Name Type Description Default <code>batch_effects_maps</code> <code>dict of str to dict of Any to int</code> <p>The batch effects map.</p> required Source code in <code>pcntoolkit/regression_model/blr/blr_data.py</code> <pre><code>def set_batch_effects_maps(self, batch_effects_maps: dict[str, dict[Any, int]]) -&gt; None:\n    \"\"\"\n    Sets the batch effects map.\n\n    Parameters\n    ----------\n    batch_effects_maps : dict of str to dict of Any to int\n        The batch effects map.\n    \"\"\"\n    self._batch_effects_maps = batch_effects_maps\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.blr.warp","title":"<code>warp</code>","text":"<p>Warping functions for transforming non-Gaussian to Gaussian distributions.</p> <p>This module implements various warping functions used to transform non-Gaussian  response variables to Gaussian variables for use in Gaussian process models. The  transformations are based on the compositionally-warped Gaussian processes framework  from Rios and Torab (2019).</p> <p>The module provides the following warping functions:     - WarpLog: Logarithmic transformation     - WarpAffine: Affine transformation (y = a + b*x)     - WarpBoxCox: Box-Cox transformation     - WarpSinArcsinh: Sinh-arcsinh transformation     - WarpCompose: Composition of multiple warping functions</p> <p>Each warping function implements three core methods:     - f(): Forward transformation (non-Gaussian -&gt; Gaussian)     - invf(): Inverse transformation (Gaussian -&gt; non-Gaussian)     - df(): Derivative of the transformation</p> Example <p>from pcntoolkit.regression_model.blr.warp import WarpBoxCox, WarpCompose</p> References <p>.. [1] Rios, G., &amp; Tobar, F. (2019). Compositionally-warped Gaussian processes.        Neural Networks, 118, 235-246.        https://www.sciencedirect.com/science/article/pii/S0893608019301856</p> See Also <p>pcntoolkit.regression_model.blr : Bayesian linear regression module</p>"},{"location":"api/#pcntoolkit.regression_model.blr.warp--single-warping-function","title":"Single warping function","text":"<p>warp = WarpBoxCox() y_gaussian = warp.f(x, params=[0.5])</p>"},{"location":"api/#pcntoolkit.regression_model.blr.warp--composition-of-warping-functions","title":"Composition of warping functions","text":"<p>warp = WarpCompose(['WarpBoxCox', 'WarpAffine']) y_gaussian = warp.f(x, params=[0.5, 0.0, 1.0])</p>"},{"location":"api/#pcntoolkit.regression_model.blr.warp.WarpAffine","title":"<code>WarpAffine</code>","text":"<p>               Bases: <code>WarpBase</code></p> <p>Affine warping function.</p> <p>Implements affine transformation y = a + b*x where:     - a: offset parameter     - b: scale parameter (constrained positive through exp transform)</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>class WarpAffine(WarpBase):\n    \"\"\"Affine warping function.\n\n    Implements affine transformation y = a + b*x where:\n        - a: offset parameter\n        - b: scale parameter (constrained positive through exp transform)\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.n_params = 2\n\n    def _get_params(self, param: List[float]) -&gt; Tuple[float, float]:\n        \"\"\"Extract and transform the affine parameters.\n\n        Parameters\n        ----------\n        param : List[float]\n            List containing [a, log(b)] where:\n                a: offset parameter\n                log(b): log of scale parameter\n\n        Returns\n        -------\n        Tuple[float, float]\n            Tuple of (a, b) parameters\n\n        Raises\n        ------\n        ValueError\n            If param length doesn't match n_params\n        \"\"\"\n        if len(param) != self.n_params:\n            raise ValueError(\"number of parameters must be \" + str(self.n_params))\n        return param[0], np.exp(param[1])\n\n    def f(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n        \"\"\"Apply affine warping.\n\n        Parameters\n        ----------\n        x : NDArray[np.float64]\n            Input values\n        params : List[float]\n            Affine parameters [a, log(b)]\n\n        Returns\n        -------\n        NDArray[np.float64]\n            a + b*x\n        \"\"\"\n        a, b = self._get_params(param)\n        y = a + b * x\n        return y\n\n    def invf(self, y: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n        \"\"\"Apply inverse affine warping.\n\n        Parameters\n        ----------\n        y : NDArray[np.float64]\n            Input values\n        params : List[float]\n            Affine parameters [a, log(b)]\n\n        Returns\n        -------\n        NDArray[np.float64]\n            (y - a)/b\n        \"\"\"\n        a, b = self._get_params(param)\n        x = (y - a) / b\n        return x\n\n    def df(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n        \"\"\"Compute derivative of affine warp.\n\n        Parameters\n        ----------\n        x : NDArray[np.float64]\n            Input values\n        params : List[float]\n            Affine parameters [a, log(b)]\n\n        Returns\n        -------\n        NDArray[np.float64]\n            Constant derivative b\n        \"\"\"\n        _, b = self._get_params(param)\n        df = np.ones(x.shape) * b\n        return df\n</code></pre> <code>df(x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]</code> \u00b6 <p>Compute derivative of affine warp.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>NDArray[float64]</code> <p>Input values</p> required <code>params</code> <code>List[float]</code> <p>Affine parameters [a, log(b)]</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Constant derivative b</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def df(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n    \"\"\"Compute derivative of affine warp.\n\n    Parameters\n    ----------\n    x : NDArray[np.float64]\n        Input values\n    params : List[float]\n        Affine parameters [a, log(b)]\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Constant derivative b\n    \"\"\"\n    _, b = self._get_params(param)\n    df = np.ones(x.shape) * b\n    return df\n</code></pre> <code>f(x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]</code> \u00b6 <p>Apply affine warping.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>NDArray[float64]</code> <p>Input values</p> required <code>params</code> <code>List[float]</code> <p>Affine parameters [a, log(b)]</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>a + b*x</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def f(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n    \"\"\"Apply affine warping.\n\n    Parameters\n    ----------\n    x : NDArray[np.float64]\n        Input values\n    params : List[float]\n        Affine parameters [a, log(b)]\n\n    Returns\n    -------\n    NDArray[np.float64]\n        a + b*x\n    \"\"\"\n    a, b = self._get_params(param)\n    y = a + b * x\n    return y\n</code></pre> <code>invf(y: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]</code> \u00b6 <p>Apply inverse affine warping.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[float64]</code> <p>Input values</p> required <code>params</code> <code>List[float]</code> <p>Affine parameters [a, log(b)]</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>(y - a)/b</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def invf(self, y: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n    \"\"\"Apply inverse affine warping.\n\n    Parameters\n    ----------\n    y : NDArray[np.float64]\n        Input values\n    params : List[float]\n        Affine parameters [a, log(b)]\n\n    Returns\n    -------\n    NDArray[np.float64]\n        (y - a)/b\n    \"\"\"\n    a, b = self._get_params(param)\n    x = (y - a) / b\n    return x\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.blr.warp.WarpBase","title":"<code>WarpBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for likelihood warping functions.</p> <p>This class implements warping functions following Rios and Torab (2019) Compositionally-warped Gaussian processes [1]_.</p> <p>All Warps must define the following methods:     - get_n_params(): Return number of parameters     - f(): Warping function (Non-Gaussian field -&gt; Gaussian)     - invf(): Inverse warp     - df(): Derivatives     - warp_predictions(): Compute predictive distribution</p> References <p>.. [1] Rios, G., &amp; Tobar, F. (2019). Compositionally-warped Gaussian processes.        Neural Networks, 118, 235-246.        https://www.sciencedirect.com/science/article/pii/S0893608019301856</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>class WarpBase(ABC):\n    \"\"\"Base class for likelihood warping functions.\n\n    This class implements warping functions following Rios and Torab (2019)\n    Compositionally-warped Gaussian processes [1]_.\n\n    All Warps must define the following methods:\n        - get_n_params(): Return number of parameters\n        - f(): Warping function (Non-Gaussian field -&gt; Gaussian)\n        - invf(): Inverse warp\n        - df(): Derivatives\n        - warp_predictions(): Compute predictive distribution\n\n    References\n    ----------\n    .. [1] Rios, G., &amp; Tobar, F. (2019). Compositionally-warped Gaussian processes.\n           Neural Networks, 118, 235-246.\n           https://www.sciencedirect.com/science/article/pii/S0893608019301856\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self.n_params: int = 0\n\n    def get_n_params(self) -&gt; int:\n        \"\"\"Return the number of parameters required by the warping function.\n\n        Returns\n        -------\n        int\n            Number of parameters\n\n        Raises\n        ------\n        AssertionError\n            If warp function is not initialized\n        \"\"\"\n        assert not np.isnan(self.n_params), \"Warp function not initialised\"\n        return self.n_params\n\n    def warp_predictions(\n        self,\n        mu: NDArray[np.float64],\n        s2: NDArray[np.float64],\n        param: List[float],\n        percentiles: List[float] | None= None,\n    ) -&gt; Tuple[NDArray[np.float64], NDArray[np.float64]]:\n        \"\"\"Compute warped predictions from a Gaussian predictive distribution.\n\n        Parameters\n        ----------\n        mu : NDArray[np.float64]\n            Gaussian predictive mean\n        s2 : NDArray[np.float64]\n            Predictive variance\n        param : List[float]\n            Warping parameters\n        percentiles : List[float], optional\n            Desired percentiles of the warped likelihood, by default [0.025, 0.975]\n\n        Returns\n        -------\n        Tuple[NDArray[np.float64], NDArray[np.float64]]\n            - median: Median of the predictive distribution\n            - pred_interval: Predictive interval(s)\n        \"\"\"\n\n        if percentiles is None:\n            percentiles = [0.025, 0.975]\n\n        # Compute percentiles of a standard Gaussian\n        N = norm\n        Z = N.ppf(percentiles)\n\n        # find the median (using mu = median)\n        median = self.invf(mu, param)\n\n        # compute the predictive intervals (non-stationary)\n        pred_interval = np.zeros((len(mu), len(Z)))\n        for i, z in enumerate(Z):\n            pred_interval[:, i] = self.invf(mu + np.sqrt(s2) * z, param)\n\n        return median, pred_interval\n\n    @abstractmethod\n    def f(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n        \"\"\"Evaluate the warping function.\n\n        Maps non-Gaussian response variables to Gaussian variables.\n\n        Parameters\n        ----------\n        x : NDArray[np.float64]\n            Input values to warp\n        param : List[float]\n            Warping parameters\n\n        Returns\n        -------\n        NDArray[np.float64]\n            Warped values\n        \"\"\"\n\n    @abstractmethod\n    def invf(self, y: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n        \"\"\"Evaluate the inverse warping function.\n\n        Maps Gaussian latent variables to non-Gaussian response variables.\n\n        Parameters\n        ----------\n        y : NDArray[np.float64]\n            Input values to inverse warp\n        param : List[float]\n            Warping parameters\n\n        Returns\n        -------\n        NDArray[np.float64]\n            Inverse warped values\n        \"\"\"\n\n    @abstractmethod\n    def df(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n        \"\"\"Return the derivative of the warp, dw(x)/dx.\n\n        Parameters\n        ----------\n        x : NDArray[np.float64]\n            Input values\n        param : List[float]\n            Warping parameters\n\n        Returns\n        -------\n        NDArray[np.float64]\n            Derivative values\n        \"\"\"\n</code></pre> <code>df(x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]</code> <code>abstractmethod</code> \u00b6 <p>Return the derivative of the warp, dw(x)/dx.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>NDArray[float64]</code> <p>Input values</p> required <code>param</code> <code>List[float]</code> <p>Warping parameters</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Derivative values</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>@abstractmethod\ndef df(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n    \"\"\"Return the derivative of the warp, dw(x)/dx.\n\n    Parameters\n    ----------\n    x : NDArray[np.float64]\n        Input values\n    param : List[float]\n        Warping parameters\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Derivative values\n    \"\"\"\n</code></pre> <code>f(x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]</code> <code>abstractmethod</code> \u00b6 <p>Evaluate the warping function.</p> <p>Maps non-Gaussian response variables to Gaussian variables.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>NDArray[float64]</code> <p>Input values to warp</p> required <code>param</code> <code>List[float]</code> <p>Warping parameters</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Warped values</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>@abstractmethod\ndef f(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n    \"\"\"Evaluate the warping function.\n\n    Maps non-Gaussian response variables to Gaussian variables.\n\n    Parameters\n    ----------\n    x : NDArray[np.float64]\n        Input values to warp\n    param : List[float]\n        Warping parameters\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Warped values\n    \"\"\"\n</code></pre> <code>get_n_params() -&gt; int</code> \u00b6 <p>Return the number of parameters required by the warping function.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of parameters</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If warp function is not initialized</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def get_n_params(self) -&gt; int:\n    \"\"\"Return the number of parameters required by the warping function.\n\n    Returns\n    -------\n    int\n        Number of parameters\n\n    Raises\n    ------\n    AssertionError\n        If warp function is not initialized\n    \"\"\"\n    assert not np.isnan(self.n_params), \"Warp function not initialised\"\n    return self.n_params\n</code></pre> <code>invf(y: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]</code> <code>abstractmethod</code> \u00b6 <p>Evaluate the inverse warping function.</p> <p>Maps Gaussian latent variables to non-Gaussian response variables.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[float64]</code> <p>Input values to inverse warp</p> required <code>param</code> <code>List[float]</code> <p>Warping parameters</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Inverse warped values</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>@abstractmethod\ndef invf(self, y: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n    \"\"\"Evaluate the inverse warping function.\n\n    Maps Gaussian latent variables to non-Gaussian response variables.\n\n    Parameters\n    ----------\n    y : NDArray[np.float64]\n        Input values to inverse warp\n    param : List[float]\n        Warping parameters\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Inverse warped values\n    \"\"\"\n</code></pre> <code>warp_predictions(mu: NDArray[np.float64], s2: NDArray[np.float64], param: List[float], percentiles: List[float] | None = None) -&gt; Tuple[NDArray[np.float64], NDArray[np.float64]]</code> \u00b6 <p>Compute warped predictions from a Gaussian predictive distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>NDArray[float64]</code> <p>Gaussian predictive mean</p> required <code>s2</code> <code>NDArray[float64]</code> <p>Predictive variance</p> required <code>param</code> <code>List[float]</code> <p>Warping parameters</p> required <code>percentiles</code> <code>List[float]</code> <p>Desired percentiles of the warped likelihood, by default [0.025, 0.975]</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[NDArray[float64], NDArray[float64]]</code> <ul> <li>median: Median of the predictive distribution</li> <li>pred_interval: Predictive interval(s)</li> </ul> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def warp_predictions(\n    self,\n    mu: NDArray[np.float64],\n    s2: NDArray[np.float64],\n    param: List[float],\n    percentiles: List[float] | None= None,\n) -&gt; Tuple[NDArray[np.float64], NDArray[np.float64]]:\n    \"\"\"Compute warped predictions from a Gaussian predictive distribution.\n\n    Parameters\n    ----------\n    mu : NDArray[np.float64]\n        Gaussian predictive mean\n    s2 : NDArray[np.float64]\n        Predictive variance\n    param : List[float]\n        Warping parameters\n    percentiles : List[float], optional\n        Desired percentiles of the warped likelihood, by default [0.025, 0.975]\n\n    Returns\n    -------\n    Tuple[NDArray[np.float64], NDArray[np.float64]]\n        - median: Median of the predictive distribution\n        - pred_interval: Predictive interval(s)\n    \"\"\"\n\n    if percentiles is None:\n        percentiles = [0.025, 0.975]\n\n    # Compute percentiles of a standard Gaussian\n    N = norm\n    Z = N.ppf(percentiles)\n\n    # find the median (using mu = median)\n    median = self.invf(mu, param)\n\n    # compute the predictive intervals (non-stationary)\n    pred_interval = np.zeros((len(mu), len(Z)))\n    for i, z in enumerate(Z):\n        pred_interval[:, i] = self.invf(mu + np.sqrt(s2) * z, param)\n\n    return median, pred_interval\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.blr.warp.WarpBoxCox","title":"<code>WarpBoxCox</code>","text":"<p>               Bases: <code>WarpBase</code></p> <p>Box-Cox warping function.</p> <p>Implements the Box-Cox transform with a single parameter (lambda): y = (sign(x) * abs(x) ** lambda - 1) / lambda</p> <p>This follows the generalization in Bicken and Doksum (1981) JASA 76 and allows x to assume negative values.</p> References <p>.. [1] Bickel, P. J., &amp; Doksum, K. A. (1981). An analysis of transformations        revisited. Journal of the American Statistical Association, 76(374), 296-311.</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>class WarpBoxCox(WarpBase):\n    \"\"\"Box-Cox warping function.\n\n    Implements the Box-Cox transform with a single parameter (lambda):\n    y = (sign(x) * abs(x) ** lambda - 1) / lambda\n\n    This follows the generalization in Bicken and Doksum (1981) JASA 76\n    and allows x to assume negative values.\n\n    References\n    ----------\n    .. [1] Bickel, P. J., &amp; Doksum, K. A. (1981). An analysis of transformations\n           revisited. Journal of the American Statistical Association, 76(374), 296-311.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.n_params = 1\n\n    def _get_params(self, param: List[float]) -&gt; NDArray[np.float64]:\n        \"\"\"Extract and transform the Box-Cox parameter.\n\n        Parameters\n        ----------\n        param : List[float]\n            List containing [log(lambda)]\n\n        Returns\n        -------\n        float\n            Transformed lambda parameter\n        \"\"\"\n        return np.exp(np.array(param))\n\n    def f(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n        \"\"\"Apply Box-Cox warping.\n\n        Parameters\n        ----------\n        x : NDArray[np.float64]\n            Input values\n        params : List[float]\n            Box-Cox parameter [log(lambda)]\n\n        Returns\n        -------\n        NDArray[np.float64]\n            Warped values\n        \"\"\"\n        lam = self._get_params(param)\n\n        if lam == 0:\n            y = np.log(x)\n        else:\n            y = (np.sign(x) * np.abs(x) ** lam - 1) / lam\n        return y\n\n    def invf(self, y: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n        \"\"\"Apply inverse Box-Cox warping.\n\n        Parameters\n        ----------\n        y : NDArray[np.float64]\n            Input values\n        params : List[float]\n            Box-Cox parameter [log(lambda)]\n\n        Returns\n        -------\n        NDArray[np.float64]\n            Inverse warped values\n        \"\"\"\n        lam = self._get_params(param)\n\n        if lam == 0:\n            x = np.exp(y)\n        else:\n            x = np.sign(lam * y + 1) * np.abs(lam * y + 1) ** (1 / lam)\n        return x\n\n    def df(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n        \"\"\"Compute derivative of Box-Cox warp.\n\n        Parameters\n        ----------\n        x : NDArray[np.float64]\n            Input values\n        params : List[float]\n            Box-Cox parameter [log(lambda)]\n\n        Returns\n        -------\n        NDArray[np.float64]\n            Derivative values\n        \"\"\"\n        lam = self._get_params(param)\n        dx = np.abs(x) ** (lam - 1)\n        return dx\n</code></pre> <code>df(x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]</code> \u00b6 <p>Compute derivative of Box-Cox warp.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>NDArray[float64]</code> <p>Input values</p> required <code>params</code> <code>List[float]</code> <p>Box-Cox parameter [log(lambda)]</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Derivative values</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def df(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n    \"\"\"Compute derivative of Box-Cox warp.\n\n    Parameters\n    ----------\n    x : NDArray[np.float64]\n        Input values\n    params : List[float]\n        Box-Cox parameter [log(lambda)]\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Derivative values\n    \"\"\"\n    lam = self._get_params(param)\n    dx = np.abs(x) ** (lam - 1)\n    return dx\n</code></pre> <code>f(x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]</code> \u00b6 <p>Apply Box-Cox warping.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>NDArray[float64]</code> <p>Input values</p> required <code>params</code> <code>List[float]</code> <p>Box-Cox parameter [log(lambda)]</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Warped values</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def f(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n    \"\"\"Apply Box-Cox warping.\n\n    Parameters\n    ----------\n    x : NDArray[np.float64]\n        Input values\n    params : List[float]\n        Box-Cox parameter [log(lambda)]\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Warped values\n    \"\"\"\n    lam = self._get_params(param)\n\n    if lam == 0:\n        y = np.log(x)\n    else:\n        y = (np.sign(x) * np.abs(x) ** lam - 1) / lam\n    return y\n</code></pre> <code>invf(y: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]</code> \u00b6 <p>Apply inverse Box-Cox warping.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[float64]</code> <p>Input values</p> required <code>params</code> <code>List[float]</code> <p>Box-Cox parameter [log(lambda)]</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Inverse warped values</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def invf(self, y: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n    \"\"\"Apply inverse Box-Cox warping.\n\n    Parameters\n    ----------\n    y : NDArray[np.float64]\n        Input values\n    params : List[float]\n        Box-Cox parameter [log(lambda)]\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Inverse warped values\n    \"\"\"\n    lam = self._get_params(param)\n\n    if lam == 0:\n        x = np.exp(y)\n    else:\n        x = np.sign(lam * y + 1) * np.abs(lam * y + 1) ** (1 / lam)\n    return x\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.blr.warp.WarpCompose","title":"<code>WarpCompose</code>","text":"<p>               Bases: <code>WarpBase</code></p> <p>Composition of multiple warping functions.</p> <p>Allows chaining multiple warps together. Warps are applied in sequence: y = warp_n(...warp_2(warp_1(x)))</p> Example <p>W = WarpCompose(('WarpBoxCox', 'WarpAffine'))</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>class WarpCompose(WarpBase):\n    \"\"\"Composition of multiple warping functions.\n\n    Allows chaining multiple warps together. Warps are applied in sequence:\n    y = warp_n(...warp_2(warp_1(x)))\n\n    Example\n    -------\n    W = WarpCompose(('WarpBoxCox', 'WarpAffine'))\n    \"\"\"\n\n    def __init__(\n        self, warpnames: Optional[List[str]] = None, debugwarp: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize composed warp.\n\n        Parameters\n        ----------\n        warpnames : Optional[List[str]], optional\n            List of warp class names to compose, by default None\n        debugwarp : bool, optional\n            Enable debug printing, by default False\n\n        Raises\n        ------\n        ValueError\n            If warpnames is None\n        \"\"\"\n        super().__init__()\n        if warpnames is None:\n            raise ValueError(\"A list of warp functions is required\")\n        self.debugwarp = debugwarp\n        self.warps: List[WarpBase] = []\n        self.n_params = 0\n        for wname in warpnames:\n            warp = literal_eval(wname + \"()\")  # type: ignore\n            self.n_params += warp.get_n_params()\n            self.warps.append(warp)\n\n    def f(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n        \"\"\"Apply composed warping functions.\n\n        Parameters\n        ----------\n        x : NDArray[np.float64]\n            Input values\n        param : List[float]\n            Combined parameters for all warps\n\n        Returns\n        -------\n        NDArray[np.float64]\n            Warped values after applying all transforms\n        \"\"\"\n        theta = param\n        theta_offset = 0\n\n        if self.debugwarp:\n            print(\"begin composition\")\n        for ci, warp in enumerate(self.warps):\n            n_params_c = warp.get_n_params()\n            theta_c = [theta[c] for c in range(theta_offset, theta_offset + n_params_c)]\n            theta_offset += n_params_c\n\n            if self.debugwarp:\n                print(\"f:\", ci, theta_c, warp)\n\n            if ci == 0:\n                fw = warp.f(x, theta_c)\n            else:\n                fw = warp.f(fw, theta_c)\n        return fw\n\n    def invf(self, y: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n        \"\"\"Apply inverse composed warping functions.\n\n        Parameters\n        ----------\n        y : NDArray[np.float64]\n            Input values\n        param : List[float]\n            Combined parameters for all warps\n\n        Returns\n        -------\n        NDArray[np.float64]\n            Inverse warped values after applying all inverse transforms\n        \"\"\"\n        theta = param\n\n        n_params = 0\n        n_warps = 0\n        if self.debugwarp:\n            print(\"begin composition\")\n\n        for ci, warp in enumerate(self.warps):\n            n_params += warp.get_n_params()\n            n_warps += 1\n        theta_offset = n_params\n        for ci, warp in reversed(list(enumerate(self.warps))):\n            n_params_c = warp.get_n_params()\n            theta_offset -= n_params_c\n            theta_c = [theta[c] for c in range(theta_offset, theta_offset + n_params_c)]\n\n            if self.debugwarp:\n                print(\"invf:\", theta_c, warp)\n\n            if ci == n_warps - 1:\n                finvw = warp.invf(y, theta_c)\n            else:\n                finvw = warp.invf(finvw, theta_c)\n\n        return finvw\n\n    def df(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n        \"\"\"Compute derivative of composed warping functions.\n\n        Parameters\n        ----------\n        x : NDArray[np.float64]\n            Input values\n        param : List[float]\n            Combined parameters for all warps\n\n        Returns\n        -------\n        NDArray[np.float64]\n            Combined derivative values\n        \"\"\"\n        theta = param\n        theta_offset = 0\n        if self.debugwarp:\n            print(\"begin composition\")\n        for ci, warp in enumerate(self.warps):\n            n_params_c = warp.get_n_params()\n\n            theta_c = [theta[c] for c in range(theta_offset, theta_offset + n_params_c)]\n            theta_offset += n_params_c\n\n            if self.debugwarp:\n                print(\"df:\", ci, theta_c, warp)\n\n            if ci == 0:\n                dfw = warp.df(x, theta_c)\n            else:\n                dfw = warp.df(dfw, theta_c)\n\n        return dfw\n</code></pre> <code>__init__(warpnames: Optional[List[str]] = None, debugwarp: bool = False) -&gt; None</code> \u00b6 <p>Initialize composed warp.</p> <p>Parameters:</p> Name Type Description Default <code>warpnames</code> <code>Optional[List[str]]</code> <p>List of warp class names to compose, by default None</p> <code>None</code> <code>debugwarp</code> <code>bool</code> <p>Enable debug printing, by default False</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If warpnames is None</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def __init__(\n    self, warpnames: Optional[List[str]] = None, debugwarp: bool = False\n) -&gt; None:\n    \"\"\"Initialize composed warp.\n\n    Parameters\n    ----------\n    warpnames : Optional[List[str]], optional\n        List of warp class names to compose, by default None\n    debugwarp : bool, optional\n        Enable debug printing, by default False\n\n    Raises\n    ------\n    ValueError\n        If warpnames is None\n    \"\"\"\n    super().__init__()\n    if warpnames is None:\n        raise ValueError(\"A list of warp functions is required\")\n    self.debugwarp = debugwarp\n    self.warps: List[WarpBase] = []\n    self.n_params = 0\n    for wname in warpnames:\n        warp = literal_eval(wname + \"()\")  # type: ignore\n        self.n_params += warp.get_n_params()\n        self.warps.append(warp)\n</code></pre> <code>df(x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]</code> \u00b6 <p>Compute derivative of composed warping functions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>NDArray[float64]</code> <p>Input values</p> required <code>param</code> <code>List[float]</code> <p>Combined parameters for all warps</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Combined derivative values</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def df(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n    \"\"\"Compute derivative of composed warping functions.\n\n    Parameters\n    ----------\n    x : NDArray[np.float64]\n        Input values\n    param : List[float]\n        Combined parameters for all warps\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Combined derivative values\n    \"\"\"\n    theta = param\n    theta_offset = 0\n    if self.debugwarp:\n        print(\"begin composition\")\n    for ci, warp in enumerate(self.warps):\n        n_params_c = warp.get_n_params()\n\n        theta_c = [theta[c] for c in range(theta_offset, theta_offset + n_params_c)]\n        theta_offset += n_params_c\n\n        if self.debugwarp:\n            print(\"df:\", ci, theta_c, warp)\n\n        if ci == 0:\n            dfw = warp.df(x, theta_c)\n        else:\n            dfw = warp.df(dfw, theta_c)\n\n    return dfw\n</code></pre> <code>f(x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]</code> \u00b6 <p>Apply composed warping functions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>NDArray[float64]</code> <p>Input values</p> required <code>param</code> <code>List[float]</code> <p>Combined parameters for all warps</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Warped values after applying all transforms</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def f(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n    \"\"\"Apply composed warping functions.\n\n    Parameters\n    ----------\n    x : NDArray[np.float64]\n        Input values\n    param : List[float]\n        Combined parameters for all warps\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Warped values after applying all transforms\n    \"\"\"\n    theta = param\n    theta_offset = 0\n\n    if self.debugwarp:\n        print(\"begin composition\")\n    for ci, warp in enumerate(self.warps):\n        n_params_c = warp.get_n_params()\n        theta_c = [theta[c] for c in range(theta_offset, theta_offset + n_params_c)]\n        theta_offset += n_params_c\n\n        if self.debugwarp:\n            print(\"f:\", ci, theta_c, warp)\n\n        if ci == 0:\n            fw = warp.f(x, theta_c)\n        else:\n            fw = warp.f(fw, theta_c)\n    return fw\n</code></pre> <code>invf(y: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]</code> \u00b6 <p>Apply inverse composed warping functions.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[float64]</code> <p>Input values</p> required <code>param</code> <code>List[float]</code> <p>Combined parameters for all warps</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Inverse warped values after applying all inverse transforms</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def invf(self, y: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n    \"\"\"Apply inverse composed warping functions.\n\n    Parameters\n    ----------\n    y : NDArray[np.float64]\n        Input values\n    param : List[float]\n        Combined parameters for all warps\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Inverse warped values after applying all inverse transforms\n    \"\"\"\n    theta = param\n\n    n_params = 0\n    n_warps = 0\n    if self.debugwarp:\n        print(\"begin composition\")\n\n    for ci, warp in enumerate(self.warps):\n        n_params += warp.get_n_params()\n        n_warps += 1\n    theta_offset = n_params\n    for ci, warp in reversed(list(enumerate(self.warps))):\n        n_params_c = warp.get_n_params()\n        theta_offset -= n_params_c\n        theta_c = [theta[c] for c in range(theta_offset, theta_offset + n_params_c)]\n\n        if self.debugwarp:\n            print(\"invf:\", theta_c, warp)\n\n        if ci == n_warps - 1:\n            finvw = warp.invf(y, theta_c)\n        else:\n            finvw = warp.invf(finvw, theta_c)\n\n    return finvw\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.blr.warp.WarpLog","title":"<code>WarpLog</code>","text":"<p>               Bases: <code>WarpBase</code></p> <p>Logarithmic warping function.</p> <p>Implements y = log(x) warping transformation.</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>class WarpLog(WarpBase):\n    \"\"\"Logarithmic warping function.\n\n    Implements y = log(x) warping transformation.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.n_params = 0\n\n    def f(\n        self, x: NDArray[np.float64], param: Optional[List[float]] = None\n    ) -&gt; NDArray[np.float64]:\n        \"\"\"Apply logarithmic warping.\n\n        Parameters\n        ----------\n        x : NDArray[np.float64]\n            Input values\n        params : Optional[List[float]], optional\n            Not used for logarithmic warp, by default None\n\n        Returns\n        -------\n        NDArray[np.float64]\n            log(x)\n        \"\"\"\n        return np.log(x)\n\n    def invf(\n        self, y: NDArray[np.float64], param: Optional[List[float]] = None\n    ) -&gt; NDArray[np.float64]:\n        \"\"\"Apply inverse logarithmic warping.\n\n        Parameters\n        ----------\n        y : NDArray[np.float64]\n            Input values\n        params : Optional[List[float]], optional\n            Not used for logarithmic warp, by default None\n\n        Returns\n        -------\n        NDArray[np.float64]\n            exp(y)\n        \"\"\"\n        return np.exp(y)\n\n    def df(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n        \"\"\"Compute derivative of logarithmic warp.\n\n        Parameters\n        ----------\n        x : NDArray[np.float64]\n            Input values\n        params : List[float]\n            Not used for logarithmic warp\n\n        Returns\n        -------\n        NDArray[np.float64]\n            1/x\n        \"\"\"\n        return 1 / x\n</code></pre> <code>df(x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]</code> \u00b6 <p>Compute derivative of logarithmic warp.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>NDArray[float64]</code> <p>Input values</p> required <code>params</code> <code>List[float]</code> <p>Not used for logarithmic warp</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>1/x</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def df(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n    \"\"\"Compute derivative of logarithmic warp.\n\n    Parameters\n    ----------\n    x : NDArray[np.float64]\n        Input values\n    params : List[float]\n        Not used for logarithmic warp\n\n    Returns\n    -------\n    NDArray[np.float64]\n        1/x\n    \"\"\"\n    return 1 / x\n</code></pre> <code>f(x: NDArray[np.float64], param: Optional[List[float]] = None) -&gt; NDArray[np.float64]</code> \u00b6 <p>Apply logarithmic warping.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>NDArray[float64]</code> <p>Input values</p> required <code>params</code> <code>Optional[List[float]]</code> <p>Not used for logarithmic warp, by default None</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>log(x)</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def f(\n    self, x: NDArray[np.float64], param: Optional[List[float]] = None\n) -&gt; NDArray[np.float64]:\n    \"\"\"Apply logarithmic warping.\n\n    Parameters\n    ----------\n    x : NDArray[np.float64]\n        Input values\n    params : Optional[List[float]], optional\n        Not used for logarithmic warp, by default None\n\n    Returns\n    -------\n    NDArray[np.float64]\n        log(x)\n    \"\"\"\n    return np.log(x)\n</code></pre> <code>invf(y: NDArray[np.float64], param: Optional[List[float]] = None) -&gt; NDArray[np.float64]</code> \u00b6 <p>Apply inverse logarithmic warping.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[float64]</code> <p>Input values</p> required <code>params</code> <code>Optional[List[float]]</code> <p>Not used for logarithmic warp, by default None</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>exp(y)</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def invf(\n    self, y: NDArray[np.float64], param: Optional[List[float]] = None\n) -&gt; NDArray[np.float64]:\n    \"\"\"Apply inverse logarithmic warping.\n\n    Parameters\n    ----------\n    y : NDArray[np.float64]\n        Input values\n    params : Optional[List[float]], optional\n        Not used for logarithmic warp, by default None\n\n    Returns\n    -------\n    NDArray[np.float64]\n        exp(y)\n    \"\"\"\n    return np.exp(y)\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.blr.warp.WarpSinArcsinh","title":"<code>WarpSinArcsinh</code>","text":"<p>               Bases: <code>WarpBase</code></p> <p>Sin-hyperbolic arcsin warping function.</p> <p>Implements warping function y = sinh(b * arcsinh(x) - a) with two parameters:     - a: controls skew     - b: controls kurtosis (constrained positive through exp transform)</p> <p>Properties:     - a = 0: symmetric     - a &gt; 0: positive skew     - a &lt; 0: negative skew     - b = 1: mesokurtic     - b &gt; 1: leptokurtic     - b &lt; 1: platykurtic</p> <p>Uses alternative parameterization from Jones and Pewsey (2019) where: y = sinh(b * arcsinh(x) + epsilon * b) and a = -epsilon*b</p> References <p>.. [1] Jones, M. C., &amp; Pewsey, A. (2019). Sigmoid-type distributions:        Generation and inference. Significance, 16(1), 12-15. .. [2] Jones, M. C., &amp; Pewsey, A. (2009). Sinh-arcsinh distributions.        Biometrika, 96(4), 761-780.</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>class WarpSinArcsinh(WarpBase):\n    \"\"\"Sin-hyperbolic arcsin warping function.\n\n    Implements warping function y = sinh(b * arcsinh(x) - a) with two parameters:\n        - a: controls skew\n        - b: controls kurtosis (constrained positive through exp transform)\n\n    Properties:\n        - a = 0: symmetric\n        - a &gt; 0: positive skew\n        - a &lt; 0: negative skew\n        - b = 1: mesokurtic\n        - b &gt; 1: leptokurtic\n        - b &lt; 1: platykurtic\n\n    Uses alternative parameterization from Jones and Pewsey (2019) where:\n    y = sinh(b * arcsinh(x) + epsilon * b) and a = -epsilon*b\n\n    References\n    ----------\n    .. [1] Jones, M. C., &amp; Pewsey, A. (2019). Sigmoid-type distributions:\n           Generation and inference. Significance, 16(1), 12-15.\n    .. [2] Jones, M. C., &amp; Pewsey, A. (2009). Sinh-arcsinh distributions.\n           Biometrika, 96(4), 761-780.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.n_params = 2\n\n    def _get_params(self, param: List[float]) -&gt; Tuple[float, float]:\n        \"\"\"Extract and transform the sinh-arcsinh parameters.\n\n        Parameters\n        ----------\n        param : List[float]\n            List containing [epsilon, log(b)]\n\n        Returns\n        -------\n        Tuple[float, float]\n            Tuple of (a, b) parameters where a = -epsilon*b\n\n        Raises\n        ------\n        ValueError\n            If param length doesn't match n_params\n        \"\"\"\n        if len(param) != self.n_params:\n            raise ValueError(\"number of parameters must be \" + str(self.n_params))\n\n        epsilon = param[0]\n        b = np.exp(param[1])\n        a = -epsilon * b\n\n        return a, b\n\n    def f(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n        \"\"\"Apply sinh-arcsinh warping.\n\n        Parameters\n        ----------\n        x : NDArray[np.float64]\n            Input values\n        params : List[float]\n            Parameters [epsilon, log(b)]\n\n        Returns\n        -------\n        NDArray[np.float64]\n            sinh(b * arcsinh(x) - a)\n        \"\"\"\n        a, b = self._get_params(param)\n        y = np.sinh(b * np.arcsinh(x) - a)\n        return y\n\n    def invf(self, y: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n        \"\"\"Apply inverse sinh-arcsinh warping.\n\n        Parameters\n        ----------\n        y : NDArray[np.float64]\n            Input values\n        params : List[float]\n            Parameters [epsilon, log(b)]\n\n        Returns\n        -------\n        NDArray[np.float64]\n            sinh((arcsinh(y) + a) / b)\n        \"\"\"\n        a, b = self._get_params(param)\n        x = np.sinh((np.arcsinh(y) + a) / b)\n        return x\n\n    def df(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n        \"\"\"Compute derivative of sinh-arcsinh warp.\n\n        Parameters\n        ----------\n        x : NDArray[np.float64]\n            Input values\n        params : List[float]\n            Parameters [epsilon, log(b)]\n\n        Returns\n        -------\n        NDArray[np.float64]\n            (b * cosh(b * arcsinh(x) - a)) / sqrt(1 + x^2)\n        \"\"\"\n        a, b = self._get_params(param)\n        dx = (b * np.cosh(b * np.arcsinh(x) - a)) / np.sqrt(1 + x**2)\n        return dx\n</code></pre> <code>df(x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]</code> \u00b6 <p>Compute derivative of sinh-arcsinh warp.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>NDArray[float64]</code> <p>Input values</p> required <code>params</code> <code>List[float]</code> <p>Parameters [epsilon, log(b)]</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>(b * cosh(b * arcsinh(x) - a)) / sqrt(1 + x^2)</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def df(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n    \"\"\"Compute derivative of sinh-arcsinh warp.\n\n    Parameters\n    ----------\n    x : NDArray[np.float64]\n        Input values\n    params : List[float]\n        Parameters [epsilon, log(b)]\n\n    Returns\n    -------\n    NDArray[np.float64]\n        (b * cosh(b * arcsinh(x) - a)) / sqrt(1 + x^2)\n    \"\"\"\n    a, b = self._get_params(param)\n    dx = (b * np.cosh(b * np.arcsinh(x) - a)) / np.sqrt(1 + x**2)\n    return dx\n</code></pre> <code>f(x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]</code> \u00b6 <p>Apply sinh-arcsinh warping.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>NDArray[float64]</code> <p>Input values</p> required <code>params</code> <code>List[float]</code> <p>Parameters [epsilon, log(b)]</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>sinh(b * arcsinh(x) - a)</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def f(self, x: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n    \"\"\"Apply sinh-arcsinh warping.\n\n    Parameters\n    ----------\n    x : NDArray[np.float64]\n        Input values\n    params : List[float]\n        Parameters [epsilon, log(b)]\n\n    Returns\n    -------\n    NDArray[np.float64]\n        sinh(b * arcsinh(x) - a)\n    \"\"\"\n    a, b = self._get_params(param)\n    y = np.sinh(b * np.arcsinh(x) - a)\n    return y\n</code></pre> <code>invf(y: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]</code> \u00b6 <p>Apply inverse sinh-arcsinh warping.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[float64]</code> <p>Input values</p> required <code>params</code> <code>List[float]</code> <p>Parameters [epsilon, log(b)]</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>sinh((arcsinh(y) + a) / b)</p> Source code in <code>pcntoolkit/regression_model/blr/warp.py</code> <pre><code>def invf(self, y: NDArray[np.float64], param: List[float]) -&gt; NDArray[np.float64]:\n    \"\"\"Apply inverse sinh-arcsinh warping.\n\n    Parameters\n    ----------\n    y : NDArray[np.float64]\n        Input values\n    params : List[float]\n        Parameters [epsilon, log(b)]\n\n    Returns\n    -------\n    NDArray[np.float64]\n        sinh((arcsinh(y) + a) / b)\n    \"\"\"\n    a, b = self._get_params(param)\n    x = np.sinh((np.arcsinh(y) + a) / b)\n    return x\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr","title":"<code>hbr</code>","text":""},{"location":"api/#pcntoolkit.regression_model.hbr.KnuOp","title":"<code>KnuOp</code>","text":""},{"location":"api/#pcntoolkit.regression_model.hbr.KnuOp.KnuOp","title":"<code>KnuOp</code>","text":"<p>               Bases: <code>BinaryScalarOp</code></p> <p>Modified Bessel function of the second kind, PyTensor wrapper for scipy.special.kv.</p> <p>This class implements a PyTensor operation for computing the modified Bessel function of the second kind (K_nu(x)). It wraps scipy.special.kv to provide automatic differentiation capabilities within PyTensor computational graphs.</p> <p>Parameters:</p> Name Type Description Default <code>dtype_converter</code> <code>callable</code> <p>Function to convert input types (typically upgrade_to_float)</p> required <code>name</code> <code>str</code> <p>Name of the operation</p> required Notes <p>The modified Bessel function K_nu(x) is a solution to the modified Bessel differential equation. This implementation supports automatic differentiation with respect to both the order (nu) and the argument (x).</p> See Also <p>scipy.special.kv : The underlying modified Bessel function implementation KnuPrimeOp : Derivative of the modified Bessel function</p> Source code in <code>pcntoolkit/regression_model/hbr/KnuOp.py</code> <pre><code>class KnuOp(BinaryScalarOp):\n    \"\"\"Modified Bessel function of the second kind, PyTensor wrapper for scipy.special.kv.\n\n    This class implements a PyTensor operation for computing the modified Bessel function\n    of the second kind (K_nu(x)). It wraps scipy.special.kv to provide automatic\n    differentiation capabilities within PyTensor computational graphs.\n\n    Parameters\n    ----------\n    dtype_converter : callable\n        Function to convert input types (typically upgrade_to_float)\n    name : str\n        Name of the operation\n\n    Notes\n    -----\n    The modified Bessel function K_nu(x) is a solution to the modified Bessel\n    differential equation. This implementation supports automatic differentiation\n    with respect to both the order (nu) and the argument (x).\n\n    See Also\n    --------\n    scipy.special.kv : The underlying modified Bessel function implementation\n    KnuPrimeOp : Derivative of the modified Bessel function\n    \"\"\"\n\n    nfunc_spec = (\"scipy.special.kv\", 2, 1)\n\n    @staticmethod\n    def st_impl(p: Union[float, int], x: Union[float, int]) -&gt; float:\n        \"\"\"Static implementation of the modified Bessel function.\n\n        Parameters\n        ----------\n        p : float or int\n            Order of the modified Bessel function\n        x : float or int\n            Argument where the function is evaluated\n\n        Returns\n        -------\n        float\n            Value of the modified Bessel function K_p(x)\n        \"\"\"\n        return spp.kv(p, x)\n\n    def impl(self, p: Union[float, int], x: Union[float, int]) -&gt; float:\n        \"\"\"Implementation of the modified Bessel function.\n\n        Parameters\n        ----------\n        p : float or int\n            Order of the modified Bessel function\n        x : float or int\n            Argument where the function is evaluated\n\n        Returns\n        -------\n        float\n            Value of the modified Bessel function K_p(x)\n        \"\"\"\n        return KnuOp.st_impl(p, x)\n\n    def grad(\n        self,\n        inputs: Sequence[Variable[Any, Any]],\n        output_gradients: Sequence[Variable[Any, Any]],\n    ) -&gt; List[Variable]:\n        \"\"\"Compute gradients of the modified Bessel function.\n\n        Parameters\n        ----------\n        inputs : list of Variables\n            List containing the order (p) and argument (x)\n        output_grads : list of Variables\n            List containing the gradient with respect to the output\n\n        Returns\n        -------\n        list of Variables\n            Gradients with respect to p and x\n\n        Notes\n        -----\n        The gradient with respect to p is computed using finite differences\n        due to the lack of a closed-form expression.\n        \"\"\"\n        dp = 1e-16\n        (p, x) = inputs\n        (gz,) = output_gradients\n        # Use finite differences for derivative with respect to p\n        dfdp = (knuop(p + dp, x) - knuop(p - dp, x)) / (2 * dp)  # type: ignore\n        return [gz * dfdp, gz * knupop(p, x)]  # type: ignore\n\n    def c_code(\n        self,\n        node: Apply[Any],\n        name: str,\n        inputs: Sequence[Any],\n        outputs: Sequence[Any],\n        sub: dict[str, str],\n    ) -&gt; Any:\n        raise NotImplementedError(\"C code generation not implemented for KnuOp\")\n</code></pre> <code>grad(inputs: Sequence[Variable[Any, Any]], output_gradients: Sequence[Variable[Any, Any]]) -&gt; List[Variable]</code> \u00b6 <p>Compute gradients of the modified Bessel function.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>list of Variables</code> <p>List containing the order (p) and argument (x)</p> required <code>output_grads</code> <code>list of Variables</code> <p>List containing the gradient with respect to the output</p> required <p>Returns:</p> Type Description <code>list of Variables</code> <p>Gradients with respect to p and x</p> Notes <p>The gradient with respect to p is computed using finite differences due to the lack of a closed-form expression.</p> Source code in <code>pcntoolkit/regression_model/hbr/KnuOp.py</code> <pre><code>def grad(\n    self,\n    inputs: Sequence[Variable[Any, Any]],\n    output_gradients: Sequence[Variable[Any, Any]],\n) -&gt; List[Variable]:\n    \"\"\"Compute gradients of the modified Bessel function.\n\n    Parameters\n    ----------\n    inputs : list of Variables\n        List containing the order (p) and argument (x)\n    output_grads : list of Variables\n        List containing the gradient with respect to the output\n\n    Returns\n    -------\n    list of Variables\n        Gradients with respect to p and x\n\n    Notes\n    -----\n    The gradient with respect to p is computed using finite differences\n    due to the lack of a closed-form expression.\n    \"\"\"\n    dp = 1e-16\n    (p, x) = inputs\n    (gz,) = output_gradients\n    # Use finite differences for derivative with respect to p\n    dfdp = (knuop(p + dp, x) - knuop(p - dp, x)) / (2 * dp)  # type: ignore\n    return [gz * dfdp, gz * knupop(p, x)]  # type: ignore\n</code></pre> <code>impl(p: Union[float, int], x: Union[float, int]) -&gt; float</code> \u00b6 <p>Implementation of the modified Bessel function.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float or int</code> <p>Order of the modified Bessel function</p> required <code>x</code> <code>float or int</code> <p>Argument where the function is evaluated</p> required <p>Returns:</p> Type Description <code>float</code> <p>Value of the modified Bessel function K_p(x)</p> Source code in <code>pcntoolkit/regression_model/hbr/KnuOp.py</code> <pre><code>def impl(self, p: Union[float, int], x: Union[float, int]) -&gt; float:\n    \"\"\"Implementation of the modified Bessel function.\n\n    Parameters\n    ----------\n    p : float or int\n        Order of the modified Bessel function\n    x : float or int\n        Argument where the function is evaluated\n\n    Returns\n    -------\n    float\n        Value of the modified Bessel function K_p(x)\n    \"\"\"\n    return KnuOp.st_impl(p, x)\n</code></pre> <code>st_impl(p: Union[float, int], x: Union[float, int]) -&gt; float</code> <code>staticmethod</code> \u00b6 <p>Static implementation of the modified Bessel function.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float or int</code> <p>Order of the modified Bessel function</p> required <code>x</code> <code>float or int</code> <p>Argument where the function is evaluated</p> required <p>Returns:</p> Type Description <code>float</code> <p>Value of the modified Bessel function K_p(x)</p> Source code in <code>pcntoolkit/regression_model/hbr/KnuOp.py</code> <pre><code>@staticmethod\ndef st_impl(p: Union[float, int], x: Union[float, int]) -&gt; float:\n    \"\"\"Static implementation of the modified Bessel function.\n\n    Parameters\n    ----------\n    p : float or int\n        Order of the modified Bessel function\n    x : float or int\n        Argument where the function is evaluated\n\n    Returns\n    -------\n    float\n        Value of the modified Bessel function K_p(x)\n    \"\"\"\n    return spp.kv(p, x)\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.KnuOp.KnuPrimeOp","title":"<code>KnuPrimeOp</code>","text":"<p>               Bases: <code>BinaryScalarOp</code></p> <p>Derivative of the modified Bessel function of the second kind.</p> <p>This class implements a PyTensor operation for computing the derivative of the modified Bessel function of the second kind with respect to its argument. It wraps scipy.special.kvp.</p> <p>Parameters:</p> Name Type Description Default <code>dtype_converter</code> <code>callable</code> <p>Function to convert input types (typically upgrade_to_float)</p> required <code>name</code> <code>str</code> <p>Name of the operation</p> required Source code in <code>pcntoolkit/regression_model/hbr/KnuOp.py</code> <pre><code>class KnuPrimeOp(BinaryScalarOp):\n    \"\"\"Derivative of the modified Bessel function of the second kind.\n\n    This class implements a PyTensor operation for computing the derivative of the\n    modified Bessel function of the second kind with respect to its argument.\n    It wraps scipy.special.kvp.\n\n    Parameters\n    ----------\n    dtype_converter : callable\n        Function to convert input types (typically upgrade_to_float)\n    name : str\n        Name of the operation\n    \"\"\"\n\n    nfunc_spec = (\"scipy.special.kvp\", 2, 1)\n\n    @staticmethod\n    def st_impl(p: Union[float, int], x: Union[float, int]) -&gt; float:\n        \"\"\"Static implementation of the Bessel function derivative.\n\n        Parameters\n        ----------\n        p : float or int\n            Order of the modified Bessel function\n        x : float or int\n            Argument where the derivative is evaluated\n\n        Returns\n        -------\n        float\n            Value of the derivative K'_p(x)\n        \"\"\"\n        return spp.kvp(p, x)\n\n    def impl(self, p: Union[float, int], x: Union[float, int]) -&gt; float:\n        \"\"\"Implementation of the Bessel function derivative.\n\n        Parameters\n        ----------\n        p : float or int\n            Order of the modified Bessel function\n        x : float or int\n            Argument where the derivative is evaluated\n\n        Returns\n        -------\n        float\n            Value of the derivative K'_p(x)\n        \"\"\"\n        return KnuPrimeOp.st_impl(p, x)\n\n    def grad(\n        self, inputs: Sequence[Variable[Any, Any]], grads: Sequence[Variable[Any, Any]]\n    ) -&gt; List[Variable]:\n        \"\"\"Compute gradients of the Bessel function derivative.\n\n        Parameters\n        ----------\n        inputs : list of Variables\n            List containing the order (p) and argument (x)\n        grads : list of Variables\n            List containing the gradient with respect to the output\n\n        Returns\n        -------\n        list of Variables\n            Gradients with respect to p and x (not implemented)\n        \"\"\"\n        return [grad_not_implemented(self, 0, \"p\"), grad_not_implemented(self, 1, \"x\")]\n\n    def c_code(\n        self,\n        node: Apply[Any],\n        name: str,\n        inputs: Sequence[Any],\n        outputs: Sequence[Any],\n        sub: dict[str, str],\n    ) -&gt; Any:\n        raise NotImplementedError(\"C code generation not implemented for KnupOp\")\n</code></pre> <code>grad(inputs: Sequence[Variable[Any, Any]], grads: Sequence[Variable[Any, Any]]) -&gt; List[Variable]</code> \u00b6 <p>Compute gradients of the Bessel function derivative.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>list of Variables</code> <p>List containing the order (p) and argument (x)</p> required <code>grads</code> <code>list of Variables</code> <p>List containing the gradient with respect to the output</p> required <p>Returns:</p> Type Description <code>list of Variables</code> <p>Gradients with respect to p and x (not implemented)</p> Source code in <code>pcntoolkit/regression_model/hbr/KnuOp.py</code> <pre><code>def grad(\n    self, inputs: Sequence[Variable[Any, Any]], grads: Sequence[Variable[Any, Any]]\n) -&gt; List[Variable]:\n    \"\"\"Compute gradients of the Bessel function derivative.\n\n    Parameters\n    ----------\n    inputs : list of Variables\n        List containing the order (p) and argument (x)\n    grads : list of Variables\n        List containing the gradient with respect to the output\n\n    Returns\n    -------\n    list of Variables\n        Gradients with respect to p and x (not implemented)\n    \"\"\"\n    return [grad_not_implemented(self, 0, \"p\"), grad_not_implemented(self, 1, \"x\")]\n</code></pre> <code>impl(p: Union[float, int], x: Union[float, int]) -&gt; float</code> \u00b6 <p>Implementation of the Bessel function derivative.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float or int</code> <p>Order of the modified Bessel function</p> required <code>x</code> <code>float or int</code> <p>Argument where the derivative is evaluated</p> required <p>Returns:</p> Type Description <code>float</code> <p>Value of the derivative K'_p(x)</p> Source code in <code>pcntoolkit/regression_model/hbr/KnuOp.py</code> <pre><code>def impl(self, p: Union[float, int], x: Union[float, int]) -&gt; float:\n    \"\"\"Implementation of the Bessel function derivative.\n\n    Parameters\n    ----------\n    p : float or int\n        Order of the modified Bessel function\n    x : float or int\n        Argument where the derivative is evaluated\n\n    Returns\n    -------\n    float\n        Value of the derivative K'_p(x)\n    \"\"\"\n    return KnuPrimeOp.st_impl(p, x)\n</code></pre> <code>st_impl(p: Union[float, int], x: Union[float, int]) -&gt; float</code> <code>staticmethod</code> \u00b6 <p>Static implementation of the Bessel function derivative.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float or int</code> <p>Order of the modified Bessel function</p> required <code>x</code> <code>float or int</code> <p>Argument where the derivative is evaluated</p> required <p>Returns:</p> Type Description <code>float</code> <p>Value of the derivative K'_p(x)</p> Source code in <code>pcntoolkit/regression_model/hbr/KnuOp.py</code> <pre><code>@staticmethod\ndef st_impl(p: Union[float, int], x: Union[float, int]) -&gt; float:\n    \"\"\"Static implementation of the Bessel function derivative.\n\n    Parameters\n    ----------\n    p : float or int\n        Order of the modified Bessel function\n    x : float or int\n        Argument where the derivative is evaluated\n\n    Returns\n    -------\n    float\n        Value of the derivative K'_p(x)\n    \"\"\"\n    return spp.kvp(p, x)\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.hbr","title":"<code>hbr</code>","text":"<p>Hierarchical Bayesian Regression model implementation.</p> <p>This class implements a Bayesian hierarchical regression model using PyMC for posterior sampling. It supports multiple likelihood functions and provides methods for model fitting, prediction, and analysis.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique identifier for the model instance</p> required <code>reg_conf</code> <code>HBRConf</code> <p>Configuration object containing model hyperparameters and structure</p> required <code>is_fitted</code> <code>bool</code> <p>Flag indicating if the model has been fitted, by default False</p> required <code>is_from_dict</code> <code>bool</code> <p>Flag indicating if model was created from dictionary, by default False</p> required <p>Attributes:</p> Name Type Description <code>idata</code> <code>InferenceData</code> <p>Contains the MCMC samples and model inference data</p> <code>pymc_model</code> <code>Model</code> <p>PyMC model object containing the computational graph</p> <code>reg_conf</code> <code>HBRConf</code> <p>Model configuration object</p> <code>is_fitted</code> <code>bool</code> <p>Indicates if model has been fitted</p> <code>name</code> <code>str</code> <p>Model identifier</p> <p>Functions:</p> Name Description <code>fit</code> <p>Fit the model to training data</p> <code>predict</code> <p>Generate predictions for new data</p> <code>centiles</code> <p>Calculate centile values for observations</p> <code>zscores</code> <p>Calculate z-scores for observations</p> <code>compile_model</code> <p>Create the PyMC model computational graph</p> <code>to_dict</code> <p>Serialize model to dictionary format</p> <code>from_dict</code> <p>Create model instance from serialized dictionary</p> <code>from_args</code> <p>Create model instance from command line arguments</p> Notes <p>The model supports Normal, SHASHb and SHASHo likelihood functions. The model structure is defined through the HBRConf configuration object which specifies the parameters (mu, sigma, epsilon, delta) and their hierarchical relationships.</p>"},{"location":"api/#pcntoolkit.regression_model.hbr.hbr.HBR","title":"<code>HBR</code>","text":"<p>               Bases: <code>RegressionModel</code></p> <p>Hierarchical Bayesian Regression model implementation.</p> <p>This class implements a Bayesian hierarchical regression model using PyMC for posterior sampling. It supports multiple likelihood functions and provides methods for model fitting, prediction, and analysis.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique identifier for the model instance</p> required <code>reg_conf</code> <code>HBRConf</code> <p>Configuration object containing model hyperparameters and structure</p> required <code>is_fitted</code> <code>bool</code> <p>Flag indicating if the model has been fitted, by default False</p> <code>False</code> <code>is_from_dict</code> <code>bool</code> <p>Flag indicating if model was created from dictionary, by default False</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>idata</code> <code>InferenceData</code> <p>Contains the MCMC samples and model inference data</p> <code>pymc_model</code> <code>Model</code> <p>PyMC model object containing the computational graph</p> <code>reg_conf</code> <code>HBRConf</code> <p>Model configuration object</p> <code>is_fitted</code> <code>bool</code> <p>Indicates if model has been fitted</p> <p>Methods:</p> Name Description <code>fit</code> <p>Fit the model to training data using MCMC sampling</p> <code>predict</code> <p>Generate predictions for new data</p> <code>fit_predict</code> <p>Fit model and generate predictions in one step</p> <code>transfer</code> <p>Perform transfer learning using existing model as prior</p> <code>centiles</code> <p>Calculate centile values for given cumulative densities</p> <code>zscores</code> <p>Calculate z-scores for observations</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>class HBR(RegressionModel):\n    \"\"\"\n    Hierarchical Bayesian Regression model implementation.\n\n    This class implements a Bayesian hierarchical regression model using PyMC for\n    posterior sampling. It supports multiple likelihood functions and provides\n    methods for model fitting, prediction, and analysis.\n\n    Parameters\n    ----------\n    name : str\n        Unique identifier for the model instance\n    reg_conf : HBRConf\n        Configuration object containing model hyperparameters and structure\n    is_fitted : bool, optional\n        Flag indicating if the model has been fitted, by default False\n    is_from_dict : bool, optional\n        Flag indicating if model was created from dictionary, by default False\n\n    Attributes\n    ----------\n    idata : arviz.InferenceData\n        Contains the MCMC samples and model inference data\n    pymc_model : pm.Model\n        PyMC model object containing the computational graph\n    reg_conf : HBRConf\n        Model configuration object\n    is_fitted : bool\n        Indicates if model has been fitted\n\n    Methods\n    -------\n    fit(hbrdata, make_new_model=True)\n        Fit the model to training data using MCMC sampling\n    predict(hbrdata)\n        Generate predictions for new data\n    fit_predict(fit_hbrdata, predict_hbrdata)\n        Fit model and generate predictions in one step\n    transfer(hbrconf, transferdata, freedom)\n        Perform transfer learning using existing model as prior\n    centiles(hbrdata, cdf, resample=True)\n        Calculate centile values for given cumulative densities\n    zscores(hbrdata, resample=False)\n        Calculate z-scores for observations\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        reg_conf: HBRConf,\n        is_fitted: bool = False,\n        is_from_dict: bool = False,\n    ):\n        \"\"\"\n        Initializes the model.\n        Any mutable parameters should be initialized here.\n        Any immutable parameters should be initialized in the configuration.\n        \"\"\"\n        super().__init__(name, reg_conf, is_fitted, is_from_dict)\n        self.idata: az.InferenceData = None  # type: ignore\n        self.pymc_model: pm.Model = None  # type: ignore\n\n    def fit(self, hbrdata: HBRData, make_new_model: bool = True) -&gt; None:\n        \"\"\"\n        Fit the model to training data using MCMC sampling.\n\n        Parameters\n        ----------\n        hbrdata : HBRData\n            Training data object containing features and targets\n        make_new_model : bool, optional\n            Whether to create a new PyMC model, by default True\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if make_new_model or (not self.pymc_model):\n            self.compile_model(hbrdata)\n        with self.pymc_model:\n            self.idata = pm.sample(\n                self.draws,\n                tune=self.tune,\n                cores=self.cores,\n                chains=self.chains,\n                nuts_sampler=self.nuts_sampler,  # type: ignore\n                init=self.init,\n            )\n        self.is_fitted = True\n\n    def predict(self, hbrdata: HBRData) -&gt; None:\n        \"\"\"\n        Generate predictions for new data.\n\n        Parameters\n        ----------\n        hbrdata : HBRData\n            Data object containing features to predict on\n\n        Returns\n        -------\n        None\n            Updates the model's inference data with predictions\n        \"\"\"\n        if not self.pymc_model:\n            self.compile_model(hbrdata)\n        hbrdata.set_data_in_existing_model(self.pymc_model)\n        if hasattr(self.idata, \"posterior_predictive\"):\n            del self.idata.posterior_predictive\n        with self.pymc_model:\n            pm.sample_posterior_predictive(\n                self.idata,\n                extend_inferencedata=True,\n                var_names=self.get_var_names() + [\"y_pred\"],\n            )\n\n    def fit_predict(self, fit_hbrdata: HBRData, predict_hbrdata: HBRData) -&gt; None:\n        \"\"\"\n        Fit model and generate predictions in one step.\n\n        Parameters\n        ----------\n        fit_hbrdata : HBRData\n            Training data for model fitting\n        predict_hbrdata : HBRData\n            Data to generate predictions for\n\n        Returns\n        -------\n        None\n            Updates model's inference data with fitted parameters and predictions\n        \"\"\"\n        if not self.pymc_model:\n            self.compile_model(fit_hbrdata)\n        with self.pymc_model:\n            self.idata = pm.sample(\n                self.draws,\n                tune=self.tune,\n                cores=self.cores,\n                chains=self.chains,\n                nuts_sampler=self.nuts_sampler,  # type: ignore\n                init=self.init,\n            )\n        self.is_fitted = True\n        predict_hbrdata.set_data_in_existing_model(self.pymc_model)\n        with self.pymc_model:\n            pm.sample_posterior_predictive(\n                self.idata,\n                extend_inferencedata=True,\n                var_names=self.get_var_names() + [\"y_pred\"],\n            )\n\n    def transfer(self, hbrconf: HBRConf, transferdata: HBRData, freedom: float) -&gt; HBR:\n        \"\"\"\n        Perform transfer learning using existing model as prior.\n\n        Parameters\n        ----------\n        hbrconf : HBRConf\n            Configuration for new model\n        transferdata : HBRData\n            Data for transfer learning\n        freedom : float\n            Parameter controlling influence of prior model (0-1)\n\n        Returns\n        -------\n        HBR\n            New model instance with transferred knowledge\n        \"\"\"\n        new_hbr_model = HBR(self.name, hbrconf)\n        new_hbr_model.compile_model(transferdata, self.idata, freedom)\n        with new_hbr_model.pymc_model:\n            new_hbr_model.idata = pm.sample(\n                hbrconf.draws,\n                tune=hbrconf.tune,\n                cores=hbrconf.cores,\n                chains=hbrconf.chains,\n                nuts_sampler=hbrconf.nuts_sampler,  # type: ignore\n            )\n            new_hbr_model.is_fitted = True\n\n        return new_hbr_model\n\n    def centiles(\n        self, hbrdata: HBRData, cdf: np.ndarray, resample: bool = True\n    ) -&gt; xr.DataArray:\n        \"\"\"\n        Calculate centile values for given cumulative densities.\n\n        Parameters\n        ----------\n        hbrdata : HBRData\n            Data to calculate centiles for\n        cdf : np.ndarray\n            Array of cumulative density values\n        resample : bool, optional\n            Whether to generate new posterior samples, by default True\n\n        Returns\n        -------\n        xr.DataArray\n            Calculated centile values\n        \"\"\"\n        var_names = self.get_var_names()\n\n        if resample:\n            self.predict(hbrdata)\n        post_pred = az.extract(\n            self.idata,\n            \"posterior_predictive\",\n            var_names=var_names,\n        )\n        array_of_vars = [self.likelihood] + list(\n            map(lambda x: np.squeeze(post_pred[x]), var_names)\n        )\n        n_datapoints, n_mcmc_samples = post_pred[\"mu_samples\"].shape\n        centiles = np.zeros((cdf.shape[0], n_datapoints, n_mcmc_samples))\n        for i, _cdf in enumerate(cdf):\n            zs = np.full(\n                (n_datapoints, n_mcmc_samples), stats.norm.ppf(_cdf), dtype=float\n            )\n            centiles[i] = xr.apply_ufunc(\n                centile,\n                *array_of_vars,\n                kwargs={\"zs\": zs},\n            )\n        return xr.DataArray(\n            centiles,\n            dims=[\"cdf\", \"datapoints\", \"sample\"],\n            coords={\"cdf\": cdf},\n        ).mean(dim=\"sample\")\n\n    def zscores(self, hbrdata: HBRData, resample: bool = False) -&gt; xr.DataArray:\n        \"\"\"\n        Calculate z-scores for observations.\n\n        Parameters\n        ----------\n        hbrdata : HBRData\n            Data containing observations to calculate z-scores for\n        resample : bool, optional\n            Whether to generate new posterior samples, by default False\n\n        Returns\n        -------\n        xr.DataArray\n            Calculated z-scores\n        \"\"\"\n        var_names = self.get_var_names()\n        if resample:\n            if self.pymc_model is None:\n                self.compile_model(hbrdata)\n            hbrdata.set_data_in_existing_model(self.pymc_model)\n            if hasattr(self.idata, \"posterior_predictive\"):\n                del self.idata.posterior_predictive\n            with self.pymc_model:  # type: ignore\n                pm.sample_posterior_predictive(\n                    self.idata,\n                    extend_inferencedata=True,\n                    var_names=var_names + [\"y_pred\"],\n                )\n        post_pred = az.extract(\n            self.idata,\n            \"posterior_predictive\",\n            var_names=var_names,\n        )\n        array_of_vars = [self.likelihood] + list(\n            map(lambda x: np.squeeze(post_pred[x]), var_names)\n        )\n\n        zscores = xr.apply_ufunc(\n            zscore,\n            *array_of_vars,\n            kwargs={\"y\": hbrdata.y[:, None]},\n        ).mean(dim=\"sample\")\n\n        return zscores\n\n    def get_var_names(self) -&gt; List[str]:\n        \"\"\"Get the variable names for the current likelihood function.\n\n        Returns\n        -------\n        List[str]\n            List of variable names required for the current likelihood function.\n            For Normal likelihood: ['mu_samples', 'sigma_samples']\n            For SHASH likelihoods: ['mu_samples', 'sigma_samples', 'epsilon_samples', 'delta_samples']\n\n        Raises\n        ------\n        RuntimeError\n            If likelihood is not supported (must be 'Normal', 'SHASHb', or 'SHASHo')\n        \"\"\"\n        likelihood = self.likelihood\n        if likelihood == \"Normal\":\n            var_names = [\"mu_samples\", \"sigma_samples\"]\n        elif likelihood.startswith(\"SHASH\"):\n            var_names = [\n                \"mu_samples\",\n                \"sigma_samples\",\n                \"epsilon_samples\",\n                \"delta_samples\",\n            ]\n        else:\n            raise RuntimeError(\"Unsupported likelihood \" + likelihood)\n        return var_names\n\n    def compile_model(\n        self,\n        data: HBRData,\n        idata: Optional[az.InferenceData] = None,\n        freedom: float = 1,\n    ) -&gt; None:\n        \"\"\"\n        Create the PyMC model computational graph.\n\n        Parameters\n        ----------\n        data : HBRData\n            Data object containing model inputs\n        idata : Optional[az.InferenceData], optional\n            Previous inference data for transfer learning, by default None\n        freedom : float, optional\n            Parameter controlling influence of prior model, by default 1\n\n        Returns\n        -------\n        None\n            Creates and stores PyMC model in instance\n        \"\"\"\n        self.pymc_model = pm.Model(coords=data.coords)\n        data.add_to_graph(self.pymc_model)\n        if self.likelihood == \"Normal\":\n            self.compile_normal(data, idata, freedom)\n        elif self.likelihood == \"SHASHb\":\n            self.compile_SHASHb(data, idata, freedom)\n        elif self.likelihood == \"SHASHo\":\n            self.compile_SHASHo(data, idata, freedom)\n        else:\n            raise NotImplementedError(\n                f\"Likelihood {self.likelihood} not implemented for {self.__class__.__name__}\"\n            )\n\n    def compile_normal(\n        self,\n        data: HBRData,\n        idata: Optional[az.InferenceData] = None,\n        freedom: float = 1,\n    ) -&gt; None:\n        \"\"\"\n        Create PyMC model with Normal likelihood.\n\n        Parameters\n        ----------\n        data : HBRData\n            Data object containing model inputs\n        idata : Optional[az.InferenceData], optional\n            Previous inference data for transfer learning, by default None\n        freedom : float, optional\n            Parameter controlling influence of prior model (0-1), by default 1\n\n        Returns\n        -------\n        None\n            Updates the PyMC model in place with Normal likelihood components\n        \"\"\"\n        self.mu.create_graph(self.pymc_model, idata, freedom)\n        self.sigma.create_graph(self.pymc_model, idata, freedom)\n        with self.pymc_model:\n            mu_samples = pm.Deterministic(\n                \"mu_samples\",\n                self.mu.get_samples(data),\n                dims=self.mu.sample_dims,\n            )\n            sigma_samples = pm.Deterministic(\n                \"sigma_samples\",\n                self.sigma.get_samples(data),\n                dims=self.sigma.sample_dims,\n            )\n            pm.Normal(\n                \"y_pred\",\n                mu=mu_samples,\n                sigma=sigma_samples,\n                observed=data.pm_y,\n                dims=(\"datapoints\",),\n            )\n\n    def compile_SHASHb(\n        self,\n        data: HBRData,\n        idata: Optional[az.InferenceData] = None,\n        freedom: float = 1,\n    ) -&gt; None:\n        \"\"\"\n        Create PyMC model with SHASHb likelihood.\n\n        Parameters\n        ----------\n        data : HBRData\n            Data object containing model inputs\n        idata : Optional[az.InferenceData], optional\n            Previous inference data for transfer learning, by default None\n        freedom : float, optional\n            Parameter controlling influence of prior model (0-1), by default 1\n\n        Returns\n        -------\n        None\n            Updates the PyMC model in place with SHASHb likelihood components\n        \"\"\"\n        self.mu.create_graph(self.pymc_model, idata, freedom)\n        self.sigma.create_graph(self.pymc_model, idata, freedom)\n        self.epsilon.create_graph(self.pymc_model, idata, freedom)\n        self.delta.create_graph(self.pymc_model, idata, freedom)\n        with self.pymc_model:\n            mu_samples = pm.Deterministic(\n                \"mu_samples\",\n                self.mu.get_samples(data),\n                dims=self.mu.sample_dims,\n            )\n            sigma_samples = pm.Deterministic(\n                \"sigma_samples\",\n                self.sigma.get_samples(data),\n                dims=self.sigma.sample_dims,\n            )\n            epsilon_samples = pm.Deterministic(\n                \"epsilon_samples\",\n                self.epsilon.get_samples(data),\n                dims=self.epsilon.sample_dims,\n            )\n            delta_samples = pm.Deterministic(\n                \"delta_samples\",\n                self.delta.get_samples(data),\n                dims=self.delta.sample_dims,\n            )\n            SHASHb(\n                \"y_pred\",\n                mu=mu_samples,\n                sigma=sigma_samples,\n                epsilon=epsilon_samples,\n                delta=delta_samples,\n                observed=data.pm_y,\n                dims=(\"datapoints\",),\n            )\n\n    def compile_SHASHo(\n        self,\n        data: HBRData,\n        idata: Optional[az.InferenceData] = None,\n        freedom: float = 1,\n    ) -&gt; None:\n        \"\"\"\n        Create PyMC model with SHASHo likelihood.\n\n        Parameters\n        ----------\n        data : HBRData\n            Data object containing model inputs\n        idata : Optional[az.InferenceData], optional\n            Previous inference data for transfer learning, by default None\n        freedom : float, optional\n            Parameter controlling influence of prior model (0-1), by default 1\n\n        Returns\n        -------\n        None\n            Updates the PyMC model in place with SHASHo likelihood components\n        \"\"\"\n        self.mu.create_graph(self.pymc_model, idata, freedom)\n        self.sigma.create_graph(self.pymc_model, idata, freedom)\n        self.epsilon.create_graph(self.pymc_model, idata, freedom)\n        self.delta.create_graph(self.pymc_model, idata, freedom)\n        with self.pymc_model:\n            mu_samples = pm.Deterministic(\n                \"mu_samples\",\n                self.mu.get_samples(data),\n                dims=self.mu.sample_dims,\n            )\n            sigma_samples = pm.Deterministic(\n                \"sigma_samples\",\n                self.sigma.get_samples(data),\n                dims=self.sigma.sample_dims,\n            )\n            epsilon_samples = pm.Deterministic(\n                \"epsilon_samples\",\n                self.epsilon.get_samples(data),\n                dims=self.epsilon.sample_dims,\n            )\n            delta_samples = pm.Deterministic(\n                \"delta_samples\",\n                self.delta.get_samples(data),\n                dims=self.delta.sample_dims,\n            )\n            SHASHo(\n                \"y_pred\",\n                mu=mu_samples,\n                sigma=sigma_samples,\n                epsilon=epsilon_samples,\n                delta=delta_samples,\n                observed=data.pm_y,\n                dims=(\"datapoints\",),\n            )\n\n    def to_dict(self, path: Optional[str] = None) -&gt; Dict[str, Any]:\n        \"\"\"\n        Serialize model to dictionary format.\n\n        Parameters\n        ----------\n        path : Optional[str], optional\n            Path to save inference data, by default None\n\n        Returns\n        -------\n        Dict[str, Any]\n            Dictionary containing serialized model\n        \"\"\"\n        my_dict = super().to_dict()\n        if self.is_fitted and (path is not None):\n            idata_path = os.path.join(path, f\"idata_{self.name}.nc\")\n            self.save_idata(idata_path)\n        return my_dict\n\n    @classmethod\n    def from_dict(cls, my_dict: Dict[str, Any], path: Optional[str] = None) -&gt; \"HBR\":\n        \"\"\"\n        Create model instance from serialized dictionary.\n\n        Parameters\n        ----------\n        dict : Dict[str, Any]\n            Dictionary containing serialized model\n        path : Optional[str], optional\n            Path to load inference data from, by default None\n\n        Returns\n        -------\n        HBR\n            New model instance\n        \"\"\"\n        name = my_dict[\"name\"]\n        conf = HBRConf.from_dict(my_dict[\"reg_conf\"])\n        is_fitted = my_dict[\"is_fitted\"]\n        is_from_dict = True\n        self = cls(name, conf, is_fitted, is_from_dict)\n        if is_fitted and (path is not None):\n            idata_path = os.path.join(path, f\"idata_{name}.nc\")\n            self.load_idata(idata_path)\n        return self\n\n    @classmethod\n    def from_args(cls, name: str, args: Dict[str, Any]) -&gt; \"HBR\":\n        \"\"\"\n        Create model instance from command line arguments.\n\n        Parameters\n        ----------\n        name : str\n            Name for new model instance\n        args : Dict[str, Any]\n            Dictionary of command line arguments\n\n        Returns\n        -------\n        HBR\n            New model instance\n        \"\"\"\n        conf = HBRConf.from_args(args)\n        is_fitted = args.get(\"is_fitted\", False)\n        is_from_dict = True\n        self = cls(name, conf, is_fitted, is_from_dict)\n        return self\n\n    def save_idata(self, path: str) -&gt; None:\n        \"\"\"\n        Save inference data to NetCDF file.\n\n        Parameters\n        ----------\n        path : str\n            Path to save inference data to. Should end in '.nc'\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        RuntimeError\n            If model is fitted but does not have inference data\n        \"\"\"\n        if self.is_fitted:\n            if hasattr(self, \"idata\"):\n                self.remove_samples_from_idata_posterior()\n                self.idata.to_netcdf(path, groups=[\"posterior\"])\n            else:\n                raise RuntimeError(\n                    \"HBR model is fitted but does not have idata. This should not happen.\"\n                )\n\n    def load_idata(self, path: str) -&gt; None:\n        \"\"\"\n        Load inference data from NetCDF file.\n\n        Parameters\n        ----------\n        path : str\n            Path to load inference data from. Should end in '.nc'\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        RuntimeError\n            If model is fitted but inference data cannot be loaded from path\n        \"\"\"\n        if self.is_fitted:\n            try:\n                self.idata = az.from_netcdf(path)\n            except Exception as exc:\n                raise RuntimeError(f\"Could not load idata from {path}.\") from exc\n            self.replace_samples_in_idata_posterior()\n\n    def remove_samples_from_idata_posterior(self) -&gt; None:\n        \"\"\"\n        Remove sample variables from the posterior group of inference data.\n\n        This method removes variables ending with '_samples' from the posterior group\n        before saving to avoid privacy issues. The variables can be recomputed from the\n        model parameters. The names of removed variables are stored in idata.attrs['removed_samples'].\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n\n        Notes\n        -----\n        This is used internally before saving the inference data to disk to reduce\n        storage size, since sample variables can be recomputed from the model parameters.\n        \"\"\"\n        post: xr.Dataset = self.idata.posterior  # type: ignore\n        for name in post.variables.mapping.keys():\n            if str(name).endswith(\"_samples\"):\n                post.drop_vars(str(name))\n                if \"removed_samples\" not in self.idata.attrs:\n                    self.idata.attrs[\"removed_samples\"] = []\n                self.idata.attrs[\"removed_samples\"].append(name)\n\n    def replace_samples_in_idata_posterior(self) -&gt; None:\n        \"\"\"\n        Replace previously removed sample variables in the posterior group.\n\n        This method adds back placeholder arrays for variables that were removed by\n        remove_samples_from_idata_posterior(). The arrays are initialized with zeros\n        and will be populated when the model is used.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n\n        Notes\n        -----\n        This is used internally after loading inference data from disk to restore\n        the structure needed for model predictions. The actual values will be\n        recomputed when needed.\n        \"\"\"\n        post: xr.Dataset = self.idata.posterior  # type: ignore\n        for name in self.idata.attrs[\"removed_samples\"]:\n            post[name] = xr.DataArray(\n                np.zeros(post[name].shape),\n                dims=post[name].dims,\n            )\n\n    # pylint: disable=C0116\n\n    @property\n    def mu(self) -&gt; Param:\n        return self.reg_conf.mu  # type: ignore\n\n    @property\n    def sigma(self) -&gt; Param:\n        return self.reg_conf.sigma  # type: ignore\n\n    @property\n    def epsilon(self) -&gt; Param:\n        return self.reg_conf.epsilon  # type: ignore\n\n    @property\n    def delta(self) -&gt; Param:\n        return self.reg_conf.delta  # type: ignore\n\n    @property\n    def likelihood(self) -&gt; str:\n        return self.reg_conf.likelihood  # type: ignore\n\n    @property\n    def draws(self) -&gt; int:\n        return self.reg_conf.draws  # type: ignore\n\n    @property\n    def tune(self) -&gt; int:\n        return self.reg_conf.tune  # type: ignore\n\n    @property\n    def cores(self) -&gt; int:\n        return self.reg_conf.cores  # type: ignore\n\n    @property\n    def chains(self) -&gt; int:\n        return self.reg_conf.chains  # type: ignore\n\n    @property\n    def nuts_sampler(self) -&gt; str:\n        return self.reg_conf.nuts_sampler  # type: ignore\n\n    @property\n    def init(self) -&gt; str:\n        return self.reg_conf.init  # type: ignore\n</code></pre> <code>__init__(name: str, reg_conf: HBRConf, is_fitted: bool = False, is_from_dict: bool = False)</code> \u00b6 <p>Initializes the model. Any mutable parameters should be initialized here. Any immutable parameters should be initialized in the configuration.</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    reg_conf: HBRConf,\n    is_fitted: bool = False,\n    is_from_dict: bool = False,\n):\n    \"\"\"\n    Initializes the model.\n    Any mutable parameters should be initialized here.\n    Any immutable parameters should be initialized in the configuration.\n    \"\"\"\n    super().__init__(name, reg_conf, is_fitted, is_from_dict)\n    self.idata: az.InferenceData = None  # type: ignore\n    self.pymc_model: pm.Model = None  # type: ignore\n</code></pre> <code>centiles(hbrdata: HBRData, cdf: np.ndarray, resample: bool = True) -&gt; xr.DataArray</code> \u00b6 <p>Calculate centile values for given cumulative densities.</p> <p>Parameters:</p> Name Type Description Default <code>hbrdata</code> <code>HBRData</code> <p>Data to calculate centiles for</p> required <code>cdf</code> <code>ndarray</code> <p>Array of cumulative density values</p> required <code>resample</code> <code>bool</code> <p>Whether to generate new posterior samples, by default True</p> <code>True</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>Calculated centile values</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>def centiles(\n    self, hbrdata: HBRData, cdf: np.ndarray, resample: bool = True\n) -&gt; xr.DataArray:\n    \"\"\"\n    Calculate centile values for given cumulative densities.\n\n    Parameters\n    ----------\n    hbrdata : HBRData\n        Data to calculate centiles for\n    cdf : np.ndarray\n        Array of cumulative density values\n    resample : bool, optional\n        Whether to generate new posterior samples, by default True\n\n    Returns\n    -------\n    xr.DataArray\n        Calculated centile values\n    \"\"\"\n    var_names = self.get_var_names()\n\n    if resample:\n        self.predict(hbrdata)\n    post_pred = az.extract(\n        self.idata,\n        \"posterior_predictive\",\n        var_names=var_names,\n    )\n    array_of_vars = [self.likelihood] + list(\n        map(lambda x: np.squeeze(post_pred[x]), var_names)\n    )\n    n_datapoints, n_mcmc_samples = post_pred[\"mu_samples\"].shape\n    centiles = np.zeros((cdf.shape[0], n_datapoints, n_mcmc_samples))\n    for i, _cdf in enumerate(cdf):\n        zs = np.full(\n            (n_datapoints, n_mcmc_samples), stats.norm.ppf(_cdf), dtype=float\n        )\n        centiles[i] = xr.apply_ufunc(\n            centile,\n            *array_of_vars,\n            kwargs={\"zs\": zs},\n        )\n    return xr.DataArray(\n        centiles,\n        dims=[\"cdf\", \"datapoints\", \"sample\"],\n        coords={\"cdf\": cdf},\n    ).mean(dim=\"sample\")\n</code></pre> <code>compile_SHASHb(data: HBRData, idata: Optional[az.InferenceData] = None, freedom: float = 1) -&gt; None</code> \u00b6 <p>Create PyMC model with SHASHb likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>HBRData</code> <p>Data object containing model inputs</p> required <code>idata</code> <code>Optional[InferenceData]</code> <p>Previous inference data for transfer learning, by default None</p> <code>None</code> <code>freedom</code> <code>float</code> <p>Parameter controlling influence of prior model (0-1), by default 1</p> <code>1</code> <p>Returns:</p> Type Description <code>None</code> <p>Updates the PyMC model in place with SHASHb likelihood components</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>def compile_SHASHb(\n    self,\n    data: HBRData,\n    idata: Optional[az.InferenceData] = None,\n    freedom: float = 1,\n) -&gt; None:\n    \"\"\"\n    Create PyMC model with SHASHb likelihood.\n\n    Parameters\n    ----------\n    data : HBRData\n        Data object containing model inputs\n    idata : Optional[az.InferenceData], optional\n        Previous inference data for transfer learning, by default None\n    freedom : float, optional\n        Parameter controlling influence of prior model (0-1), by default 1\n\n    Returns\n    -------\n    None\n        Updates the PyMC model in place with SHASHb likelihood components\n    \"\"\"\n    self.mu.create_graph(self.pymc_model, idata, freedom)\n    self.sigma.create_graph(self.pymc_model, idata, freedom)\n    self.epsilon.create_graph(self.pymc_model, idata, freedom)\n    self.delta.create_graph(self.pymc_model, idata, freedom)\n    with self.pymc_model:\n        mu_samples = pm.Deterministic(\n            \"mu_samples\",\n            self.mu.get_samples(data),\n            dims=self.mu.sample_dims,\n        )\n        sigma_samples = pm.Deterministic(\n            \"sigma_samples\",\n            self.sigma.get_samples(data),\n            dims=self.sigma.sample_dims,\n        )\n        epsilon_samples = pm.Deterministic(\n            \"epsilon_samples\",\n            self.epsilon.get_samples(data),\n            dims=self.epsilon.sample_dims,\n        )\n        delta_samples = pm.Deterministic(\n            \"delta_samples\",\n            self.delta.get_samples(data),\n            dims=self.delta.sample_dims,\n        )\n        SHASHb(\n            \"y_pred\",\n            mu=mu_samples,\n            sigma=sigma_samples,\n            epsilon=epsilon_samples,\n            delta=delta_samples,\n            observed=data.pm_y,\n            dims=(\"datapoints\",),\n        )\n</code></pre> <code>compile_SHASHo(data: HBRData, idata: Optional[az.InferenceData] = None, freedom: float = 1) -&gt; None</code> \u00b6 <p>Create PyMC model with SHASHo likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>HBRData</code> <p>Data object containing model inputs</p> required <code>idata</code> <code>Optional[InferenceData]</code> <p>Previous inference data for transfer learning, by default None</p> <code>None</code> <code>freedom</code> <code>float</code> <p>Parameter controlling influence of prior model (0-1), by default 1</p> <code>1</code> <p>Returns:</p> Type Description <code>None</code> <p>Updates the PyMC model in place with SHASHo likelihood components</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>def compile_SHASHo(\n    self,\n    data: HBRData,\n    idata: Optional[az.InferenceData] = None,\n    freedom: float = 1,\n) -&gt; None:\n    \"\"\"\n    Create PyMC model with SHASHo likelihood.\n\n    Parameters\n    ----------\n    data : HBRData\n        Data object containing model inputs\n    idata : Optional[az.InferenceData], optional\n        Previous inference data for transfer learning, by default None\n    freedom : float, optional\n        Parameter controlling influence of prior model (0-1), by default 1\n\n    Returns\n    -------\n    None\n        Updates the PyMC model in place with SHASHo likelihood components\n    \"\"\"\n    self.mu.create_graph(self.pymc_model, idata, freedom)\n    self.sigma.create_graph(self.pymc_model, idata, freedom)\n    self.epsilon.create_graph(self.pymc_model, idata, freedom)\n    self.delta.create_graph(self.pymc_model, idata, freedom)\n    with self.pymc_model:\n        mu_samples = pm.Deterministic(\n            \"mu_samples\",\n            self.mu.get_samples(data),\n            dims=self.mu.sample_dims,\n        )\n        sigma_samples = pm.Deterministic(\n            \"sigma_samples\",\n            self.sigma.get_samples(data),\n            dims=self.sigma.sample_dims,\n        )\n        epsilon_samples = pm.Deterministic(\n            \"epsilon_samples\",\n            self.epsilon.get_samples(data),\n            dims=self.epsilon.sample_dims,\n        )\n        delta_samples = pm.Deterministic(\n            \"delta_samples\",\n            self.delta.get_samples(data),\n            dims=self.delta.sample_dims,\n        )\n        SHASHo(\n            \"y_pred\",\n            mu=mu_samples,\n            sigma=sigma_samples,\n            epsilon=epsilon_samples,\n            delta=delta_samples,\n            observed=data.pm_y,\n            dims=(\"datapoints\",),\n        )\n</code></pre> <code>compile_model(data: HBRData, idata: Optional[az.InferenceData] = None, freedom: float = 1) -&gt; None</code> \u00b6 <p>Create the PyMC model computational graph.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>HBRData</code> <p>Data object containing model inputs</p> required <code>idata</code> <code>Optional[InferenceData]</code> <p>Previous inference data for transfer learning, by default None</p> <code>None</code> <code>freedom</code> <code>float</code> <p>Parameter controlling influence of prior model, by default 1</p> <code>1</code> <p>Returns:</p> Type Description <code>None</code> <p>Creates and stores PyMC model in instance</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>def compile_model(\n    self,\n    data: HBRData,\n    idata: Optional[az.InferenceData] = None,\n    freedom: float = 1,\n) -&gt; None:\n    \"\"\"\n    Create the PyMC model computational graph.\n\n    Parameters\n    ----------\n    data : HBRData\n        Data object containing model inputs\n    idata : Optional[az.InferenceData], optional\n        Previous inference data for transfer learning, by default None\n    freedom : float, optional\n        Parameter controlling influence of prior model, by default 1\n\n    Returns\n    -------\n    None\n        Creates and stores PyMC model in instance\n    \"\"\"\n    self.pymc_model = pm.Model(coords=data.coords)\n    data.add_to_graph(self.pymc_model)\n    if self.likelihood == \"Normal\":\n        self.compile_normal(data, idata, freedom)\n    elif self.likelihood == \"SHASHb\":\n        self.compile_SHASHb(data, idata, freedom)\n    elif self.likelihood == \"SHASHo\":\n        self.compile_SHASHo(data, idata, freedom)\n    else:\n        raise NotImplementedError(\n            f\"Likelihood {self.likelihood} not implemented for {self.__class__.__name__}\"\n        )\n</code></pre> <code>compile_normal(data: HBRData, idata: Optional[az.InferenceData] = None, freedom: float = 1) -&gt; None</code> \u00b6 <p>Create PyMC model with Normal likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>HBRData</code> <p>Data object containing model inputs</p> required <code>idata</code> <code>Optional[InferenceData]</code> <p>Previous inference data for transfer learning, by default None</p> <code>None</code> <code>freedom</code> <code>float</code> <p>Parameter controlling influence of prior model (0-1), by default 1</p> <code>1</code> <p>Returns:</p> Type Description <code>None</code> <p>Updates the PyMC model in place with Normal likelihood components</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>def compile_normal(\n    self,\n    data: HBRData,\n    idata: Optional[az.InferenceData] = None,\n    freedom: float = 1,\n) -&gt; None:\n    \"\"\"\n    Create PyMC model with Normal likelihood.\n\n    Parameters\n    ----------\n    data : HBRData\n        Data object containing model inputs\n    idata : Optional[az.InferenceData], optional\n        Previous inference data for transfer learning, by default None\n    freedom : float, optional\n        Parameter controlling influence of prior model (0-1), by default 1\n\n    Returns\n    -------\n    None\n        Updates the PyMC model in place with Normal likelihood components\n    \"\"\"\n    self.mu.create_graph(self.pymc_model, idata, freedom)\n    self.sigma.create_graph(self.pymc_model, idata, freedom)\n    with self.pymc_model:\n        mu_samples = pm.Deterministic(\n            \"mu_samples\",\n            self.mu.get_samples(data),\n            dims=self.mu.sample_dims,\n        )\n        sigma_samples = pm.Deterministic(\n            \"sigma_samples\",\n            self.sigma.get_samples(data),\n            dims=self.sigma.sample_dims,\n        )\n        pm.Normal(\n            \"y_pred\",\n            mu=mu_samples,\n            sigma=sigma_samples,\n            observed=data.pm_y,\n            dims=(\"datapoints\",),\n        )\n</code></pre> <code>fit(hbrdata: HBRData, make_new_model: bool = True) -&gt; None</code> \u00b6 <p>Fit the model to training data using MCMC sampling.</p> <p>Parameters:</p> Name Type Description Default <code>hbrdata</code> <code>HBRData</code> <p>Training data object containing features and targets</p> required <code>make_new_model</code> <code>bool</code> <p>Whether to create a new PyMC model, by default True</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>def fit(self, hbrdata: HBRData, make_new_model: bool = True) -&gt; None:\n    \"\"\"\n    Fit the model to training data using MCMC sampling.\n\n    Parameters\n    ----------\n    hbrdata : HBRData\n        Training data object containing features and targets\n    make_new_model : bool, optional\n        Whether to create a new PyMC model, by default True\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if make_new_model or (not self.pymc_model):\n        self.compile_model(hbrdata)\n    with self.pymc_model:\n        self.idata = pm.sample(\n            self.draws,\n            tune=self.tune,\n            cores=self.cores,\n            chains=self.chains,\n            nuts_sampler=self.nuts_sampler,  # type: ignore\n            init=self.init,\n        )\n    self.is_fitted = True\n</code></pre> <code>fit_predict(fit_hbrdata: HBRData, predict_hbrdata: HBRData) -&gt; None</code> \u00b6 <p>Fit model and generate predictions in one step.</p> <p>Parameters:</p> Name Type Description Default <code>fit_hbrdata</code> <code>HBRData</code> <p>Training data for model fitting</p> required <code>predict_hbrdata</code> <code>HBRData</code> <p>Data to generate predictions for</p> required <p>Returns:</p> Type Description <code>None</code> <p>Updates model's inference data with fitted parameters and predictions</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>def fit_predict(self, fit_hbrdata: HBRData, predict_hbrdata: HBRData) -&gt; None:\n    \"\"\"\n    Fit model and generate predictions in one step.\n\n    Parameters\n    ----------\n    fit_hbrdata : HBRData\n        Training data for model fitting\n    predict_hbrdata : HBRData\n        Data to generate predictions for\n\n    Returns\n    -------\n    None\n        Updates model's inference data with fitted parameters and predictions\n    \"\"\"\n    if not self.pymc_model:\n        self.compile_model(fit_hbrdata)\n    with self.pymc_model:\n        self.idata = pm.sample(\n            self.draws,\n            tune=self.tune,\n            cores=self.cores,\n            chains=self.chains,\n            nuts_sampler=self.nuts_sampler,  # type: ignore\n            init=self.init,\n        )\n    self.is_fitted = True\n    predict_hbrdata.set_data_in_existing_model(self.pymc_model)\n    with self.pymc_model:\n        pm.sample_posterior_predictive(\n            self.idata,\n            extend_inferencedata=True,\n            var_names=self.get_var_names() + [\"y_pred\"],\n        )\n</code></pre> <code>from_args(name: str, args: Dict[str, Any]) -&gt; 'HBR'</code> <code>classmethod</code> \u00b6 <p>Create model instance from command line arguments.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name for new model instance</p> required <code>args</code> <code>Dict[str, Any]</code> <p>Dictionary of command line arguments</p> required <p>Returns:</p> Type Description <code>HBR</code> <p>New model instance</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>@classmethod\ndef from_args(cls, name: str, args: Dict[str, Any]) -&gt; \"HBR\":\n    \"\"\"\n    Create model instance from command line arguments.\n\n    Parameters\n    ----------\n    name : str\n        Name for new model instance\n    args : Dict[str, Any]\n        Dictionary of command line arguments\n\n    Returns\n    -------\n    HBR\n        New model instance\n    \"\"\"\n    conf = HBRConf.from_args(args)\n    is_fitted = args.get(\"is_fitted\", False)\n    is_from_dict = True\n    self = cls(name, conf, is_fitted, is_from_dict)\n    return self\n</code></pre> <code>from_dict(my_dict: Dict[str, Any], path: Optional[str] = None) -&gt; 'HBR'</code> <code>classmethod</code> \u00b6 <p>Create model instance from serialized dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <code>Dict[str, Any]</code> <p>Dictionary containing serialized model</p> required <code>path</code> <code>Optional[str]</code> <p>Path to load inference data from, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>HBR</code> <p>New model instance</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>@classmethod\ndef from_dict(cls, my_dict: Dict[str, Any], path: Optional[str] = None) -&gt; \"HBR\":\n    \"\"\"\n    Create model instance from serialized dictionary.\n\n    Parameters\n    ----------\n    dict : Dict[str, Any]\n        Dictionary containing serialized model\n    path : Optional[str], optional\n        Path to load inference data from, by default None\n\n    Returns\n    -------\n    HBR\n        New model instance\n    \"\"\"\n    name = my_dict[\"name\"]\n    conf = HBRConf.from_dict(my_dict[\"reg_conf\"])\n    is_fitted = my_dict[\"is_fitted\"]\n    is_from_dict = True\n    self = cls(name, conf, is_fitted, is_from_dict)\n    if is_fitted and (path is not None):\n        idata_path = os.path.join(path, f\"idata_{name}.nc\")\n        self.load_idata(idata_path)\n    return self\n</code></pre> <code>get_var_names() -&gt; List[str]</code> \u00b6 <p>Get the variable names for the current likelihood function.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of variable names required for the current likelihood function. For Normal likelihood: ['mu_samples', 'sigma_samples'] For SHASH likelihoods: ['mu_samples', 'sigma_samples', 'epsilon_samples', 'delta_samples']</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If likelihood is not supported (must be 'Normal', 'SHASHb', or 'SHASHo')</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>def get_var_names(self) -&gt; List[str]:\n    \"\"\"Get the variable names for the current likelihood function.\n\n    Returns\n    -------\n    List[str]\n        List of variable names required for the current likelihood function.\n        For Normal likelihood: ['mu_samples', 'sigma_samples']\n        For SHASH likelihoods: ['mu_samples', 'sigma_samples', 'epsilon_samples', 'delta_samples']\n\n    Raises\n    ------\n    RuntimeError\n        If likelihood is not supported (must be 'Normal', 'SHASHb', or 'SHASHo')\n    \"\"\"\n    likelihood = self.likelihood\n    if likelihood == \"Normal\":\n        var_names = [\"mu_samples\", \"sigma_samples\"]\n    elif likelihood.startswith(\"SHASH\"):\n        var_names = [\n            \"mu_samples\",\n            \"sigma_samples\",\n            \"epsilon_samples\",\n            \"delta_samples\",\n        ]\n    else:\n        raise RuntimeError(\"Unsupported likelihood \" + likelihood)\n    return var_names\n</code></pre> <code>load_idata(path: str) -&gt; None</code> \u00b6 <p>Load inference data from NetCDF file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to load inference data from. Should end in '.nc'</p> required <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If model is fitted but inference data cannot be loaded from path</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>def load_idata(self, path: str) -&gt; None:\n    \"\"\"\n    Load inference data from NetCDF file.\n\n    Parameters\n    ----------\n    path : str\n        Path to load inference data from. Should end in '.nc'\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    RuntimeError\n        If model is fitted but inference data cannot be loaded from path\n    \"\"\"\n    if self.is_fitted:\n        try:\n            self.idata = az.from_netcdf(path)\n        except Exception as exc:\n            raise RuntimeError(f\"Could not load idata from {path}.\") from exc\n        self.replace_samples_in_idata_posterior()\n</code></pre> <code>predict(hbrdata: HBRData) -&gt; None</code> \u00b6 <p>Generate predictions for new data.</p> <p>Parameters:</p> Name Type Description Default <code>hbrdata</code> <code>HBRData</code> <p>Data object containing features to predict on</p> required <p>Returns:</p> Type Description <code>None</code> <p>Updates the model's inference data with predictions</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>def predict(self, hbrdata: HBRData) -&gt; None:\n    \"\"\"\n    Generate predictions for new data.\n\n    Parameters\n    ----------\n    hbrdata : HBRData\n        Data object containing features to predict on\n\n    Returns\n    -------\n    None\n        Updates the model's inference data with predictions\n    \"\"\"\n    if not self.pymc_model:\n        self.compile_model(hbrdata)\n    hbrdata.set_data_in_existing_model(self.pymc_model)\n    if hasattr(self.idata, \"posterior_predictive\"):\n        del self.idata.posterior_predictive\n    with self.pymc_model:\n        pm.sample_posterior_predictive(\n            self.idata,\n            extend_inferencedata=True,\n            var_names=self.get_var_names() + [\"y_pred\"],\n        )\n</code></pre> <code>remove_samples_from_idata_posterior() -&gt; None</code> \u00b6 <p>Remove sample variables from the posterior group of inference data.</p> <p>This method removes variables ending with '_samples' from the posterior group before saving to avoid privacy issues. The variables can be recomputed from the model parameters. The names of removed variables are stored in idata.attrs['removed_samples'].</p> <p>Parameters:</p> Name Type Description Default <code>None</code> required <p>Returns:</p> Type Description <code>None</code> Notes <p>This is used internally before saving the inference data to disk to reduce storage size, since sample variables can be recomputed from the model parameters.</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>def remove_samples_from_idata_posterior(self) -&gt; None:\n    \"\"\"\n    Remove sample variables from the posterior group of inference data.\n\n    This method removes variables ending with '_samples' from the posterior group\n    before saving to avoid privacy issues. The variables can be recomputed from the\n    model parameters. The names of removed variables are stored in idata.attrs['removed_samples'].\n\n    Parameters\n    ----------\n    None\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    This is used internally before saving the inference data to disk to reduce\n    storage size, since sample variables can be recomputed from the model parameters.\n    \"\"\"\n    post: xr.Dataset = self.idata.posterior  # type: ignore\n    for name in post.variables.mapping.keys():\n        if str(name).endswith(\"_samples\"):\n            post.drop_vars(str(name))\n            if \"removed_samples\" not in self.idata.attrs:\n                self.idata.attrs[\"removed_samples\"] = []\n            self.idata.attrs[\"removed_samples\"].append(name)\n</code></pre> <code>replace_samples_in_idata_posterior() -&gt; None</code> \u00b6 <p>Replace previously removed sample variables in the posterior group.</p> <p>This method adds back placeholder arrays for variables that were removed by remove_samples_from_idata_posterior(). The arrays are initialized with zeros and will be populated when the model is used.</p> <p>Parameters:</p> Name Type Description Default <code>None</code> required <p>Returns:</p> Type Description <code>None</code> Notes <p>This is used internally after loading inference data from disk to restore the structure needed for model predictions. The actual values will be recomputed when needed.</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>def replace_samples_in_idata_posterior(self) -&gt; None:\n    \"\"\"\n    Replace previously removed sample variables in the posterior group.\n\n    This method adds back placeholder arrays for variables that were removed by\n    remove_samples_from_idata_posterior(). The arrays are initialized with zeros\n    and will be populated when the model is used.\n\n    Parameters\n    ----------\n    None\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    This is used internally after loading inference data from disk to restore\n    the structure needed for model predictions. The actual values will be\n    recomputed when needed.\n    \"\"\"\n    post: xr.Dataset = self.idata.posterior  # type: ignore\n    for name in self.idata.attrs[\"removed_samples\"]:\n        post[name] = xr.DataArray(\n            np.zeros(post[name].shape),\n            dims=post[name].dims,\n        )\n</code></pre> <code>save_idata(path: str) -&gt; None</code> \u00b6 <p>Save inference data to NetCDF file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to save inference data to. Should end in '.nc'</p> required <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If model is fitted but does not have inference data</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>def save_idata(self, path: str) -&gt; None:\n    \"\"\"\n    Save inference data to NetCDF file.\n\n    Parameters\n    ----------\n    path : str\n        Path to save inference data to. Should end in '.nc'\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    RuntimeError\n        If model is fitted but does not have inference data\n    \"\"\"\n    if self.is_fitted:\n        if hasattr(self, \"idata\"):\n            self.remove_samples_from_idata_posterior()\n            self.idata.to_netcdf(path, groups=[\"posterior\"])\n        else:\n            raise RuntimeError(\n                \"HBR model is fitted but does not have idata. This should not happen.\"\n            )\n</code></pre> <code>to_dict(path: Optional[str] = None) -&gt; Dict[str, Any]</code> \u00b6 <p>Serialize model to dictionary format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>Path to save inference data, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing serialized model</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>def to_dict(self, path: Optional[str] = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Serialize model to dictionary format.\n\n    Parameters\n    ----------\n    path : Optional[str], optional\n        Path to save inference data, by default None\n\n    Returns\n    -------\n    Dict[str, Any]\n        Dictionary containing serialized model\n    \"\"\"\n    my_dict = super().to_dict()\n    if self.is_fitted and (path is not None):\n        idata_path = os.path.join(path, f\"idata_{self.name}.nc\")\n        self.save_idata(idata_path)\n    return my_dict\n</code></pre> <code>transfer(hbrconf: HBRConf, transferdata: HBRData, freedom: float) -&gt; HBR</code> \u00b6 <p>Perform transfer learning using existing model as prior.</p> <p>Parameters:</p> Name Type Description Default <code>hbrconf</code> <code>HBRConf</code> <p>Configuration for new model</p> required <code>transferdata</code> <code>HBRData</code> <p>Data for transfer learning</p> required <code>freedom</code> <code>float</code> <p>Parameter controlling influence of prior model (0-1)</p> required <p>Returns:</p> Type Description <code>HBR</code> <p>New model instance with transferred knowledge</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>def transfer(self, hbrconf: HBRConf, transferdata: HBRData, freedom: float) -&gt; HBR:\n    \"\"\"\n    Perform transfer learning using existing model as prior.\n\n    Parameters\n    ----------\n    hbrconf : HBRConf\n        Configuration for new model\n    transferdata : HBRData\n        Data for transfer learning\n    freedom : float\n        Parameter controlling influence of prior model (0-1)\n\n    Returns\n    -------\n    HBR\n        New model instance with transferred knowledge\n    \"\"\"\n    new_hbr_model = HBR(self.name, hbrconf)\n    new_hbr_model.compile_model(transferdata, self.idata, freedom)\n    with new_hbr_model.pymc_model:\n        new_hbr_model.idata = pm.sample(\n            hbrconf.draws,\n            tune=hbrconf.tune,\n            cores=hbrconf.cores,\n            chains=hbrconf.chains,\n            nuts_sampler=hbrconf.nuts_sampler,  # type: ignore\n        )\n        new_hbr_model.is_fitted = True\n\n    return new_hbr_model\n</code></pre> <code>zscores(hbrdata: HBRData, resample: bool = False) -&gt; xr.DataArray</code> \u00b6 <p>Calculate z-scores for observations.</p> <p>Parameters:</p> Name Type Description Default <code>hbrdata</code> <code>HBRData</code> <p>Data containing observations to calculate z-scores for</p> required <code>resample</code> <code>bool</code> <p>Whether to generate new posterior samples, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>Calculated z-scores</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr.py</code> <pre><code>def zscores(self, hbrdata: HBRData, resample: bool = False) -&gt; xr.DataArray:\n    \"\"\"\n    Calculate z-scores for observations.\n\n    Parameters\n    ----------\n    hbrdata : HBRData\n        Data containing observations to calculate z-scores for\n    resample : bool, optional\n        Whether to generate new posterior samples, by default False\n\n    Returns\n    -------\n    xr.DataArray\n        Calculated z-scores\n    \"\"\"\n    var_names = self.get_var_names()\n    if resample:\n        if self.pymc_model is None:\n            self.compile_model(hbrdata)\n        hbrdata.set_data_in_existing_model(self.pymc_model)\n        if hasattr(self.idata, \"posterior_predictive\"):\n            del self.idata.posterior_predictive\n        with self.pymc_model:  # type: ignore\n            pm.sample_posterior_predictive(\n                self.idata,\n                extend_inferencedata=True,\n                var_names=var_names + [\"y_pred\"],\n            )\n    post_pred = az.extract(\n        self.idata,\n        \"posterior_predictive\",\n        var_names=var_names,\n    )\n    array_of_vars = [self.likelihood] + list(\n        map(lambda x: np.squeeze(post_pred[x]), var_names)\n    )\n\n    zscores = xr.apply_ufunc(\n        zscore,\n        *array_of_vars,\n        kwargs={\"y\": hbrdata.y[:, None]},\n    ).mean(dim=\"sample\")\n\n    return zscores\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.hbr_conf","title":"<code>hbr_conf</code>","text":"<p>Configuration module for Hierarchical Bayesian Regression (HBR) models.</p> <p>This module provides the HBRConf class for configuring Hierarchical Bayesian Regression models in PCNToolkit. It defines parameters for MCMC sampling, model specification, and prior distributions, ensuring consistent configuration across HBR model instances.</p> <p>The module implements a comprehensive configuration system that handles: - MCMC sampling parameters (draws, chains, tuning) - Prior distribution specifications - Model likelihood selection - Parallel computation settings - Initialization strategies</p> <p>Classes:</p> Name Description <code>HBRConf</code> <p>Configuration class for HBR models, inheriting from RegConf. Handles all parameters needed to specify and fit an HBR model.</p> Notes <p>The configuration system supports multiple likelihood functions: - Normal: Standard normal likelihood - SHASHb: Sinh-arcsinh distribution (basic) - SHASHo: Sinh-arcsinh distribution (original) - SHASHo2: Sinh-arcsinh distribution (original v2)</p> <p>The module supports two NUTS sampler implementations: - pymc: Default PyMC implementation - nutpie: Alternative NutPie implementation</p> Example <p>conf = HBRConf( ...     draws=2000, ...     chains=4, ...     likelihood=\"Normal\", ...     cores=2 ... ) conf.to_dict() {'draws': 2000, 'chains': 4, 'likelihood': 'Normal', 'cores': 2, ...}</p> See Also <p>pcntoolkit.regression_model.reg_conf : Base configuration module pcntoolkit.regression_model.hbr.param : Prior parameter specifications pcntoolkit.regression_model.hbr.hbr : HBR model implementation</p>"},{"location":"api/#pcntoolkit.regression_model.hbr.hbr_conf.HBRConf","title":"<code>HBRConf</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegConf</code></p> <p>Configuration class for Hierarchical Bayesian Regression (HBR) models.</p> <p>This class defines the configuration parameters for HBR models, including sampling settings, model specification, and prior distributions. It inherits from RegConf and implements configuration validation specific to HBR models.</p> <p>Parameters:</p> Name Type Description Default <code>draws</code> <code>int</code> <p>Number of posterior samples to draw, by default 1000</p> <code>DRAWS</code> <code>tune</code> <code>int</code> <p>Number of tuning steps for the MCMC sampler, by default 1000</p> <code>TUNE</code> <code>chains</code> <code>int</code> <p>Number of parallel MCMC chains to run, by default 2</p> <code>CHAINS</code> <code>cores</code> <code>int</code> <p>Number of CPU cores to use for parallel sampling, by default 1</p> <code>CORES</code> <code>nuts_sampler</code> <code>str</code> <p>NUTS sampler implementation to use ('pymc' or 'nutpie'), by default 'pymc'</p> <code>NUTS_SAMPLER</code> <code>init</code> <code>str</code> <p>Initialization strategy for MCMC chains, by default 'jitter+adapt_diag'</p> <code>INIT</code> <code>likelihood</code> <code>str</code> <p>Likelihood function to use ('Normal', 'SHASHb', 'SHASHo', or 'SHASHo2'), by default 'Normal'</p> <code>LIKELIHOOD</code> <code>mu</code> <code>Param</code> <p>Prior parameters for the mean (\u03bc), defaults to Param.default_mu()</p> <code>default_mu()</code> <code>sigma</code> <code>Param</code> <p>Prior parameters for the standard deviation (\u03c3), defaults to Param.default_sigma()</p> <code>default_sigma()</code> <code>epsilon</code> <code>Param</code> <p>Prior parameters for epsilon (\u03b5), defaults to Param.default_epsilon()</p> <code>default_epsilon()</code> <code>delta</code> <code>Param</code> <p>Prior parameters for delta (\u03b4), defaults to Param.default_delta()</p> <code>default_delta()</code> <p>Attributes:</p> Name Type Description <code>has_random_effect</code> <code>bool</code> <p>Always True for HBR models as they include random effects by design</p> <code>__dataclass_fields__</code> <code>ClassVar[Dict[str, Any]]</code> <p>Class variable storing dataclass field definitions</p> <p>Methods:</p> Name Description <code>detect_configuration_problems</code> <p>Validates the configuration parameters and returns a list of any problems</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; conf = HBRConf(draws=2000, chains=4, likelihood='Normal')\n&gt;&gt;&gt; conf.detect_configuration_problems()\n[]\n</code></pre> Notes <ul> <li>Uses the dataclass decorator with frozen=True for immutability</li> <li>Implements comprehensive validation of all configuration parameters</li> <li>Supports multiple likelihood functions for different modeling scenarios</li> <li>Provides default values for all parameters based on common use cases</li> </ul> Source code in <code>pcntoolkit/regression_model/hbr/hbr_conf.py</code> <pre><code>@dataclass(frozen=True)\nclass HBRConf(RegConf):\n    \"\"\"\n    Configuration class for Hierarchical Bayesian Regression (HBR) models.\n\n    This class defines the configuration parameters for HBR models, including sampling\n    settings, model specification, and prior distributions. It inherits from RegConf\n    and implements configuration validation specific to HBR models.\n\n    Parameters\n    ----------\n    draws : int, optional\n        Number of posterior samples to draw, by default 1000\n    tune : int, optional\n        Number of tuning steps for the MCMC sampler, by default 1000\n    chains : int, optional\n        Number of parallel MCMC chains to run, by default 2\n    cores : int, optional\n        Number of CPU cores to use for parallel sampling, by default 1\n    nuts_sampler : str, optional\n        NUTS sampler implementation to use ('pymc' or 'nutpie'), by default 'pymc'\n    init : str, optional\n        Initialization strategy for MCMC chains, by default 'jitter+adapt_diag'\n    likelihood : str, optional\n        Likelihood function to use ('Normal', 'SHASHb', 'SHASHo', or 'SHASHo2'),\n        by default 'Normal'\n    mu : Param, optional\n        Prior parameters for the mean (\u03bc), defaults to Param.default_mu()\n    sigma : Param, optional\n        Prior parameters for the standard deviation (\u03c3), defaults to Param.default_sigma()\n    epsilon : Param, optional\n        Prior parameters for epsilon (\u03b5), defaults to Param.default_epsilon()\n    delta : Param, optional\n        Prior parameters for delta (\u03b4), defaults to Param.default_delta()\n\n    Attributes\n    ----------\n    has_random_effect : bool\n        Always True for HBR models as they include random effects by design\n    __dataclass_fields__ : ClassVar[Dict[str, Any]]\n        Class variable storing dataclass field definitions\n\n    Methods\n    -------\n    detect_configuration_problems()\n        Validates the configuration parameters and returns a list of any problems\n\n    Examples\n    --------\n    &gt;&gt;&gt; conf = HBRConf(draws=2000, chains=4, likelihood='Normal')\n    &gt;&gt;&gt; conf.detect_configuration_problems()\n    []\n\n    Notes\n    -----\n    - Uses the dataclass decorator with frozen=True for immutability\n    - Implements comprehensive validation of all configuration parameters\n    - Supports multiple likelihood functions for different modeling scenarios\n    - Provides default values for all parameters based on common use cases\n    \"\"\"\n\n    # sampling config\n    draws: int = DRAWS\n    tune: int = TUNE\n    chains: int = CHAINS\n    cores: int = CORES\n\n    nuts_sampler: str = NUTS_SAMPLER\n    init: str = INIT\n\n    # model config\n    likelihood: str = LIKELIHOOD\n\n    # prior config with defaults\n    mu: Param = field(default_factory=Param.default_mu)\n    sigma: Param = field(default_factory=Param.default_sigma)\n    epsilon: Param = field(default_factory=Param.default_epsilon)\n    delta: Param = field(default_factory=Param.default_delta)\n\n    # Add class variable for dataclass fields\n    __dataclass_fields__: ClassVar[Dict[str, Any]]\n\n    def detect_configuration_problems(self) -&gt; List[str]:\n        \"\"\"\n        Detects problems in the configuration and returns them as a list of strings.\n        \"\"\"\n        configuration_problems: List[str] = []\n\n        def add_problem(problem: str) -&gt; None:\n            nonlocal configuration_problems\n            configuration_problems.append(f\"{problem}\")\n\n        # Check if nuts_sampler is valid\n        if self.nuts_sampler not in [\"pymc\", \"nutpie\"]:\n            add_problem(\n                f\"\"\"Nuts sampler '{self.nuts_sampler}' is not supported. Please specify a valid nuts sampler. Available\n                options are 'pymc' and 'nutpie'.\"\"\"\n            )\n\n        # Check if likelihood is valid\n        if self.likelihood not in [\"Normal\", \"SHASHb\", \"SHASHo\", \"SHASHo2\"]:\n            add_problem(\n                f\"\"\"Likelihood '{self.likelihood}' is not supported. Please specify a valid likelihood.\"\"\"\n            )\n\n        # Check positivity of sigma\n        if self.sigma:\n            if self.sigma.linear:\n                if self.sigma.mapping == \"identity\":\n                    add_problem(\n                        \"\"\"Sigma must be strictly positive. As it's derived from a linear regression, it could \n                        potentially be negative without a proper mapping to the positive domain. To ensure positivity, \n                        use 'mapping=softplus' or 'mapping=exp'.\"\"\"\n                    )\n        # Check positivity of delta\n        if self.likelihood.startswith(\"SHASH\"):\n            if self.delta:\n                if self.delta.linear:\n                    if self.delta.mapping == \"identity\":\n                        add_problem(\n                            \"\"\"Delta must be strictly positive. As it's derived from a linear regression, it could \n                            potentially be negative without a proper mapping to the positive domain. To ensure \n                            positivity, use 'mapping=softplus' or 'mapping=exp'.\"\"\"\n                        )\n        # Check if epsilon and delta are provided for SHASH likelihoods\n        if self.likelihood.startswith(\"SHASH\"):\n            if not self.epsilon:\n                add_problem(\n                    \"Epsilon must be provided for SHASH likelihoods. Please specify epsilon.\"\n                )\n            if not self.delta:\n                add_problem(\n                    \"Delta must be provided for SHASH likelihoods. Please specify delta.\"\n                )\n\n        return configuration_problems\n\n    @classmethod\n    def from_args(cls, args: Dict[str, Any]) -&gt; \"HBRConf\":\n        \"\"\"\n        Creates a configuration from command line arguments parsed by argparse.\n        \"\"\"\n        # Filter out the arguments that are not relevant for this configuration\n        args_filt = {k: v for k, v in args.items() if k in cls.__dataclass_fields__}\n        likelihood = args_filt.get(\"likelihood\", \"Normal\")\n        if likelihood == \"Normal\":\n            args_filt[\"mu\"] = Param.from_args(\"mu\", args)\n            args_filt[\"sigma\"] = Param.from_args(\"sigma\", args)\n        elif likelihood.startswith(\"SHASH\"):\n            args_filt[\"mu\"] = Param.from_args(\"mu\", args)\n            args_filt[\"sigma\"] = Param.from_args(\"sigma\", args)\n            args_filt[\"epsilon\"] = Param.from_args(\"epsilon\", args)\n            args_filt[\"delta\"] = Param.from_args(\"delta\", args)\n        self = cls(**args_filt)\n        return self\n\n    @classmethod\n    def from_dict(cls, dct: Dict[str, Any]) -&gt; \"HBRConf\":\n        \"\"\"\n        Creates a configuration from a dictionary.\n        \"\"\"\n        # Filter out the arguments that are not relevant for this configuration\n        args_filt = {k: v for k, v in dct.items() if k in cls.__dataclass_fields__}\n        likelihood = args_filt.get(\"likelihood\", \"Normal\")\n        if likelihood == \"Normal\":\n            args_filt[\"mu\"] = Param.from_dict(dct[\"mu\"])\n            args_filt[\"sigma\"] = Param.from_dict(dct[\"sigma\"])\n        elif likelihood.startswith(\"SHASH\"):\n            args_filt[\"mu\"] = Param.from_dict(dct[\"mu\"])\n            args_filt[\"sigma\"] = Param.from_dict(dct[\"sigma\"])\n            args_filt[\"epsilon\"] = Param.from_dict(dct[\"epsilon\"])\n            args_filt[\"delta\"] = Param.from_dict(dct[\"delta\"])\n        self = cls(**args_filt)\n        return self\n\n    def to_dict(self, path: Optional[str] = None) -&gt; Dict[str, Any]:\n        \"\"\"Converts the configuration to a dictionary.\n        Parameters\n        ----------\n        path : str | None, optional\n            Optional file path for configurations that include file references.\n            Used to resolve relative paths to absolute paths.\n\n        Returns:\n        ----------\n            Dict[str, Any]: Dictionary containing the configuration.\n        \"\"\"\n        conf_dict = {\n            \"draws\": self.draws,\n            \"tune\": self.tune,\n            \"cores\": self.cores,\n            \"likelihood\": self.likelihood,\n            \"nuts_sampler\": self.nuts_sampler,\n        }\n        if self.mu:\n            conf_dict[\"mu\"] = self.mu.to_dict()\n        if self.sigma:\n            conf_dict[\"sigma\"] = self.sigma.to_dict()\n        if self.epsilon:\n            conf_dict[\"epsilon\"] = self.epsilon.to_dict()\n        if self.delta:\n            conf_dict[\"delta\"] = self.delta.to_dict()\n        return conf_dict\n\n    @property\n    def has_random_effect(self) -&gt; bool:\n        for attr in [\"mu\", \"sigma\", \"epsilon\", \"delta\"]:\n            if getattr(self, attr) and getattr(self, attr).has_random_effect:\n                return True\n        return False\n</code></pre> <code>detect_configuration_problems() -&gt; List[str]</code> \u00b6 <p>Detects problems in the configuration and returns them as a list of strings.</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr_conf.py</code> <pre><code>def detect_configuration_problems(self) -&gt; List[str]:\n    \"\"\"\n    Detects problems in the configuration and returns them as a list of strings.\n    \"\"\"\n    configuration_problems: List[str] = []\n\n    def add_problem(problem: str) -&gt; None:\n        nonlocal configuration_problems\n        configuration_problems.append(f\"{problem}\")\n\n    # Check if nuts_sampler is valid\n    if self.nuts_sampler not in [\"pymc\", \"nutpie\"]:\n        add_problem(\n            f\"\"\"Nuts sampler '{self.nuts_sampler}' is not supported. Please specify a valid nuts sampler. Available\n            options are 'pymc' and 'nutpie'.\"\"\"\n        )\n\n    # Check if likelihood is valid\n    if self.likelihood not in [\"Normal\", \"SHASHb\", \"SHASHo\", \"SHASHo2\"]:\n        add_problem(\n            f\"\"\"Likelihood '{self.likelihood}' is not supported. Please specify a valid likelihood.\"\"\"\n        )\n\n    # Check positivity of sigma\n    if self.sigma:\n        if self.sigma.linear:\n            if self.sigma.mapping == \"identity\":\n                add_problem(\n                    \"\"\"Sigma must be strictly positive. As it's derived from a linear regression, it could \n                    potentially be negative without a proper mapping to the positive domain. To ensure positivity, \n                    use 'mapping=softplus' or 'mapping=exp'.\"\"\"\n                )\n    # Check positivity of delta\n    if self.likelihood.startswith(\"SHASH\"):\n        if self.delta:\n            if self.delta.linear:\n                if self.delta.mapping == \"identity\":\n                    add_problem(\n                        \"\"\"Delta must be strictly positive. As it's derived from a linear regression, it could \n                        potentially be negative without a proper mapping to the positive domain. To ensure \n                        positivity, use 'mapping=softplus' or 'mapping=exp'.\"\"\"\n                    )\n    # Check if epsilon and delta are provided for SHASH likelihoods\n    if self.likelihood.startswith(\"SHASH\"):\n        if not self.epsilon:\n            add_problem(\n                \"Epsilon must be provided for SHASH likelihoods. Please specify epsilon.\"\n            )\n        if not self.delta:\n            add_problem(\n                \"Delta must be provided for SHASH likelihoods. Please specify delta.\"\n            )\n\n    return configuration_problems\n</code></pre> <code>from_args(args: Dict[str, Any]) -&gt; HBRConf</code> <code>classmethod</code> \u00b6 <p>Creates a configuration from command line arguments parsed by argparse.</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr_conf.py</code> <pre><code>@classmethod\ndef from_args(cls, args: Dict[str, Any]) -&gt; \"HBRConf\":\n    \"\"\"\n    Creates a configuration from command line arguments parsed by argparse.\n    \"\"\"\n    # Filter out the arguments that are not relevant for this configuration\n    args_filt = {k: v for k, v in args.items() if k in cls.__dataclass_fields__}\n    likelihood = args_filt.get(\"likelihood\", \"Normal\")\n    if likelihood == \"Normal\":\n        args_filt[\"mu\"] = Param.from_args(\"mu\", args)\n        args_filt[\"sigma\"] = Param.from_args(\"sigma\", args)\n    elif likelihood.startswith(\"SHASH\"):\n        args_filt[\"mu\"] = Param.from_args(\"mu\", args)\n        args_filt[\"sigma\"] = Param.from_args(\"sigma\", args)\n        args_filt[\"epsilon\"] = Param.from_args(\"epsilon\", args)\n        args_filt[\"delta\"] = Param.from_args(\"delta\", args)\n    self = cls(**args_filt)\n    return self\n</code></pre> <code>from_dict(dct: Dict[str, Any]) -&gt; HBRConf</code> <code>classmethod</code> \u00b6 <p>Creates a configuration from a dictionary.</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr_conf.py</code> <pre><code>@classmethod\ndef from_dict(cls, dct: Dict[str, Any]) -&gt; \"HBRConf\":\n    \"\"\"\n    Creates a configuration from a dictionary.\n    \"\"\"\n    # Filter out the arguments that are not relevant for this configuration\n    args_filt = {k: v for k, v in dct.items() if k in cls.__dataclass_fields__}\n    likelihood = args_filt.get(\"likelihood\", \"Normal\")\n    if likelihood == \"Normal\":\n        args_filt[\"mu\"] = Param.from_dict(dct[\"mu\"])\n        args_filt[\"sigma\"] = Param.from_dict(dct[\"sigma\"])\n    elif likelihood.startswith(\"SHASH\"):\n        args_filt[\"mu\"] = Param.from_dict(dct[\"mu\"])\n        args_filt[\"sigma\"] = Param.from_dict(dct[\"sigma\"])\n        args_filt[\"epsilon\"] = Param.from_dict(dct[\"epsilon\"])\n        args_filt[\"delta\"] = Param.from_dict(dct[\"delta\"])\n    self = cls(**args_filt)\n    return self\n</code></pre> <code>to_dict(path: Optional[str] = None) -&gt; Dict[str, Any]</code> \u00b6 <p>Converts the configuration to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | None</code> <p>Optional file path for configurations that include file references. Used to resolve relative paths to absolute paths.</p> <code>None</code> Returns: <pre><code>Dict[str, Any]: Dictionary containing the configuration.\n</code></pre> Source code in <code>pcntoolkit/regression_model/hbr/hbr_conf.py</code> <pre><code>def to_dict(self, path: Optional[str] = None) -&gt; Dict[str, Any]:\n    \"\"\"Converts the configuration to a dictionary.\n    Parameters\n    ----------\n    path : str | None, optional\n        Optional file path for configurations that include file references.\n        Used to resolve relative paths to absolute paths.\n\n    Returns:\n    ----------\n        Dict[str, Any]: Dictionary containing the configuration.\n    \"\"\"\n    conf_dict = {\n        \"draws\": self.draws,\n        \"tune\": self.tune,\n        \"cores\": self.cores,\n        \"likelihood\": self.likelihood,\n        \"nuts_sampler\": self.nuts_sampler,\n    }\n    if self.mu:\n        conf_dict[\"mu\"] = self.mu.to_dict()\n    if self.sigma:\n        conf_dict[\"sigma\"] = self.sigma.to_dict()\n    if self.epsilon:\n        conf_dict[\"epsilon\"] = self.epsilon.to_dict()\n    if self.delta:\n        conf_dict[\"delta\"] = self.delta.to_dict()\n    return conf_dict\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.hbr_data","title":"<code>hbr_data</code>","text":"<p>Data container and processor for Hierarchical Bayesian Regression (HBR) models.</p> <p>This module provides the HBRData class which handles data preparation, validation, and integration with PyMC models for hierarchical Bayesian regression. It manages covariates, response variables, and batch effects while providing utilities for data transformation and model integration.</p> Notes <p>The module requires PyMC for Bayesian modeling integration and NumPy for array operations. All data is validated and transformed into appropriate formats for use in HBR models.</p>"},{"location":"api/#pcntoolkit.regression_model.hbr.hbr_data.HBRData","title":"<code>HBRData</code>","text":"<p>A data container class for Hierarchical Bayesian Regression models.</p> <p>This class handles the preparation and management of data for HBR models, including covariates, response variables, and batch effects. It provides functionality for data validation, transformation, and integration with PyMC models.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Covariate matrix of shape (n_samples, n_features)</p> required <code>y</code> <code>ndarray</code> <p>Response variable array of shape (n_samples,) or (n_samples, 1)</p> <code>None</code> <code>batch_effects</code> <code>ndarray</code> <p>Batch effects matrix of shape (n_samples, n_batch_effects)</p> <code>None</code> <code>response_var</code> <code>str</code> <p>Name of the response variable</p> <code>None</code> <code>covariate_dims</code> <code>List[str]</code> <p>Names of the covariate dimensions</p> <code>None</code> <code>batch_effect_dims</code> <code>List[str]</code> <p>Names of the batch effect dimensions</p> <code>None</code> <code>datapoint_coords</code> <code>List[Any]</code> <p>Coordinates for each datapoint</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>X</code> <code>ndarray</code> <p>Processed covariate matrix</p> <code>y</code> <code>ndarray</code> <p>Processed response variable array</p> <code>batch_effects</code> <code>ndarray</code> <p>Processed batch effects matrix</p> <code>response_var</code> <code>str</code> <p>Name of the response variable</p> <code>covariate_dims</code> <code>List[str]</code> <p>Names of covariate dimensions</p> <code>batch_effect_dims</code> <code>List[str]</code> <p>Names of batch effect dimensions</p> <code>pm_X</code> <code>Data</code> <p>PyMC Data container for covariates</p> <code>pm_y</code> <code>Data</code> <p>PyMC Data container for response variable</p> <code>pm_batch_effect_indices</code> <code>Tuple[Data, ...]</code> <p>PyMC Data containers for batch effect indices</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X = np.random.randn(100, 5)\n&gt;&gt;&gt; y = np.random.randn(100)\n&gt;&gt;&gt; batch_effects = np.random.randint(0, 3, (100, 2))\n&gt;&gt;&gt; data = HBRData(X, y, batch_effects)\n&gt;&gt;&gt; with pm.Model() as model:\n...     data.add_to_graph(model)\n</code></pre> Source code in <code>pcntoolkit/regression_model/hbr/hbr_data.py</code> <pre><code>class HBRData:\n    \"\"\"\n    A data container class for Hierarchical Bayesian Regression models.\n\n    This class handles the preparation and management of data for HBR models,\n    including covariates, response variables, and batch effects. It provides\n    functionality for data validation, transformation, and integration with\n    PyMC models.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Covariate matrix of shape (n_samples, n_features)\n    y : np.ndarray, optional\n        Response variable array of shape (n_samples,) or (n_samples, 1)\n    batch_effects : np.ndarray, optional\n        Batch effects matrix of shape (n_samples, n_batch_effects)\n    response_var : str, optional\n        Name of the response variable\n    covariate_dims : List[str], optional\n        Names of the covariate dimensions\n    batch_effect_dims : List[str], optional\n        Names of the batch effect dimensions\n    datapoint_coords : List[Any], optional\n        Coordinates for each datapoint\n\n    Attributes\n    ----------\n    X : np.ndarray\n        Processed covariate matrix\n    y : np.ndarray\n        Processed response variable array\n    batch_effects : np.ndarray\n        Processed batch effects matrix\n    response_var : str\n        Name of the response variable\n    covariate_dims : List[str]\n        Names of covariate dimensions\n    batch_effect_dims : List[str]\n        Names of batch effect dimensions\n    pm_X : pm.Data\n        PyMC Data container for covariates\n    pm_y : pm.Data\n        PyMC Data container for response variable\n    pm_batch_effect_indices : Tuple[pm.Data, ...]\n        PyMC Data containers for batch effect indices\n\n    Examples\n    --------\n    &gt;&gt;&gt; X = np.random.randn(100, 5)\n    &gt;&gt;&gt; y = np.random.randn(100)\n    &gt;&gt;&gt; batch_effects = np.random.randint(0, 3, (100, 2))\n    &gt;&gt;&gt; data = HBRData(X, y, batch_effects)\n    &gt;&gt;&gt; with pm.Model() as model:\n    ...     data.add_to_graph(model)\n    \"\"\"\n\n    def __init__(\n        self,\n        X: np.ndarray,\n        y: Optional[np.ndarray] = None,\n        batch_effects: Optional[np.ndarray] = None,\n        response_var: Optional[str] = None,\n        covariate_dims: Optional[List[str]] = None,\n        batch_effect_dims: Optional[List[str]] = None,\n        datapoint_coords: Optional[List[Any]] = None,\n    ) -&gt; None:\n        self.check_and_set_data(X, y, batch_effects)\n\n        # Set the response var\n        self.response_var = response_var\n\n        # Set the number of covariates, datapoints and batch effect columns\n        self._n_covariates = self.X.shape[1]\n        self._n_datapoints = self.X.shape[0]\n        self._n_batch_effect_columns = self.batch_effects.shape[1]\n\n        # The coords will be passed to the pymc model\n        self._coords = OrderedDict()  # This preserves the order of the keys\n\n        # Create datapoint coordinates\n        self._coords[\"datapoints\"] = datapoint_coords or list(\n            np.arange(self._n_datapoints)\n        )\n\n        # Create covariate dims if they are not provided\n        self.covariate_dims = covariate_dims or [\n            \"covariate_\" + str(i) for i in range(self._n_covariates)\n        ]\n        assert (\n            len(self.covariate_dims) == self._n_covariates\n        ), \"The number of covariate dimensions must match the number of covariates\"\n        self._coords[\"covariates\"] = self.covariate_dims\n\n        # Create batch_effect dims if they are not provided\n        self.batch_effect_dims = batch_effect_dims or [\n            \"batch_effect_\" + str(i) for i in range(self._n_batch_effect_columns)\n        ]\n        assert (\n            len(self.batch_effect_dims) == self._n_batch_effect_columns\n        ), \"The number of batch effect dimensions must match the number of batch effect columns\"\n        self._coords[\"batch_effects\"] = self.batch_effect_dims\n\n        # This will be used to index the batch effects in the pymc model\n        self._batch_effects_maps = {}\n        for i, v in enumerate(self.batch_effect_dims):\n            be_values = np.unique(self.batch_effects[:, i])\n            self._batch_effects_maps[v] = {w: j for j, w in enumerate(be_values)}\n            self._coords[v] = be_values\n\n        self.pm_X: pm.Data = None  # type: ignore\n        self.pm_y: pm.Data = None  # type:ignore\n        self.pm_batch_effect_indices: Tuple[pm.Data, ...] = None  # type: ignore\n\n        self.create_batch_effect_indices()\n\n    def check_and_set_data(\n        self,\n        X: np.ndarray,\n        y: Optional[np.ndarray],\n        batch_effects: Optional[np.ndarray],\n    ) -&gt; None:\n        \"\"\"\n        Validate and set the input data arrays.\n\n        Performs validation checks on the input arrays and sets them as instance\n        attributes. Ensures proper dimensionality and compatibility between arrays.\n\n        Parameters\n        ----------\n        X : np.ndarray\n            Covariate matrix of shape (n_samples, n_features)\n        y : np.ndarray or None\n            Response variable array of shape (n_samples,) or (n_samples, 1).\n            If None, creates zero array of appropriate shape.\n        batch_effects : np.ndarray or None\n            Batch effects matrix of shape (n_samples, n_batch_effects).\n            If None, creates zero array of appropriate shape.\n\n        Raises\n        ------\n        ValueError\n            If X is None\n        AssertionError\n            If array dimensions are incompatible or y has incorrect shape\n\n        Notes\n        -----\n        - Automatically expands 1D arrays to 2D\n        - Squeezes y to 1D if it's a 2D array with one column\n        \"\"\"\n\n        if X is None:\n            raise ValueError(\"X must be provided\")\n        else:\n            self.X = X\n\n        if y is None:\n            self.y = np.zeros((X.shape[0], 1))\n        else:\n            self.y = y\n\n        if batch_effects is None:\n            warnings.warn(\n                \"batch_effects is not provided, setting self.batch_effects to zeros\"\n            )\n            self.batch_effects = np.zeros((X.shape[0], 1))\n        else:\n            self.batch_effects = batch_effects\n\n        self.X, self.batch_effects = self.expand_all(\"X\", \"batch_effects\")\n\n        # Check that the dimensions are correct\n        assert (\n            self.X.shape[0] == self.y.shape[0] == self.batch_effects.shape[0]\n        ), \"X, y and batch_effects must have the same number of rows\"\n        if len(self.y.shape) &gt; 1:\n            assert (\n                self.y.shape[1] == 1\n            ), \"y can only have one column, or it must be a 1D array\"\n            self.y = np.squeeze(self.y)\n\n    def add_to_graph(self, model: pm.Model) -&gt; None:\n        \"\"\"\n        Add data variables to a PyMC model graph.\n\n        Parameters\n        ----------\n        model : pm.Model\n            PyMC model to add the data variables to\n\n        Notes\n        -----\n        Creates PyMC Data objects for X, y, and batch effect indices within\n        the model context. Also adds custom batch effect dimensions to the model.\n        \"\"\"\n        with model:\n            self.pm_X = pm.Data(\"X\", self.X, dims=(\"datapoints\", \"covariates\"))\n            self.pm_y = pm.Data(\"y\", self.y, dims=(\"datapoints\",))\n            self.pm_batch_effect_indices = tuple(\n                [\n                    pm.Data(\n                        str(self.batch_effect_dims[i]) + \"_data\",\n                        self.batch_effect_indices[i],\n                        dims=(\"datapoints\",),\n                    )\n                    for i in range(self._n_batch_effect_columns)\n                ]\n            )\n\n            model.custom_batch_effect_dims = self.batch_effect_dims  # type: ignore\n\n    def set_data_in_existing_model(self, model: pm.Model) -&gt; None:\n        \"\"\"\n        Update data values in an existing PyMC model.\n\n        Parameters\n        ----------\n        model : pm.Model\n            Existing PyMC model whose data values need to be updated\n\n        Notes\n        -----\n        Updates the values of X, y, and batch effect indices in the model\n        while preserving the model structure.\n        \"\"\"\n        model.set_data(\n            \"X\",\n            self.X,\n            coords={\"datapoints\": self._coords[\"datapoints\"]},\n        )\n        self.pm_X = model[\"X\"]\n        model.set_data(\"y\", self.y)\n        self.pm_y = model[\"y\"]\n        be_acc = []\n        for i in range(self._n_batch_effect_columns):\n            model.set_data(\n                str(self.batch_effect_dims[i]) + \"_data\", self.batch_effect_indices[i]\n            )\n            be_acc.append(model[self.batch_effect_dims[i] + \"_data\"])\n        self.pm_batch_effect_indices = tuple(be_acc)\n\n    def expand_all(self, *args: str) -&gt; Tuple[np.ndarray, ...]:\n        \"\"\"\n        Expand multiple data attributes to 2D arrays if necessary.\n\n        Parameters\n        ----------\n        *args : str\n            Names of attributes to expand\n\n        Returns\n        -------\n        Tuple[np.ndarray, ...]\n            Tuple of expanded arrays\n\n        See Also\n        --------\n        expand : Method for expanding individual arrays\n        \"\"\"\n        return tuple(self.expand(arg) for arg in args)\n\n    def expand(self, data_attr_str: str) -&gt; np.ndarray:\n        \"\"\"\n        Expand a single data attribute to a 2D array if necessary.\n\n        Parameters\n        ----------\n        data_attr_str : str\n            Name of the attribute to expand\n\n        Returns\n        -------\n        np.ndarray\n            Expanded array\n\n        Raises\n        ------\n        AssertionError\n            If the array is not 1D or 2D\n        \"\"\"\n        data_attr: np.ndarray = getattr(self, data_attr_str)\n        if len(data_attr.shape) == 1:\n            data_attr = data_attr.reshape(-1, 1)\n        assert len(data_attr.shape) == 2, f\"{data_attr_str} must be a 1D or 2D array\"\n        return data_attr\n\n    def set_batch_effects_maps(\n        self, batch_effects_maps: Dict[str, Dict[Any, int]]\n    ) -&gt; None:\n        \"\"\"\n        Set the mapping between batch effect values and their indices.\n\n        Parameters\n        ----------\n        batch_effects_maps : Dict[str, Dict[Any, int]]\n            Mapping from batch effect names to value-index dictionaries\n\n        Notes\n        -----\n        Updates the batch effect indices after setting the new maps.\n        \"\"\"\n        self._batch_effects_maps = batch_effects_maps\n        self.create_batch_effect_indices()\n\n    def create_batch_effect_indices(self) -&gt; None:\n        \"\"\"\n        Create numerical indices for batch effects.\n\n        Creates a list of arrays where each array contains the numerical indices\n        corresponding to the categorical batch effect values. These indices are\n        used for indexing in the PyMC model.\n\n        Notes\n        -----\n        The indices are created based on the current batch_effects_maps.\n        \"\"\"\n        self.batch_effect_indices = []\n        for i, v in enumerate(self.batch_effect_dims):\n            self.batch_effect_indices.append(\n                np.array(\n                    [self._batch_effects_maps[v][w] for w in self.batch_effects[:, i]]\n                )\n            )\n\n    # pylint: disable=C0116\n\n    @property\n    def n_covariates(self) -&gt; int:\n        return self._n_covariates\n\n    @property\n    def n_datapoints(self) -&gt; int:\n        return self._n_datapoints\n\n    @property\n    def n_batch_effect_columns(self) -&gt; int:\n        return self._n_batch_effect_columns\n\n    @property\n    def coords(self) -&gt; OrderedDict:\n        return self._coords\n\n    @property\n    def batch_effects_maps(self) -&gt; Dict[str, Dict[Any, int]]:\n        return self._batch_effects_maps\n</code></pre> <code>add_to_graph(model: pm.Model) -&gt; None</code> \u00b6 <p>Add data variables to a PyMC model graph.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>PyMC model to add the data variables to</p> required Notes <p>Creates PyMC Data objects for X, y, and batch effect indices within the model context. Also adds custom batch effect dimensions to the model.</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr_data.py</code> <pre><code>def add_to_graph(self, model: pm.Model) -&gt; None:\n    \"\"\"\n    Add data variables to a PyMC model graph.\n\n    Parameters\n    ----------\n    model : pm.Model\n        PyMC model to add the data variables to\n\n    Notes\n    -----\n    Creates PyMC Data objects for X, y, and batch effect indices within\n    the model context. Also adds custom batch effect dimensions to the model.\n    \"\"\"\n    with model:\n        self.pm_X = pm.Data(\"X\", self.X, dims=(\"datapoints\", \"covariates\"))\n        self.pm_y = pm.Data(\"y\", self.y, dims=(\"datapoints\",))\n        self.pm_batch_effect_indices = tuple(\n            [\n                pm.Data(\n                    str(self.batch_effect_dims[i]) + \"_data\",\n                    self.batch_effect_indices[i],\n                    dims=(\"datapoints\",),\n                )\n                for i in range(self._n_batch_effect_columns)\n            ]\n        )\n\n        model.custom_batch_effect_dims = self.batch_effect_dims  # type: ignore\n</code></pre> <code>check_and_set_data(X: np.ndarray, y: Optional[np.ndarray], batch_effects: Optional[np.ndarray]) -&gt; None</code> \u00b6 <p>Validate and set the input data arrays.</p> <p>Performs validation checks on the input arrays and sets them as instance attributes. Ensures proper dimensionality and compatibility between arrays.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Covariate matrix of shape (n_samples, n_features)</p> required <code>y</code> <code>ndarray or None</code> <p>Response variable array of shape (n_samples,) or (n_samples, 1). If None, creates zero array of appropriate shape.</p> required <code>batch_effects</code> <code>ndarray or None</code> <p>Batch effects matrix of shape (n_samples, n_batch_effects). If None, creates zero array of appropriate shape.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If X is None</p> <code>AssertionError</code> <p>If array dimensions are incompatible or y has incorrect shape</p> Notes <ul> <li>Automatically expands 1D arrays to 2D</li> <li>Squeezes y to 1D if it's a 2D array with one column</li> </ul> Source code in <code>pcntoolkit/regression_model/hbr/hbr_data.py</code> <pre><code>def check_and_set_data(\n    self,\n    X: np.ndarray,\n    y: Optional[np.ndarray],\n    batch_effects: Optional[np.ndarray],\n) -&gt; None:\n    \"\"\"\n    Validate and set the input data arrays.\n\n    Performs validation checks on the input arrays and sets them as instance\n    attributes. Ensures proper dimensionality and compatibility between arrays.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Covariate matrix of shape (n_samples, n_features)\n    y : np.ndarray or None\n        Response variable array of shape (n_samples,) or (n_samples, 1).\n        If None, creates zero array of appropriate shape.\n    batch_effects : np.ndarray or None\n        Batch effects matrix of shape (n_samples, n_batch_effects).\n        If None, creates zero array of appropriate shape.\n\n    Raises\n    ------\n    ValueError\n        If X is None\n    AssertionError\n        If array dimensions are incompatible or y has incorrect shape\n\n    Notes\n    -----\n    - Automatically expands 1D arrays to 2D\n    - Squeezes y to 1D if it's a 2D array with one column\n    \"\"\"\n\n    if X is None:\n        raise ValueError(\"X must be provided\")\n    else:\n        self.X = X\n\n    if y is None:\n        self.y = np.zeros((X.shape[0], 1))\n    else:\n        self.y = y\n\n    if batch_effects is None:\n        warnings.warn(\n            \"batch_effects is not provided, setting self.batch_effects to zeros\"\n        )\n        self.batch_effects = np.zeros((X.shape[0], 1))\n    else:\n        self.batch_effects = batch_effects\n\n    self.X, self.batch_effects = self.expand_all(\"X\", \"batch_effects\")\n\n    # Check that the dimensions are correct\n    assert (\n        self.X.shape[0] == self.y.shape[0] == self.batch_effects.shape[0]\n    ), \"X, y and batch_effects must have the same number of rows\"\n    if len(self.y.shape) &gt; 1:\n        assert (\n            self.y.shape[1] == 1\n        ), \"y can only have one column, or it must be a 1D array\"\n        self.y = np.squeeze(self.y)\n</code></pre> <code>create_batch_effect_indices() -&gt; None</code> \u00b6 <p>Create numerical indices for batch effects.</p> <p>Creates a list of arrays where each array contains the numerical indices corresponding to the categorical batch effect values. These indices are used for indexing in the PyMC model.</p> Notes <p>The indices are created based on the current batch_effects_maps.</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr_data.py</code> <pre><code>def create_batch_effect_indices(self) -&gt; None:\n    \"\"\"\n    Create numerical indices for batch effects.\n\n    Creates a list of arrays where each array contains the numerical indices\n    corresponding to the categorical batch effect values. These indices are\n    used for indexing in the PyMC model.\n\n    Notes\n    -----\n    The indices are created based on the current batch_effects_maps.\n    \"\"\"\n    self.batch_effect_indices = []\n    for i, v in enumerate(self.batch_effect_dims):\n        self.batch_effect_indices.append(\n            np.array(\n                [self._batch_effects_maps[v][w] for w in self.batch_effects[:, i]]\n            )\n        )\n</code></pre> <code>expand(data_attr_str: str) -&gt; np.ndarray</code> \u00b6 <p>Expand a single data attribute to a 2D array if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>data_attr_str</code> <code>str</code> <p>Name of the attribute to expand</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Expanded array</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the array is not 1D or 2D</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr_data.py</code> <pre><code>def expand(self, data_attr_str: str) -&gt; np.ndarray:\n    \"\"\"\n    Expand a single data attribute to a 2D array if necessary.\n\n    Parameters\n    ----------\n    data_attr_str : str\n        Name of the attribute to expand\n\n    Returns\n    -------\n    np.ndarray\n        Expanded array\n\n    Raises\n    ------\n    AssertionError\n        If the array is not 1D or 2D\n    \"\"\"\n    data_attr: np.ndarray = getattr(self, data_attr_str)\n    if len(data_attr.shape) == 1:\n        data_attr = data_attr.reshape(-1, 1)\n    assert len(data_attr.shape) == 2, f\"{data_attr_str} must be a 1D or 2D array\"\n    return data_attr\n</code></pre> <code>expand_all(*args: str) -&gt; Tuple[np.ndarray, ...]</code> \u00b6 <p>Expand multiple data attributes to 2D arrays if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>Names of attributes to expand</p> <code>()</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ...]</code> <p>Tuple of expanded arrays</p> See Also <p>expand : Method for expanding individual arrays</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr_data.py</code> <pre><code>def expand_all(self, *args: str) -&gt; Tuple[np.ndarray, ...]:\n    \"\"\"\n    Expand multiple data attributes to 2D arrays if necessary.\n\n    Parameters\n    ----------\n    *args : str\n        Names of attributes to expand\n\n    Returns\n    -------\n    Tuple[np.ndarray, ...]\n        Tuple of expanded arrays\n\n    See Also\n    --------\n    expand : Method for expanding individual arrays\n    \"\"\"\n    return tuple(self.expand(arg) for arg in args)\n</code></pre> <code>set_batch_effects_maps(batch_effects_maps: Dict[str, Dict[Any, int]]) -&gt; None</code> \u00b6 <p>Set the mapping between batch effect values and their indices.</p> <p>Parameters:</p> Name Type Description Default <code>batch_effects_maps</code> <code>Dict[str, Dict[Any, int]]</code> <p>Mapping from batch effect names to value-index dictionaries</p> required Notes <p>Updates the batch effect indices after setting the new maps.</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr_data.py</code> <pre><code>def set_batch_effects_maps(\n    self, batch_effects_maps: Dict[str, Dict[Any, int]]\n) -&gt; None:\n    \"\"\"\n    Set the mapping between batch effect values and their indices.\n\n    Parameters\n    ----------\n    batch_effects_maps : Dict[str, Dict[Any, int]]\n        Mapping from batch effect names to value-index dictionaries\n\n    Notes\n    -----\n    Updates the batch effect indices after setting the new maps.\n    \"\"\"\n    self._batch_effects_maps = batch_effects_maps\n    self.create_batch_effect_indices()\n</code></pre> <code>set_data_in_existing_model(model: pm.Model) -&gt; None</code> \u00b6 <p>Update data values in an existing PyMC model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Existing PyMC model whose data values need to be updated</p> required Notes <p>Updates the values of X, y, and batch effect indices in the model while preserving the model structure.</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr_data.py</code> <pre><code>def set_data_in_existing_model(self, model: pm.Model) -&gt; None:\n    \"\"\"\n    Update data values in an existing PyMC model.\n\n    Parameters\n    ----------\n    model : pm.Model\n        Existing PyMC model whose data values need to be updated\n\n    Notes\n    -----\n    Updates the values of X, y, and batch effect indices in the model\n    while preserving the model structure.\n    \"\"\"\n    model.set_data(\n        \"X\",\n        self.X,\n        coords={\"datapoints\": self._coords[\"datapoints\"]},\n    )\n    self.pm_X = model[\"X\"]\n    model.set_data(\"y\", self.y)\n    self.pm_y = model[\"y\"]\n    be_acc = []\n    for i in range(self._n_batch_effect_columns):\n        model.set_data(\n            str(self.batch_effect_dims[i]) + \"_data\", self.batch_effect_indices[i]\n        )\n        be_acc.append(model[self.batch_effect_dims[i] + \"_data\"])\n    self.pm_batch_effect_indices = tuple(be_acc)\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.hbr_util","title":"<code>hbr_util</code>","text":"<p>Utility functions for Heteroscedastic Bayesian Regression (HBR) models.</p> <p>This module provides mathematical utility functions for implementing various likelihood models in HBR, particularly focusing on the SHASH (Sinh-Arcsinh) family of distributions and normal distributions. It includes functions for computing z-scores, centiles, and various transformations needed for these likelihood models.</p> <p>The module implements three variants of the SHASH distribution: - SHASHo: Original SHASH implementation - SHASHo2: Modified SHASH with delta-scaled sigma - SHASHb: SHASH with bias correction As well as the standard Normal distribution.</p> <p>Functions:</p> Name Description <code>S_inv : Inverse sinh transformation</code> <code>K : Modified Bessel function computation for unique values</code> <code>P : P function implementation from Jones et al.</code> <code>m : Uncentered moment calculation</code> <code>centile : Centile computation for different likelihood models</code> <code>zscore : Z-score computation for different likelihood models</code> References <p>.. [1] Jones, M. C., &amp; Pewsey, A. (2009). Sinh-arcsinh distributions.        Biometrika, 96(4), 761-780.</p>"},{"location":"api/#pcntoolkit.regression_model.hbr.hbr_util.K","title":"<code>K(p: NDArray[np.float64], x: float) -&gt; NDArray[np.float64]</code>","text":"<p>Compute modified Bessel function of the second kind for unique values.</p> <p>Computes the values of spp.kv(p,x) for only the unique values of p to improve efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>NDArray[float64]</code> <p>Array of values for which to compute the Bessel function</p> required <code>x</code> <code>float</code> <p>Second parameter of the Bessel function</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Modified Bessel function values reshaped to match input shape</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr_util.py</code> <pre><code>def K(p: NDArray[np.float64], x: float) -&gt; NDArray[np.float64]:\n    \"\"\"Compute modified Bessel function of the second kind for unique values.\n\n    Computes the values of spp.kv(p,x) for only the unique values of p to improve efficiency.\n\n    Parameters\n    ----------\n    p : NDArray[np.float64]\n        Array of values for which to compute the Bessel function\n    x : float\n        Second parameter of the Bessel function\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Modified Bessel function values reshaped to match input shape\n    \"\"\"\n    ps, idxs = np.unique(p, return_inverse=True)\n    return spp.kv(ps, x)[idxs].reshape(p.shape)\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.hbr_util.P","title":"<code>P(q: NDArray[np.float64]) -&gt; NDArray[np.float64]</code>","text":"<p>Compute the P function as given in Jones et al.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>NDArray[float64]</code> <p>Input array for P function calculation</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Result of P function computation</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr_util.py</code> <pre><code>def P(q: NDArray[np.float64]) -&gt; NDArray[np.float64]:\n    \"\"\"Compute the P function as given in Jones et al.\n\n    Parameters\n    ----------\n    q : NDArray[np.float64]\n        Input array for P function calculation\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Result of P function computation\n    \"\"\"\n    frac = np.exp(1 / 4) / np.sqrt(8 * np.pi)\n    K1 = K((q + 1) / 2, 1 / 4)\n    K2 = K((q - 1) / 2, 1 / 4)\n    a = (K1 + K2) * frac\n    return a\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.hbr_util.S","title":"<code>S(x: NDArray[np.float64], e: NDArray[np.float64], d: NDArray[np.float64]) -&gt; NDArray[np.float64]</code>","text":"<p>Sinh transformation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>NDArray[float64]</code> <p>Input array to transform</p> required <code>e</code> <code>NDArray[float64]</code> <p>Epsilon parameter for the transformation</p> required <code>d</code> <code>NDArray[float64]</code> <p>Delta parameter for the transformation</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Transformed array using sinh function</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr_util.py</code> <pre><code>def S(\n    x: NDArray[np.float64], e: NDArray[np.float64], d: NDArray[np.float64]\n) -&gt; NDArray[np.float64]:\n    \"\"\"Sinh transformation.\n\n    Parameters\n    ----------\n    x : NDArray[np.float64]\n        Input array to transform\n    e : NDArray[np.float64]\n        Epsilon parameter for the transformation\n    d : NDArray[np.float64]\n        Delta parameter for the transformation\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Transformed array using sinh function\n    \"\"\"\n    return np.sinh(np.arcsinh(x) * d - e)\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.hbr_util.S_inv","title":"<code>S_inv(x: NDArray[np.float64], e: NDArray[np.float64], d: NDArray[np.float64]) -&gt; NDArray[np.float64]</code>","text":"<p>Inverse sinh arcsinh transformation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>NDArray[float64]</code> <p>Input array to transform</p> required <code>e</code> <code>NDArray[float64]</code> <p>Epsilon parameter for the transformation</p> required <code>d</code> <code>NDArray[float64]</code> <p>Delta parameter for the transformation</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Transformed array using inverse sinh function</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr_util.py</code> <pre><code>def S_inv(\n    x: NDArray[np.float64], e: NDArray[np.float64], d: NDArray[np.float64]\n) -&gt; NDArray[np.float64]:\n    \"\"\"Inverse sinh arcsinh transformation.\n\n    Parameters\n    ----------\n    x : NDArray[np.float64]\n        Input array to transform\n    e : NDArray[np.float64]\n        Epsilon parameter for the transformation\n    d : NDArray[np.float64]\n        Delta parameter for the transformation\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Transformed array using inverse sinh function\n    \"\"\"\n    return np.sinh((np.arcsinh(x) + e) / d)\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.hbr_util.centile","title":"<code>centile(likelihood: Literal['SHASHo', 'SHASHo2', 'SHASHb', 'Normal'], mu: NDArray[np.float64], sigma: NDArray[np.float64], zs: NDArray[np.float64], epsilon: NDArray[np.float64] = None, delta: NDArray[np.float64] = None) -&gt; NDArray[np.float64]</code>","text":"<p>Compute centiles for different likelihood models.</p> <p>Parameters:</p> Name Type Description Default <code>likelihood</code> <code>(SHASHo, SHASHo2, SHASHb, Normal)</code> <p>The likelihood model to use</p> <code>\"SHASHo\"</code> <code>mu</code> <code>NDArray[float64]</code> <p>Mean parameter array</p> required <code>sigma</code> <code>NDArray[float64]</code> <p>Standard deviation parameter array</p> required <code>epsilon</code> <code>NDArray[float64] or None</code> <p>Epsilon parameter for SHASH models</p> <code>None</code> <code>delta</code> <code>NDArray[float64] or None</code> <p>Delta parameter for SHASH models</p> <code>None</code> <code>zs</code> <code>NDArray[float64] or float</code> <p>Z-scores for quantile computation, default 0</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Computed quantiles</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr_util.py</code> <pre><code>def centile(\n    likelihood: Literal[\"SHASHo\", \"SHASHo2\", \"SHASHb\", \"Normal\"],\n    mu: NDArray[np.float64],\n    sigma: NDArray[np.float64],\n    zs: NDArray[np.float64],\n    epsilon: NDArray[np.float64] = None,  # type: ignore\n    delta: NDArray[np.float64] = None,  # type: ignore\n) -&gt; NDArray[np.float64]:\n    \"\"\"Compute centiles for different likelihood models.\n\n    Parameters\n    ----------\n    likelihood : {\"SHASHo\", \"SHASHo2\", \"SHASHb\", \"Normal\"}\n        The likelihood model to use\n    mu : NDArray[np.float64]\n        Mean parameter array\n    sigma : NDArray[np.float64]\n        Standard deviation parameter array\n    epsilon : NDArray[np.float64] or None, optional\n        Epsilon parameter for SHASH models\n    delta : NDArray[np.float64] or None, optional\n        Delta parameter for SHASH models\n    zs : NDArray[np.float64] or float, optional\n        Z-scores for quantile computation, default 0\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Computed quantiles\n    \"\"\"\n    if zs is None:\n        zs = 0\n\n    if likelihood == \"SHASHo\":\n        quantiles = S_inv(zs, epsilon, delta) * sigma + mu\n    elif likelihood == \"SHASHo2\":\n        sigma_d = sigma / delta\n        quantiles = S_inv(zs, epsilon, delta) * sigma_d + mu\n    elif likelihood == \"SHASHb\":\n        true_mu = m(epsilon, delta, 1)\n        true_sigma = np.sqrt((m(epsilon, delta, 2) - true_mu**2))\n        SHASH_c = (S_inv(zs, epsilon, delta) - true_mu) / true_sigma\n        quantiles = SHASH_c * sigma + mu\n    elif likelihood == \"Normal\":\n        quantiles = zs * sigma + mu\n    else:\n        raise ValueError(\"Unsupported likelihood\")\n    return quantiles\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.hbr_util.m","title":"<code>m(epsilon: NDArray[np.float64], delta: NDArray[np.float64], r: int) -&gt; NDArray[np.float64]</code>","text":"<p>Calculate the r'th uncentered moment as given in Jones et al.</p> <p>Parameters:</p> Name Type Description Default <code>epsilon</code> <code>NDArray[float64]</code> <p>Epsilon parameter array</p> required <code>delta</code> <code>NDArray[float64]</code> <p>Delta parameter array</p> required <code>r</code> <code>int</code> <p>Order of the moment to calculate</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>The r'th uncentered moment</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr_util.py</code> <pre><code>def m(\n    epsilon: NDArray[np.float64], delta: NDArray[np.float64], r: int\n) -&gt; NDArray[np.float64]:\n    \"\"\"Calculate the r'th uncentered moment as given in Jones et al.\n\n    Parameters\n    ----------\n    epsilon : NDArray[np.float64]\n        Epsilon parameter array\n    delta : NDArray[np.float64]\n        Delta parameter array\n    r : int\n        Order of the moment to calculate\n\n    Returns\n    -------\n    NDArray[np.float64]\n        The r'th uncentered moment\n    \"\"\"\n    frac1 = 1 / np.power(2, r)\n    acc = 0\n    for i in range(r + 1):\n        combs = spp.comb(r, i)\n        flip = np.power(-1, i)\n        ex = np.exp((r - 2 * i) * epsilon / delta)\n        p = P((r - 2 * i) / delta)\n        acc += combs * flip * ex * p\n    return frac1 * acc\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.hbr_util.zscore","title":"<code>zscore(likelihood: Literal['SHASHo', 'SHASHo2', 'SHASHb', 'Normal'], mu: NDArray[np.float64], sigma: NDArray[np.float64], y: NDArray[np.float64], epsilon: NDArray[np.float64] = None, delta: NDArray[np.float64] = None) -&gt; NDArray[np.float64]</code>","text":"<p>Compute z-scores for different likelihood models.</p> <p>Parameters:</p> Name Type Description Default <code>likelihood</code> <code>(SHASHo, SHASHo2, SHASHb, Normal)</code> <p>The likelihood model to use</p> <code>\"SHASHo\"</code> <code>mu</code> <code>NDArray[float64]</code> <p>Mean parameter array</p> required <code>sigma</code> <code>NDArray[float64]</code> <p>Standard deviation parameter array</p> required <code>epsilon</code> <code>NDArray[float64] or None</code> <p>Epsilon parameter for SHASH models</p> <code>None</code> <code>delta</code> <code>NDArray[float64] or None</code> <p>Delta parameter for SHASH models</p> <code>None</code> <code>y</code> <code>NDArray[float64] or None</code> <p>Observed values for z-score computation</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Computed z-scores</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If likelihood is not one of the supported types</p> Source code in <code>pcntoolkit/regression_model/hbr/hbr_util.py</code> <pre><code>def zscore(\n    likelihood: Literal[\"SHASHo\", \"SHASHo2\", \"SHASHb\", \"Normal\"],\n    mu: NDArray[np.float64],\n    sigma: NDArray[np.float64],\n    y: NDArray[np.float64],\n    epsilon: NDArray[np.float64] = None,  # type: ignore\n    delta: NDArray[np.float64] = None,  # type: ignore\n) -&gt; NDArray[np.float64]:\n    \"\"\"Compute z-scores for different likelihood models.\n\n    Parameters\n    ----------\n    likelihood : {\"SHASHo\", \"SHASHo2\", \"SHASHb\", \"Normal\"}\n        The likelihood model to use\n    mu : NDArray[np.float64]\n        Mean parameter array\n    sigma : NDArray[np.float64]\n        Standard deviation parameter array\n    epsilon : NDArray[np.float64] or None, optional\n        Epsilon parameter for SHASH models\n    delta : NDArray[np.float64] or None, optional\n        Delta parameter for SHASH models\n    y : NDArray[np.float64] or None, optional\n        Observed values for z-score computation\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Computed z-scores\n\n    Raises\n    ------\n    ValueError\n        If likelihood is not one of the supported types\n    \"\"\"\n    if likelihood == \"SHASHo\":\n        SHASH = (y - mu) / sigma\n        Z = np.sinh(np.arcsinh(SHASH) * delta - epsilon)\n    elif likelihood == \"SHASHo2\":\n        sigma_d = sigma / delta\n        SHASH = (y - mu) / sigma_d\n        Z = np.sinh(np.arcsinh(SHASH) * delta - epsilon)\n    elif likelihood == \"SHASHb\":\n        true_mu = m(epsilon, delta, 1)\n        true_sigma = np.sqrt((m(epsilon, delta, 2) - true_mu**2))\n        SHASH_c = (y - mu) / sigma\n        SHASH = SHASH_c * true_sigma + true_mu\n        Z = np.sinh(np.arcsinh(SHASH) * delta - epsilon)\n    elif likelihood == \"Normal\":\n        Z = (y - mu) / sigma\n    else:\n        raise ValueError(\"Unsupported likelihood\")\n    return Z\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.param","title":"<code>param</code>","text":"<p>Parameter handling module for Hierarchical Bayesian Regression models.</p> <p>This module provides the Param class which handles parameter definitions,  distributions, and transformations for hierarchical Bayesian regression models.  It supports linear parameters, random effects, and various probability distributions.</p> <p>The module integrates with PyMC for Bayesian modeling and provides utilities for parameter initialization, transformation, and serialization.</p>"},{"location":"api/#pcntoolkit.regression_model.hbr.param.Param","title":"<code>Param</code>  <code>dataclass</code>","text":"<p>A class representing parameters in hierarchical Bayesian regression models.</p> <p>This class handles parameter definitions including their distributions, linear relationships, random effects, and transformations. It supports both centered and non-centered parameterizations.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the parameter</p> required <code>dims</code> <code>Optional[Union[Tuple[str, ...], str]]</code> <p>Dimension names for the parameter, by default None</p> <code>None</code> <code>dist_name</code> <code>str</code> <p>Name of the probability distribution, by default \"Normal\"</p> <code>'Normal'</code> <code>dist_params</code> <code>tuple</code> <p>Parameters for the probability distribution, by default (0, 10.0)</p> <code>(0, 10.0)</code> <code>linear</code> <code>bool</code> <p>Whether parameter has linear relationship, by default False</p> <code>False</code> <code>slope</code> <code>Param</code> <p>Slope parameter for linear relationships, by default None</p> <code>None</code> <code>intercept</code> <code>Param</code> <p>Intercept parameter for linear relationships, by default None</p> <code>None</code> <code>mapping</code> <code>str</code> <p>Transformation mapping type, by default \"identity\"</p> <code>'identity'</code> <code>mapping_params</code> <code>tuple</code> <p>Parameters for the transformation mapping, by default (0, 1)</p> <code>(0, 1)</code> <code>random</code> <code>bool</code> <p>Whether parameter has random effects, by default False</p> <code>False</code> <code>centered</code> <code>bool</code> <p>Whether to use centered parameterization, by default False</p> <code>False</code> <code>mu</code> <code>Param</code> <p>Mean parameter for random effects, by default None</p> <code>None</code> <code>sigma</code> <code>Param</code> <p>Standard deviation parameter for random effects, by default None</p> <code>None</code> <code>freedom</code> <code>float</code> <p>Degrees of freedom parameter, by default 1.0</p> <code>1.0</code> <p>Attributes:</p> Name Type Description <code>has_covariate_dim</code> <code>bool</code> <p>Whether parameter has covariate dimensions</p> <code>has_random_effect</code> <code>bool</code> <p>Whether parameter has random effects</p> <code>distmap</code> <code>Dict[str, Any]</code> <p>Mapping of distribution names to PyMC distribution classes</p> <code>dist</code> <code>Any</code> <p>PyMC distribution object</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>@dataclass\nclass Param:\n    \"\"\"\n    A class representing parameters in hierarchical Bayesian regression models.\n\n    This class handles parameter definitions including their distributions,\n    linear relationships, random effects, and transformations. It supports\n    both centered and non-centered parameterizations.\n\n    Parameters\n    ----------\n    name : str\n        Name of the parameter\n    dims : Optional[Union[Tuple[str, ...], str]], optional\n        Dimension names for the parameter, by default None\n    dist_name : str, optional\n        Name of the probability distribution, by default \"Normal\"\n    dist_params : tuple, optional\n        Parameters for the probability distribution, by default (0, 10.0)\n    linear : bool, optional\n        Whether parameter has linear relationship, by default False\n    slope : Param, optional\n        Slope parameter for linear relationships, by default None\n    intercept : Param, optional\n        Intercept parameter for linear relationships, by default None\n    mapping : str, optional\n        Transformation mapping type, by default \"identity\"\n    mapping_params : tuple, optional\n        Parameters for the transformation mapping, by default (0, 1)\n    random : bool, optional\n        Whether parameter has random effects, by default False\n    centered : bool, optional\n        Whether to use centered parameterization, by default False\n    mu : Param, optional\n        Mean parameter for random effects, by default None\n    sigma : Param, optional\n        Standard deviation parameter for random effects, by default None\n    freedom : float, optional\n        Degrees of freedom parameter, by default 1.0\n\n    Attributes\n    ----------\n    has_covariate_dim : bool\n        Whether parameter has covariate dimensions\n    has_random_effect : bool\n        Whether parameter has random effects\n    distmap : Dict[str, Any]\n        Mapping of distribution names to PyMC distribution classes\n    dist : Any\n        PyMC distribution object\n    \"\"\"\n    name: str\n    dims: Optional[Union[Tuple[str, ...], str]] = None\n\n    dist_name: str = \"Normal\"\n    dist_params: tuple = (0, 10.0)\n\n    linear: bool = False\n    slope: Param = None  # type: ignore\n    intercept: Param = None  # type: ignore\n    mapping: str = \"identity\"\n    mapping_params: tuple = (0, 1)\n\n    random: bool = False\n    centered: bool = False\n    mu: Param = field(default=None)  # type: ignore\n    sigma: Param = field(default=None)  # type: ignore\n    offset: Param = field(default=None)  # type: ignore\n\n    has_covariate_dim: bool = field(init=False, default=False)\n    has_random_effect: bool = field(init=False, default=False)\n    distmap: Dict[str, Any] = field(init=False, default_factory=dict)\n    dist: Any = field(init=False, default=None)\n\n    freedom: float = 1.0\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"\n        Initialize parameter attributes after instance creation.\n\n        Validates parameter configuration and sets up appropriate parameter structure\n        based on whether it's linear, random, or basic parameter.\n\n        Raises\n        ------\n        ValueError\n            If slope parameter is missing required covariate dimension\n        \"\"\"\n        self.has_covariate_dim = False if not self.dims else \"covariates\" in self.dims\n        if self.name.startswith(\"slope\") and not self.has_covariate_dim:\n            raise ValueError(\n                f\"\"\"Parameter {self.name} must have a covariate dimension, but none was provided.\n                             Please provide a covariate dimension in the dims argument: dims=(\"covariates\",)\"\"\"\n            )\n        self.distmap = {\n            \"Normal\": pm.Normal,\n            \"Cauchy\": pm.Cauchy,\n            \"HalfNormal\": pm.HalfNormal,\n            \"HalfCauchy\": pm.HalfCauchy,\n            \"Uniform\": pm.Uniform,\n            \"Gamma\": pm.Gamma,\n            \"InvGamma\": pm.InverseGamma,\n            \"LogNormal\": pm.LogNormal,\n        }\n\n        if self.linear:\n            self.set_linear_params()\n            self.sample_dims = (\"datapoints\",)\n\n        elif self.random:\n            if self.centered:\n                self.set_centered_random_params()\n            else:\n                self.set_noncentered_random_params()\n            self.sample_dims = (\"datapoints\",)\n\n        else:\n            # If the parameter is really only a single number, we need to add an empty dimension so our outputs are always 2D\n            if (self.dims == ()) or (self.dims == []):\n                self.dims = None\n                self.shape = None\n            else:\n                self.shape = None\n                if isinstance(self.dims, str):\n                    self.dims = (self.dims,)\n            self.sample_dims = () #type: ignore\n\n        self.has_random_effect = (self.random and not self.linear) or (\n            self.linear and (self.slope.random or self.intercept.random)\n        )\n\n    def create_graph(\n        self, model: Any, idata: Optional[Any] = None, freedom: float = 1\n    ) -&gt; None:\n        \"\"\"\n        Create PyMC computational graph for the parameter.\n\n        Parameters\n        ----------\n        model : Any\n            PyMC model object\n        idata : Optional[Any], optional\n            Inference data for parameter initialization, by default None\n        freedom : float, optional\n            Degrees of freedom parameter, by default 1\n\n        Notes\n        -----\n        Creates appropriate PyMC variables based on parameter configuration\n        (linear, random, or basic) and adds them to the model graph.\n        \"\"\"\n        self.freedom = freedom\n        with model:\n            if self.linear:\n                self.slope.create_graph(model, idata, freedom)\n                self.intercept.create_graph(model, idata, freedom)\n            elif self.random:\n                if self.centered:\n                    self.mu.create_graph(model, idata, freedom)\n                    self.sigma.create_graph(model, idata, freedom)\n                    self.dist = pm.Normal(\n                        self.name,\n                        mu=self.mu.dist,\n                        sigma=self.sigma.dist,\n                        dims=(*model.custom_batch_effect_dims, *self._dims),\n                    )\n                else:\n                    self.mu.create_graph(model, idata, freedom)\n                    self.sigma.create_graph(model, idata, freedom)\n                    self.offset = pm.Normal(\n                        \"offset_\" + self.name,\n                        mu=0,\n                        sigma=1,\n                        dims=(*model.custom_batch_effect_dims, *self._dims),\n                    )\n                    self.dist = pm.Deterministic(\n                        self.name,\n                        self.mu.dist + self.offset * self.sigma.dist,\n                        dims=(*model.custom_batch_effect_dims, *self._dims),\n                    )\n            else:\n                if idata is not None:\n                    self.approximate_marginal(\n                        model,\n                        self.dist_name,\n                        az.extract(idata, var_names=self.name),\n                    )\n                self.dist = self.distmap[self.dist_name](\n                    self.name, *self.dist_params, shape=self.shape, dims=self.dims\n                )\n\n    def approximate_marginal(\n        self, model: Any, dist_name: str, samples: xr.DataArray\n    ) -&gt; None:\n        \"\"\"\n        Approximate marginal distribution parameters from MCMC samples.\n\n        Parameters\n        ----------\n        model : Any\n            PyMC model object\n        dist_name : str\n            Name of distribution to fit\n        samples : xr.DataArray\n            MCMC samples to fit distribution to\n\n        Raises\n        ------\n        ValueError\n            If distribution name is not recognized\n\n        Notes\n        -----\n        Uses scipy.stats to fit distribution parameters to the samples.\n        \"\"\"\n        \"\"\"#TODO At some point, we want to flatten over all dimensions except the covariate dimension.\"\"\"\n        print(\n            f\"Approximating factorized posterior for {self.name} with {dist_name} and freedom {self.freedom}\"\n        )\n        samples_flat = samples.to_numpy().flatten()\n        with model:\n            if dist_name == \"Normal\":\n                temp = stats.norm.fit(samples_flat)\n                self.dist_params = (temp[0], self.freedom * temp[1])\n            elif dist_name == \"HalfNormal\":\n                temp = stats.halfnorm.fit(samples_flat)\n                self.dist_params = (self.freedom * temp[1],)\n            elif dist_name == \"LogNormal\":\n                temp = stats.lognorm.fit(samples_flat)\n                self.dist_params = (temp[0], self.freedom * temp[1])\n            elif dist_name == \"Cauchy\":\n                temp = stats.cauchy.fit(samples_flat)\n                self.dist_params = (temp[0], self.freedom * temp[1])\n            elif dist_name == \"HalfCauchy\":\n                temp = stats.halfcauchy.fit(samples_flat)\n                self.dist_params = (self.freedom * temp[1],)\n            elif dist_name == \"Uniform\":\n                temp = stats.uniform.fit(samples_flat)\n                self.dist_params = (temp[0], temp[1])\n            elif dist_name == \"Gamma\":\n                temp = stats.gamma.fit(samples_flat)\n                self.dist_params = (temp[0], temp[1], self.freedom * temp[2])\n            elif dist_name == \"InvGamma\":\n                temp = stats.invgamma.fit(samples_flat)\n                self.dist_params = (temp[0], temp[1], self.freedom * temp[2])\n            else:\n                raise ValueError(f\"Unknown distribution name {dist_name}\")\n\n    def set_noncentered_random_params(self) -&gt; None:\n        \"\"\"\n        Set up non-centered parameterization for random effects.\n\n        Creates default mu and sigma parameters if they don't exist.\n        For non-centered parameterization, parameters are expressed as:\n        param = mu + sigma * offset, where offset ~ Normal(0,1)\n        \"\"\"\n        if not self.mu:\n            self.mu = Param.default_sub_mu(self.name, self.dims)\n            self.mu.name = f\"mu_{self.name}\"\n        if not self.sigma:\n            self.sigma = Param.default_sub_sigma(self.name, self.dims)\n            self.sigma = Param(\n                f\"sigma_{self.name}\",\n                dims=self.dims,\n                dist_name=\"LogNormal\",\n                dist_params=(2.0,),\n            )\n\n    def set_centered_random_params(self) -&gt; None:\n        \"\"\"\n        Set up centered parameterization for random effects.\n\n        Currently delegates to non-centered parameterization setup.\n        \"\"\"\n        self.set_noncentered_random_params()\n\n    def set_linear_params(self) -&gt; None:\n        \"\"\"\n        Set up parameters for linear relationships.\n\n        Creates default slope and intercept parameters if they don't exist.\n        \"\"\"\n        if not self.slope:\n            self.slope = Param.default_slope(self.name, (*self._dims, \"covariates\"))\n        if not self.intercept:\n            self.intercept = Param.default_intercept(self.name, self._dims)\n\n    def get_samples(self, data: HBRData) -&gt; Any:\n        \"\"\"\n        Generate samples from the parameter distribution.\n\n        Parameters\n        ----------\n        data : HBRData\n            Data object containing covariates and batch effects\n\n        Returns\n        -------\n        Any\n            PyMC distribution or transformed random variable\n        \"\"\"\n        if self.linear:\n            slope_samples = self.slope.get_samples(data)\n            intercept_samples = self.intercept.get_samples(data)\n            result = math.sum(slope_samples * data.pm_X, axis=1) + intercept_samples\n            return self.apply_mapping(result)\n\n        elif self.random:\n            return self.dist[data.pm_batch_effect_indices]\n        else:\n            return self.dist\n\n    def apply_mapping(self, x: Any) -&gt; Any:\n        \"\"\"\n        Apply transformation mapping to parameter values.\n\n        Parameters\n        ----------\n        x : Any\n            Input value to transform\n\n        Returns\n        -------\n        Any\n            Transformed value\n\n        Raises\n        ------\n        ValueError\n            If mapping type is not recognized\n        \"\"\"\n        a, b = self.mapping_params[0], self.mapping_params[1]\n        if self.mapping == \"identity\":\n            toreturn = x\n        elif self.mapping == \"exp\":\n            toreturn = math.exp(a + x / b) * b\n        elif self.mapping == \"softplus\":\n            toreturn = math.log(1 + math.exp(a + x / b)) * b #type: ignore\n        else:\n            raise ValueError(f\"Unknown mapping {self.mapping}\")\n        if len(self.mapping_params) &gt; 2:\n            toreturn = toreturn + self.mapping_params[2]\n        return toreturn\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Convert parameter configuration to dictionary format.\n\n        Returns\n        -------\n        Dict[str, Any]\n            Dictionary containing parameter configuration\n        \"\"\"\n        param_dict: dict[str, Any] = {\n            \"name\": self.name,\n            \"dims\": self.dims,\n            \"linear\": self.linear,\n            \"random\": self.random,\n            \"centered\": self.centered,\n            \"has_covariate_dim\": self.has_covariate_dim,\n            \"has_random_effect\": self.has_random_effect,\n        }\n        if self.linear:\n            param_dict[\"slope\"] = self.slope.to_dict()\n            param_dict[\"intercept\"] = self.intercept.to_dict()\n            param_dict[\"mapping\"] = self.mapping\n            param_dict[\"mapping_params\"] = self.mapping_params\n        elif self.random:\n            param_dict[\"mu\"] = self.mu.to_dict()\n            param_dict[\"sigma\"] = self.sigma.to_dict()\n        else:\n            param_dict[\"dist_name\"] = self.dist_name\n            param_dict[\"dist_params\"] = self.dist_params\n        return param_dict\n\n    @property\n    def _dims(self) -&gt; Tuple[str, ...]:\n        \"\"\"\n        Get parameter dimensions as tuple.\n\n        Returns\n        -------\n        Tuple[str, ...]\n            Parameter dimensions as tuple, empty tuple if no dimensions\n        \"\"\"\n        return self.dims if self.dims else () #type: ignore\n\n    @classmethod\n    def from_args(\n        cls,\n        name: str,\n        args: Dict[str, Any],\n        dims: Optional[Union[Tuple[str, ...], str]] = None,\n    ) -&gt; Param:\n        \"\"\"\n        Create parameter from command line arguments.\n\n        Parameters\n        ----------\n        name : str\n            Parameter name\n        args : Dict[str, Any]\n            Dictionary of arguments\n        dims : Optional[Union[Tuple[str, ...], str]], optional\n            Parameter dimensions, by default None\n\n        Returns\n        -------\n        Param\n            New parameter instance\n        \"\"\"\n        tupdims = dims if dims else ()\n        if args.get(f\"linear_{name}\", False):\n            slope = cls.from_args(f\"slope_{name}\", args, dims=(*tupdims, \"covariates\"))\n            intercept = cls.from_args(f\"intercept_{name}\", args, dims=dims)\n            return cls(\n                name,\n                dims=dims,\n                linear=True,\n                slope=slope,\n                intercept=intercept,\n                mapping=args.get(f\"mapping_{name}\", \"identity\"),\n                mapping_params=args.get(f\"mapping_params_{name}\", (0.0, 1.0)),\n            )\n        elif args.get(f\"random_{name}\", False):\n            if args.get(f\"centered_{name}\", False):\n                mu = cls.from_args(f\"mu_{name}\", args, dims=dims)\n                sigma = cls.from_args(f\"sigma_{name}\", args, dims=dims)\n                return cls(\n                    name, dims=dims, random=True, centered=True, mu=mu, sigma=sigma\n                )\n            else:\n                mu = cls.from_args(f\"mu_{name}\", args, dims=dims)\n                sigma = cls.from_args(f\"sigma_{name}\", args, dims=dims)\n                return cls(\n                    name, dims=dims, random=True, centered=False, mu=mu, sigma=sigma\n                )\n        else:\n            (default_dist, default_params) = (\n                (\"LogNormal\", (2.0,))\n                if (name.startswith(\"sigma\") or (name == \"delta\"))\n                else (\"Normal\", (0.0, 10.0))\n            )\n            return cls(\n                name,\n                dims=dims,\n                dist_name=args.get(f\"{name}_dist_name\", default_dist),\n                dist_params=args.get(f\"{name}_dist_params\", default_params),\n            )\n\n    @classmethod\n    def from_dict(cls, dict_: Dict[str, Any]) -&gt; Param:\n        \"\"\"\n        Create parameter from dictionary configuration.\n\n        Parameters\n        ----------\n        dict_ : Dict[str, Any]\n            Dictionary containing parameter configuration\n\n        Returns\n        -------\n        Param\n            New parameter instance\n        \"\"\"\n        if dict_.get(\"linear\", False):\n            slope = cls.from_dict(dict_[\"slope\"])\n            intercept = cls.from_dict(dict_[\"intercept\"])\n            return cls(\n                dict_[\"name\"],\n                dims=dict_[\"dims\"],\n                linear=True,\n                slope=slope,\n                intercept=intercept,\n                mapping=dict_.get(\"mapping\", \"identity\"),\n                mapping_params=dict_.get(\"mapping_params\", (0.0, 1.0)),\n            )\n        elif dict_.get(\"random\", False):\n            if dict_.get(\"centered\", False):\n                mu = cls.from_dict(dict_[\"mu\"])\n                sigma = cls.from_dict(dict_[\"sigma\"])\n                return cls(\n                    dict_[\"name\"],\n                    dims=dict_[\"dims\"],\n                    random=True,\n                    centered=True,\n                    mu=mu,\n                    sigma=sigma,\n                )\n            else:\n                mu = cls.from_dict(dict_[\"mu\"])\n                sigma = cls.from_dict(dict_[\"sigma\"])\n                return cls(\n                    dict_[\"name\"],\n                    dims=dict_[\"dims\"],\n                    random=True,\n                    centered=False,\n                    mu=mu,\n                    sigma=sigma,\n                )\n        else:\n            name = dict_[\"name\"]\n            if name.startswith(\"sigma\") or name == \"delta\":\n                default_dist = \"LogNormal\"\n                default_params: tuple[float, ...] = (2.0,)\n            else:\n                default_dist = \"Normal\"\n                default_params= (0.0, 10.0)\n            return cls(\n                dict_[\"name\"],\n                dims=dict_[\"dims\"],\n                dist_name=default_dist,\n                dist_params=default_params,\n            )\n\n    @classmethod\n    def default_mu(cls) -&gt; Param:\n        \"\"\"\n        Create default mean parameter.\n\n        Returns\n        -------\n        Param\n            Default mu parameter with linear relationship\n        \"\"\"\n        slope = cls.default_slope(\"mu\")\n        intercept = cls.default_intercept(\"mu\")\n        return cls(\n            \"mu\",\n            linear=True,\n            slope=slope,\n            intercept=intercept,\n        )\n\n    @classmethod\n    def default_sigma(cls) -&gt; Param:\n        \"\"\"\n        Create default standard deviation parameter.\n\n        Returns\n        -------\n        Param\n            Default sigma parameter with linear relationship and softplus mapping\n        \"\"\"\n        slope = cls.default_slope(\"sigma\")\n        intercept = cls.default_intercept(\"sigma\")\n        return cls(\n            \"sigma\",\n            linear=True,\n            slope=slope,\n            intercept=intercept,\n            mapping=\"softplus\",\n            mapping_params=(0.0, 10.0),\n        )\n\n    @classmethod\n    def default_epsilon(cls) -&gt; Param:\n        \"\"\"\n        Create default epsilon (error) parameter.\n\n        Returns\n        -------\n        Param\n            Default epsilon parameter with normal distribution\n        \"\"\"\n        return cls(\n            \"epsilon\",\n            linear=False,\n            random=False,\n            dist_name=\"Normal\",\n            dist_params=(0.0, 2.0,),\n        )\n\n    @classmethod\n    def default_delta(cls) -&gt; Param:\n        \"\"\"\n        Create default delta parameter.\n\n        Returns\n        -------\n        Param\n            Default delta parameter with normal distribution and softplus mapping\n        \"\"\"\n        return cls(\n            \"delta\",\n            linear=False,\n            random=False,\n            dist_name=\"Normal\",\n            dist_params=(0.0, 2.0,),\n            mapping=\"softplus\",\n            mapping_params=(0.0, 3.0, 0.3),\n        )\n\n    @classmethod\n    def default_slope(\n        cls, name: str, dims: Union[Tuple[str, ...], str] = (\"covariates\",)\n    ) -&gt; Param:\n        \"\"\"\n        Create default slope parameter.\n\n        Parameters\n        ----------\n        name : str\n            Base name for slope parameter\n        dims : Union[Tuple[str, ...], str], optional\n            Parameter dimensions, by default (\"covariates\",)\n\n        Returns\n        -------\n        Param\n            Default slope parameter with normal distribution\n        \"\"\"\n        return cls(\n            f\"slope_{name}\",\n            linear=False,\n            random=False,\n            dims=dims,\n            dist_name=\"Normal\",\n            dist_params=(0.0, 10.0,),\n        )\n\n    @classmethod\n    def default_intercept(\n        cls, name: str, dims: Optional[Union[Tuple[str, ...], str]] = None\n    ) -&gt; Param:\n        \"\"\"\n        Create default intercept parameter.\n\n        Parameters\n        ----------\n        name : str\n            Base name for intercept parameter\n        dims : Optional[Union[Tuple[str, ...], str]], optional\n            Parameter dimensions, by default None\n\n        Returns\n        -------\n        Param\n            Default intercept parameter with normal distribution\n        \"\"\"\n        return cls(\n            f\"intercept_{name}\",\n            linear=False,\n            random=False,\n            dims=dims,\n            dist_name=\"Normal\",\n            dist_params=(0.0, 10.0,),\n        )\n\n    @classmethod\n    def default_sub_mu(\n        cls, name: str, dims: Optional[Union[Tuple[str, ...], str]] = None\n    ) -&gt; Param:\n        \"\"\"\n        Create default sub-model mean parameter.\n\n        Parameters\n        ----------\n        name : str\n            Base name for mu parameter\n        dims : Optional[Union[Tuple[str, ...], str]], optional\n            Parameter dimensions, by default None\n\n        Returns\n        -------\n        Param\n            Default sub-model mu parameter with normal distribution\n        \"\"\"\n        return cls(\n            f\"mu_{name}\",\n            linear=False,\n            random=False,\n            dims=dims,\n            dist_name=\"Normal\",\n            dist_params=(0.0, 10.0,),\n        )\n\n    @classmethod\n    def default_sub_sigma(\n        cls, name: str, dims: Optional[Union[Tuple[str, ...], str]] = None\n    ) -&gt; Param:\n        \"\"\"\n        Create default sub-model standard deviation parameter.\n\n        Parameters\n        ----------\n        name : str\n            Base name for sigma parameter\n        dims : Optional[Union[Tuple[str, ...], str]], optional\n            Parameter dimensions, by default None\n\n        Returns\n        -------\n        Param\n            Default sub-model sigma parameter with lognormal distribution\n        \"\"\"\n        return cls(\n            f\"sigma_{name}\",\n            linear=False,\n            random=False,\n            dims=dims,\n            dist_name=\"LogNormal\",\n            dist_params=(2.0,),\n        )\n</code></pre> <code>__post_init__() -&gt; None</code> \u00b6 <p>Initialize parameter attributes after instance creation.</p> <p>Validates parameter configuration and sets up appropriate parameter structure based on whether it's linear, random, or basic parameter.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If slope parameter is missing required covariate dimension</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"\n    Initialize parameter attributes after instance creation.\n\n    Validates parameter configuration and sets up appropriate parameter structure\n    based on whether it's linear, random, or basic parameter.\n\n    Raises\n    ------\n    ValueError\n        If slope parameter is missing required covariate dimension\n    \"\"\"\n    self.has_covariate_dim = False if not self.dims else \"covariates\" in self.dims\n    if self.name.startswith(\"slope\") and not self.has_covariate_dim:\n        raise ValueError(\n            f\"\"\"Parameter {self.name} must have a covariate dimension, but none was provided.\n                         Please provide a covariate dimension in the dims argument: dims=(\"covariates\",)\"\"\"\n        )\n    self.distmap = {\n        \"Normal\": pm.Normal,\n        \"Cauchy\": pm.Cauchy,\n        \"HalfNormal\": pm.HalfNormal,\n        \"HalfCauchy\": pm.HalfCauchy,\n        \"Uniform\": pm.Uniform,\n        \"Gamma\": pm.Gamma,\n        \"InvGamma\": pm.InverseGamma,\n        \"LogNormal\": pm.LogNormal,\n    }\n\n    if self.linear:\n        self.set_linear_params()\n        self.sample_dims = (\"datapoints\",)\n\n    elif self.random:\n        if self.centered:\n            self.set_centered_random_params()\n        else:\n            self.set_noncentered_random_params()\n        self.sample_dims = (\"datapoints\",)\n\n    else:\n        # If the parameter is really only a single number, we need to add an empty dimension so our outputs are always 2D\n        if (self.dims == ()) or (self.dims == []):\n            self.dims = None\n            self.shape = None\n        else:\n            self.shape = None\n            if isinstance(self.dims, str):\n                self.dims = (self.dims,)\n        self.sample_dims = () #type: ignore\n\n    self.has_random_effect = (self.random and not self.linear) or (\n        self.linear and (self.slope.random or self.intercept.random)\n    )\n</code></pre> <code>apply_mapping(x: Any) -&gt; Any</code> \u00b6 <p>Apply transformation mapping to parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>Input value to transform</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Transformed value</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If mapping type is not recognized</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>def apply_mapping(self, x: Any) -&gt; Any:\n    \"\"\"\n    Apply transformation mapping to parameter values.\n\n    Parameters\n    ----------\n    x : Any\n        Input value to transform\n\n    Returns\n    -------\n    Any\n        Transformed value\n\n    Raises\n    ------\n    ValueError\n        If mapping type is not recognized\n    \"\"\"\n    a, b = self.mapping_params[0], self.mapping_params[1]\n    if self.mapping == \"identity\":\n        toreturn = x\n    elif self.mapping == \"exp\":\n        toreturn = math.exp(a + x / b) * b\n    elif self.mapping == \"softplus\":\n        toreturn = math.log(1 + math.exp(a + x / b)) * b #type: ignore\n    else:\n        raise ValueError(f\"Unknown mapping {self.mapping}\")\n    if len(self.mapping_params) &gt; 2:\n        toreturn = toreturn + self.mapping_params[2]\n    return toreturn\n</code></pre> <code>approximate_marginal(model: Any, dist_name: str, samples: xr.DataArray) -&gt; None</code> \u00b6 <p>Approximate marginal distribution parameters from MCMC samples.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>PyMC model object</p> required <code>dist_name</code> <code>str</code> <p>Name of distribution to fit</p> required <code>samples</code> <code>DataArray</code> <p>MCMC samples to fit distribution to</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If distribution name is not recognized</p> Notes <p>Uses scipy.stats to fit distribution parameters to the samples.</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>def approximate_marginal(\n    self, model: Any, dist_name: str, samples: xr.DataArray\n) -&gt; None:\n    \"\"\"\n    Approximate marginal distribution parameters from MCMC samples.\n\n    Parameters\n    ----------\n    model : Any\n        PyMC model object\n    dist_name : str\n        Name of distribution to fit\n    samples : xr.DataArray\n        MCMC samples to fit distribution to\n\n    Raises\n    ------\n    ValueError\n        If distribution name is not recognized\n\n    Notes\n    -----\n    Uses scipy.stats to fit distribution parameters to the samples.\n    \"\"\"\n    \"\"\"#TODO At some point, we want to flatten over all dimensions except the covariate dimension.\"\"\"\n    print(\n        f\"Approximating factorized posterior for {self.name} with {dist_name} and freedom {self.freedom}\"\n    )\n    samples_flat = samples.to_numpy().flatten()\n    with model:\n        if dist_name == \"Normal\":\n            temp = stats.norm.fit(samples_flat)\n            self.dist_params = (temp[0], self.freedom * temp[1])\n        elif dist_name == \"HalfNormal\":\n            temp = stats.halfnorm.fit(samples_flat)\n            self.dist_params = (self.freedom * temp[1],)\n        elif dist_name == \"LogNormal\":\n            temp = stats.lognorm.fit(samples_flat)\n            self.dist_params = (temp[0], self.freedom * temp[1])\n        elif dist_name == \"Cauchy\":\n            temp = stats.cauchy.fit(samples_flat)\n            self.dist_params = (temp[0], self.freedom * temp[1])\n        elif dist_name == \"HalfCauchy\":\n            temp = stats.halfcauchy.fit(samples_flat)\n            self.dist_params = (self.freedom * temp[1],)\n        elif dist_name == \"Uniform\":\n            temp = stats.uniform.fit(samples_flat)\n            self.dist_params = (temp[0], temp[1])\n        elif dist_name == \"Gamma\":\n            temp = stats.gamma.fit(samples_flat)\n            self.dist_params = (temp[0], temp[1], self.freedom * temp[2])\n        elif dist_name == \"InvGamma\":\n            temp = stats.invgamma.fit(samples_flat)\n            self.dist_params = (temp[0], temp[1], self.freedom * temp[2])\n        else:\n            raise ValueError(f\"Unknown distribution name {dist_name}\")\n</code></pre> <code>create_graph(model: Any, idata: Optional[Any] = None, freedom: float = 1) -&gt; None</code> \u00b6 <p>Create PyMC computational graph for the parameter.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>PyMC model object</p> required <code>idata</code> <code>Optional[Any]</code> <p>Inference data for parameter initialization, by default None</p> <code>None</code> <code>freedom</code> <code>float</code> <p>Degrees of freedom parameter, by default 1</p> <code>1</code> Notes <p>Creates appropriate PyMC variables based on parameter configuration (linear, random, or basic) and adds them to the model graph.</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>def create_graph(\n    self, model: Any, idata: Optional[Any] = None, freedom: float = 1\n) -&gt; None:\n    \"\"\"\n    Create PyMC computational graph for the parameter.\n\n    Parameters\n    ----------\n    model : Any\n        PyMC model object\n    idata : Optional[Any], optional\n        Inference data for parameter initialization, by default None\n    freedom : float, optional\n        Degrees of freedom parameter, by default 1\n\n    Notes\n    -----\n    Creates appropriate PyMC variables based on parameter configuration\n    (linear, random, or basic) and adds them to the model graph.\n    \"\"\"\n    self.freedom = freedom\n    with model:\n        if self.linear:\n            self.slope.create_graph(model, idata, freedom)\n            self.intercept.create_graph(model, idata, freedom)\n        elif self.random:\n            if self.centered:\n                self.mu.create_graph(model, idata, freedom)\n                self.sigma.create_graph(model, idata, freedom)\n                self.dist = pm.Normal(\n                    self.name,\n                    mu=self.mu.dist,\n                    sigma=self.sigma.dist,\n                    dims=(*model.custom_batch_effect_dims, *self._dims),\n                )\n            else:\n                self.mu.create_graph(model, idata, freedom)\n                self.sigma.create_graph(model, idata, freedom)\n                self.offset = pm.Normal(\n                    \"offset_\" + self.name,\n                    mu=0,\n                    sigma=1,\n                    dims=(*model.custom_batch_effect_dims, *self._dims),\n                )\n                self.dist = pm.Deterministic(\n                    self.name,\n                    self.mu.dist + self.offset * self.sigma.dist,\n                    dims=(*model.custom_batch_effect_dims, *self._dims),\n                )\n        else:\n            if idata is not None:\n                self.approximate_marginal(\n                    model,\n                    self.dist_name,\n                    az.extract(idata, var_names=self.name),\n                )\n            self.dist = self.distmap[self.dist_name](\n                self.name, *self.dist_params, shape=self.shape, dims=self.dims\n            )\n</code></pre> <code>default_delta() -&gt; Param</code> <code>classmethod</code> \u00b6 <p>Create default delta parameter.</p> <p>Returns:</p> Type Description <code>Param</code> <p>Default delta parameter with normal distribution and softplus mapping</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>@classmethod\ndef default_delta(cls) -&gt; Param:\n    \"\"\"\n    Create default delta parameter.\n\n    Returns\n    -------\n    Param\n        Default delta parameter with normal distribution and softplus mapping\n    \"\"\"\n    return cls(\n        \"delta\",\n        linear=False,\n        random=False,\n        dist_name=\"Normal\",\n        dist_params=(0.0, 2.0,),\n        mapping=\"softplus\",\n        mapping_params=(0.0, 3.0, 0.3),\n    )\n</code></pre> <code>default_epsilon() -&gt; Param</code> <code>classmethod</code> \u00b6 <p>Create default epsilon (error) parameter.</p> <p>Returns:</p> Type Description <code>Param</code> <p>Default epsilon parameter with normal distribution</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>@classmethod\ndef default_epsilon(cls) -&gt; Param:\n    \"\"\"\n    Create default epsilon (error) parameter.\n\n    Returns\n    -------\n    Param\n        Default epsilon parameter with normal distribution\n    \"\"\"\n    return cls(\n        \"epsilon\",\n        linear=False,\n        random=False,\n        dist_name=\"Normal\",\n        dist_params=(0.0, 2.0,),\n    )\n</code></pre> <code>default_intercept(name: str, dims: Optional[Union[Tuple[str, ...], str]] = None) -&gt; Param</code> <code>classmethod</code> \u00b6 <p>Create default intercept parameter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Base name for intercept parameter</p> required <code>dims</code> <code>Optional[Union[Tuple[str, ...], str]]</code> <p>Parameter dimensions, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Param</code> <p>Default intercept parameter with normal distribution</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>@classmethod\ndef default_intercept(\n    cls, name: str, dims: Optional[Union[Tuple[str, ...], str]] = None\n) -&gt; Param:\n    \"\"\"\n    Create default intercept parameter.\n\n    Parameters\n    ----------\n    name : str\n        Base name for intercept parameter\n    dims : Optional[Union[Tuple[str, ...], str]], optional\n        Parameter dimensions, by default None\n\n    Returns\n    -------\n    Param\n        Default intercept parameter with normal distribution\n    \"\"\"\n    return cls(\n        f\"intercept_{name}\",\n        linear=False,\n        random=False,\n        dims=dims,\n        dist_name=\"Normal\",\n        dist_params=(0.0, 10.0,),\n    )\n</code></pre> <code>default_mu() -&gt; Param</code> <code>classmethod</code> \u00b6 <p>Create default mean parameter.</p> <p>Returns:</p> Type Description <code>Param</code> <p>Default mu parameter with linear relationship</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>@classmethod\ndef default_mu(cls) -&gt; Param:\n    \"\"\"\n    Create default mean parameter.\n\n    Returns\n    -------\n    Param\n        Default mu parameter with linear relationship\n    \"\"\"\n    slope = cls.default_slope(\"mu\")\n    intercept = cls.default_intercept(\"mu\")\n    return cls(\n        \"mu\",\n        linear=True,\n        slope=slope,\n        intercept=intercept,\n    )\n</code></pre> <code>default_sigma() -&gt; Param</code> <code>classmethod</code> \u00b6 <p>Create default standard deviation parameter.</p> <p>Returns:</p> Type Description <code>Param</code> <p>Default sigma parameter with linear relationship and softplus mapping</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>@classmethod\ndef default_sigma(cls) -&gt; Param:\n    \"\"\"\n    Create default standard deviation parameter.\n\n    Returns\n    -------\n    Param\n        Default sigma parameter with linear relationship and softplus mapping\n    \"\"\"\n    slope = cls.default_slope(\"sigma\")\n    intercept = cls.default_intercept(\"sigma\")\n    return cls(\n        \"sigma\",\n        linear=True,\n        slope=slope,\n        intercept=intercept,\n        mapping=\"softplus\",\n        mapping_params=(0.0, 10.0),\n    )\n</code></pre> <code>default_slope(name: str, dims: Union[Tuple[str, ...], str] = ('covariates')) -&gt; Param</code> <code>classmethod</code> \u00b6 <p>Create default slope parameter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Base name for slope parameter</p> required <code>dims</code> <code>Union[Tuple[str, ...], str]</code> <p>Parameter dimensions, by default (\"covariates\",)</p> <code>('covariates')</code> <p>Returns:</p> Type Description <code>Param</code> <p>Default slope parameter with normal distribution</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>@classmethod\ndef default_slope(\n    cls, name: str, dims: Union[Tuple[str, ...], str] = (\"covariates\",)\n) -&gt; Param:\n    \"\"\"\n    Create default slope parameter.\n\n    Parameters\n    ----------\n    name : str\n        Base name for slope parameter\n    dims : Union[Tuple[str, ...], str], optional\n        Parameter dimensions, by default (\"covariates\",)\n\n    Returns\n    -------\n    Param\n        Default slope parameter with normal distribution\n    \"\"\"\n    return cls(\n        f\"slope_{name}\",\n        linear=False,\n        random=False,\n        dims=dims,\n        dist_name=\"Normal\",\n        dist_params=(0.0, 10.0,),\n    )\n</code></pre> <code>default_sub_mu(name: str, dims: Optional[Union[Tuple[str, ...], str]] = None) -&gt; Param</code> <code>classmethod</code> \u00b6 <p>Create default sub-model mean parameter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Base name for mu parameter</p> required <code>dims</code> <code>Optional[Union[Tuple[str, ...], str]]</code> <p>Parameter dimensions, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Param</code> <p>Default sub-model mu parameter with normal distribution</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>@classmethod\ndef default_sub_mu(\n    cls, name: str, dims: Optional[Union[Tuple[str, ...], str]] = None\n) -&gt; Param:\n    \"\"\"\n    Create default sub-model mean parameter.\n\n    Parameters\n    ----------\n    name : str\n        Base name for mu parameter\n    dims : Optional[Union[Tuple[str, ...], str]], optional\n        Parameter dimensions, by default None\n\n    Returns\n    -------\n    Param\n        Default sub-model mu parameter with normal distribution\n    \"\"\"\n    return cls(\n        f\"mu_{name}\",\n        linear=False,\n        random=False,\n        dims=dims,\n        dist_name=\"Normal\",\n        dist_params=(0.0, 10.0,),\n    )\n</code></pre> <code>default_sub_sigma(name: str, dims: Optional[Union[Tuple[str, ...], str]] = None) -&gt; Param</code> <code>classmethod</code> \u00b6 <p>Create default sub-model standard deviation parameter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Base name for sigma parameter</p> required <code>dims</code> <code>Optional[Union[Tuple[str, ...], str]]</code> <p>Parameter dimensions, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Param</code> <p>Default sub-model sigma parameter with lognormal distribution</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>@classmethod\ndef default_sub_sigma(\n    cls, name: str, dims: Optional[Union[Tuple[str, ...], str]] = None\n) -&gt; Param:\n    \"\"\"\n    Create default sub-model standard deviation parameter.\n\n    Parameters\n    ----------\n    name : str\n        Base name for sigma parameter\n    dims : Optional[Union[Tuple[str, ...], str]], optional\n        Parameter dimensions, by default None\n\n    Returns\n    -------\n    Param\n        Default sub-model sigma parameter with lognormal distribution\n    \"\"\"\n    return cls(\n        f\"sigma_{name}\",\n        linear=False,\n        random=False,\n        dims=dims,\n        dist_name=\"LogNormal\",\n        dist_params=(2.0,),\n    )\n</code></pre> <code>from_args(name: str, args: Dict[str, Any], dims: Optional[Union[Tuple[str, ...], str]] = None) -&gt; Param</code> <code>classmethod</code> \u00b6 <p>Create parameter from command line arguments.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <code>args</code> <code>Dict[str, Any]</code> <p>Dictionary of arguments</p> required <code>dims</code> <code>Optional[Union[Tuple[str, ...], str]]</code> <p>Parameter dimensions, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Param</code> <p>New parameter instance</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>@classmethod\ndef from_args(\n    cls,\n    name: str,\n    args: Dict[str, Any],\n    dims: Optional[Union[Tuple[str, ...], str]] = None,\n) -&gt; Param:\n    \"\"\"\n    Create parameter from command line arguments.\n\n    Parameters\n    ----------\n    name : str\n        Parameter name\n    args : Dict[str, Any]\n        Dictionary of arguments\n    dims : Optional[Union[Tuple[str, ...], str]], optional\n        Parameter dimensions, by default None\n\n    Returns\n    -------\n    Param\n        New parameter instance\n    \"\"\"\n    tupdims = dims if dims else ()\n    if args.get(f\"linear_{name}\", False):\n        slope = cls.from_args(f\"slope_{name}\", args, dims=(*tupdims, \"covariates\"))\n        intercept = cls.from_args(f\"intercept_{name}\", args, dims=dims)\n        return cls(\n            name,\n            dims=dims,\n            linear=True,\n            slope=slope,\n            intercept=intercept,\n            mapping=args.get(f\"mapping_{name}\", \"identity\"),\n            mapping_params=args.get(f\"mapping_params_{name}\", (0.0, 1.0)),\n        )\n    elif args.get(f\"random_{name}\", False):\n        if args.get(f\"centered_{name}\", False):\n            mu = cls.from_args(f\"mu_{name}\", args, dims=dims)\n            sigma = cls.from_args(f\"sigma_{name}\", args, dims=dims)\n            return cls(\n                name, dims=dims, random=True, centered=True, mu=mu, sigma=sigma\n            )\n        else:\n            mu = cls.from_args(f\"mu_{name}\", args, dims=dims)\n            sigma = cls.from_args(f\"sigma_{name}\", args, dims=dims)\n            return cls(\n                name, dims=dims, random=True, centered=False, mu=mu, sigma=sigma\n            )\n    else:\n        (default_dist, default_params) = (\n            (\"LogNormal\", (2.0,))\n            if (name.startswith(\"sigma\") or (name == \"delta\"))\n            else (\"Normal\", (0.0, 10.0))\n        )\n        return cls(\n            name,\n            dims=dims,\n            dist_name=args.get(f\"{name}_dist_name\", default_dist),\n            dist_params=args.get(f\"{name}_dist_params\", default_params),\n        )\n</code></pre> <code>from_dict(dict_: Dict[str, Any]) -&gt; Param</code> <code>classmethod</code> \u00b6 <p>Create parameter from dictionary configuration.</p> <p>Parameters:</p> Name Type Description Default <code>dict_</code> <code>Dict[str, Any]</code> <p>Dictionary containing parameter configuration</p> required <p>Returns:</p> Type Description <code>Param</code> <p>New parameter instance</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>@classmethod\ndef from_dict(cls, dict_: Dict[str, Any]) -&gt; Param:\n    \"\"\"\n    Create parameter from dictionary configuration.\n\n    Parameters\n    ----------\n    dict_ : Dict[str, Any]\n        Dictionary containing parameter configuration\n\n    Returns\n    -------\n    Param\n        New parameter instance\n    \"\"\"\n    if dict_.get(\"linear\", False):\n        slope = cls.from_dict(dict_[\"slope\"])\n        intercept = cls.from_dict(dict_[\"intercept\"])\n        return cls(\n            dict_[\"name\"],\n            dims=dict_[\"dims\"],\n            linear=True,\n            slope=slope,\n            intercept=intercept,\n            mapping=dict_.get(\"mapping\", \"identity\"),\n            mapping_params=dict_.get(\"mapping_params\", (0.0, 1.0)),\n        )\n    elif dict_.get(\"random\", False):\n        if dict_.get(\"centered\", False):\n            mu = cls.from_dict(dict_[\"mu\"])\n            sigma = cls.from_dict(dict_[\"sigma\"])\n            return cls(\n                dict_[\"name\"],\n                dims=dict_[\"dims\"],\n                random=True,\n                centered=True,\n                mu=mu,\n                sigma=sigma,\n            )\n        else:\n            mu = cls.from_dict(dict_[\"mu\"])\n            sigma = cls.from_dict(dict_[\"sigma\"])\n            return cls(\n                dict_[\"name\"],\n                dims=dict_[\"dims\"],\n                random=True,\n                centered=False,\n                mu=mu,\n                sigma=sigma,\n            )\n    else:\n        name = dict_[\"name\"]\n        if name.startswith(\"sigma\") or name == \"delta\":\n            default_dist = \"LogNormal\"\n            default_params: tuple[float, ...] = (2.0,)\n        else:\n            default_dist = \"Normal\"\n            default_params= (0.0, 10.0)\n        return cls(\n            dict_[\"name\"],\n            dims=dict_[\"dims\"],\n            dist_name=default_dist,\n            dist_params=default_params,\n        )\n</code></pre> <code>get_samples(data: HBRData) -&gt; Any</code> \u00b6 <p>Generate samples from the parameter distribution.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>HBRData</code> <p>Data object containing covariates and batch effects</p> required <p>Returns:</p> Type Description <code>Any</code> <p>PyMC distribution or transformed random variable</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>def get_samples(self, data: HBRData) -&gt; Any:\n    \"\"\"\n    Generate samples from the parameter distribution.\n\n    Parameters\n    ----------\n    data : HBRData\n        Data object containing covariates and batch effects\n\n    Returns\n    -------\n    Any\n        PyMC distribution or transformed random variable\n    \"\"\"\n    if self.linear:\n        slope_samples = self.slope.get_samples(data)\n        intercept_samples = self.intercept.get_samples(data)\n        result = math.sum(slope_samples * data.pm_X, axis=1) + intercept_samples\n        return self.apply_mapping(result)\n\n    elif self.random:\n        return self.dist[data.pm_batch_effect_indices]\n    else:\n        return self.dist\n</code></pre> <code>set_centered_random_params() -&gt; None</code> \u00b6 <p>Set up centered parameterization for random effects.</p> <p>Currently delegates to non-centered parameterization setup.</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>def set_centered_random_params(self) -&gt; None:\n    \"\"\"\n    Set up centered parameterization for random effects.\n\n    Currently delegates to non-centered parameterization setup.\n    \"\"\"\n    self.set_noncentered_random_params()\n</code></pre> <code>set_linear_params() -&gt; None</code> \u00b6 <p>Set up parameters for linear relationships.</p> <p>Creates default slope and intercept parameters if they don't exist.</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>def set_linear_params(self) -&gt; None:\n    \"\"\"\n    Set up parameters for linear relationships.\n\n    Creates default slope and intercept parameters if they don't exist.\n    \"\"\"\n    if not self.slope:\n        self.slope = Param.default_slope(self.name, (*self._dims, \"covariates\"))\n    if not self.intercept:\n        self.intercept = Param.default_intercept(self.name, self._dims)\n</code></pre> <code>set_noncentered_random_params() -&gt; None</code> \u00b6 <p>Set up non-centered parameterization for random effects.</p> <p>Creates default mu and sigma parameters if they don't exist. For non-centered parameterization, parameters are expressed as: param = mu + sigma * offset, where offset ~ Normal(0,1)</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>def set_noncentered_random_params(self) -&gt; None:\n    \"\"\"\n    Set up non-centered parameterization for random effects.\n\n    Creates default mu and sigma parameters if they don't exist.\n    For non-centered parameterization, parameters are expressed as:\n    param = mu + sigma * offset, where offset ~ Normal(0,1)\n    \"\"\"\n    if not self.mu:\n        self.mu = Param.default_sub_mu(self.name, self.dims)\n        self.mu.name = f\"mu_{self.name}\"\n    if not self.sigma:\n        self.sigma = Param.default_sub_sigma(self.name, self.dims)\n        self.sigma = Param(\n            f\"sigma_{self.name}\",\n            dims=self.dims,\n            dist_name=\"LogNormal\",\n            dist_params=(2.0,),\n        )\n</code></pre> <code>to_dict() -&gt; Dict[str, Any]</code> \u00b6 <p>Convert parameter configuration to dictionary format.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing parameter configuration</p> Source code in <code>pcntoolkit/regression_model/hbr/param.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert parameter configuration to dictionary format.\n\n    Returns\n    -------\n    Dict[str, Any]\n        Dictionary containing parameter configuration\n    \"\"\"\n    param_dict: dict[str, Any] = {\n        \"name\": self.name,\n        \"dims\": self.dims,\n        \"linear\": self.linear,\n        \"random\": self.random,\n        \"centered\": self.centered,\n        \"has_covariate_dim\": self.has_covariate_dim,\n        \"has_random_effect\": self.has_random_effect,\n    }\n    if self.linear:\n        param_dict[\"slope\"] = self.slope.to_dict()\n        param_dict[\"intercept\"] = self.intercept.to_dict()\n        param_dict[\"mapping\"] = self.mapping\n        param_dict[\"mapping_params\"] = self.mapping_params\n    elif self.random:\n        param_dict[\"mu\"] = self.mu.to_dict()\n        param_dict[\"sigma\"] = self.sigma.to_dict()\n    else:\n        param_dict[\"dist_name\"] = self.dist_name\n        param_dict[\"dist_params\"] = self.dist_params\n    return param_dict\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.shash","title":"<code>shash</code>","text":"<p>Sinh-Arcsinh (SHASH) Distribution Implementation Module.</p> <p>This module implements the Sinh-Arcsinh (SHASH) distribution and its variants as described in Jones and Pewsey (2009) [1]_. The SHASH distribution is a flexible distribution family that can model skewness and kurtosis through separate parameters.</p> <p>The module provides:</p> <ol> <li> <p>Basic SHASH transformations (S, S_inv, C)</p> </li> <li> <p>SHASH distribution (base implementation)</p> </li> <li> <p>SHASHo distribution (location-scale variant)</p> </li> <li> <p>SHASHo2 distribution (alternative parameterization)</p> </li> <li> <p>SHASHb distribution (standardized variant)</p> </li> </ol> References <p>.. [1] Jones, M. C., &amp; Pewsey, A. (2009). Sinh-arcsinh distributions. Biometrika, 96(4), 761-780.        https://doi.org/10.1093/biomet/asp053</p> Notes <p>The implementation uses PyMC and PyTensor for probabilistic programming capabilities. All distributions support random sampling and log-probability calculations.</p>"},{"location":"api/#pcntoolkit.regression_model.hbr.shash.CONST1","title":"<code>CONST1 = np.exp(0.25) / np.power(8.0 * np.pi, 0.5)</code>  <code>module-attribute</code>","text":"<p>Constant used in P function calculations.</p>"},{"location":"api/#pcntoolkit.regression_model.hbr.shash.CONST2","title":"<code>CONST2 = -np.log(2 * np.pi) / 2</code>  <code>module-attribute</code>","text":"<p>Constant used in log-probability calculations.</p>"},{"location":"api/#pcntoolkit.regression_model.hbr.shash.SHASH","title":"<code>SHASH</code>","text":"<p>               Bases: <code>Continuous</code></p> <p>Sinh-arcsinh distribution based on standard normal.</p> <p>A flexible distribution family that extends the normal distribution by adding skewness and kurtosis parameters while maintaining many desirable properties.</p> <p>Parameters:</p> Name Type Description Default <code>epsilon</code> <code>float</code> <p>Skewness parameter controlling asymmetry</p> required <code>delta</code> <code>float</code> <p>Kurtosis parameter controlling tail weight</p> required Notes <p>The distribution reduces to standard normal when epsilon=0 and delta=1. Positive epsilon produces positive skewness. Delta &lt; 1 produces heavier tails than normal.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pymc as pm\n&gt;&gt;&gt; with pm.Model():\n...     x = pm.SHASH('x', epsilon=0.5, delta=1.2)\n</code></pre> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>class SHASH(Continuous):\n    \"\"\"Sinh-arcsinh distribution based on standard normal.\n\n    A flexible distribution family that extends the normal distribution by adding\n    skewness and kurtosis parameters while maintaining many desirable properties.\n\n    Parameters\n    ----------\n    epsilon : float\n        Skewness parameter controlling asymmetry\n    delta : float\n        Kurtosis parameter controlling tail weight\n\n    Notes\n    -----\n    The distribution reduces to standard normal when epsilon=0 and delta=1.\n    Positive epsilon produces positive skewness.\n    Delta &lt; 1 produces heavier tails than normal.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pymc as pm\n    &gt;&gt;&gt; with pm.Model():\n    ...     x = pm.SHASH('x', epsilon=0.5, delta=1.2)\n    \"\"\"\n\n    rv_op = shash\n    my_K = Elemwise(knuop)\n\n    @staticmethod\n    @lru_cache(maxsize=128)\n    def P(q: float) -&gt; float:\n        \"\"\"The P function as given in Jones et al.\n\n        Parameters\n        ----------\n        q : float\n            Input parameter for the P function\n\n        Returns\n        -------\n        float\n            Result of the P function computation\n        \"\"\"\n        K1 = SHASH.my_K((q + 1) / 2, 0.25)\n        K2 = SHASH.my_K((q - 1) / 2, 0.25)\n        a: Variable[Any, Any] = (K1 + K2) * CONST1  # type: ignore\n        return a  # type: ignore\n\n    @staticmethod\n    def m1(epsilon: float, delta: float) -&gt; float:\n        \"\"\"The first moment of the SHASH distribution parametrized by epsilon and delta.\n\n        Parameters\n        ----------\n        epsilon : float\n            Skewness parameter controlling asymmetry\n        delta : float\n            Kurtosis parameter controlling tail weight\n\n        Returns\n        -------\n        float\n            First moment of the SHASH distribution\n        \"\"\"\n        return np.sinh(epsilon / delta) * SHASH.P(1 / delta)\n\n    @staticmethod\n    def m2(epsilon: float, delta: float) -&gt; float:\n        \"\"\"The second moment of the SHASH distribution parametrized by epsilon and delta.\n\n        Parameters\n        ----------\n        epsilon : float\n            Skewness parameter controlling asymmetry\n        delta : float\n            Kurtosis parameter controlling tail weight\n\n        Returns\n        -------\n        float\n            Second moment of the SHASH distribution\n        \"\"\"\n        return (np.cosh(2 * epsilon / delta) * SHASH.P(2 / delta) - 1) / 2\n\n    @staticmethod\n    def m1m2(epsilon: float, delta: float) -&gt; Tuple[float, float]:\n        \"\"\"Compute both first and second moments of the SHASH distribution.\n\n        This method efficiently calculates both moments together to avoid redundant \n        computations of the P function.\n\n        Parameters\n        ----------\n        epsilon : float\n            Skewness parameter controlling distribution asymmetry\n        delta : float\n            Kurtosis parameter controlling tail weight\n\n        Returns\n        -------\n        mean : float\n            First moment (mean) of the distribution\n        var : float\n            Second central moment (variance) of the distribution\n\n        Notes\n        -----\n        This method is more efficient than calling m1() and m2() separately\n        as it reuses intermediate calculations.\n\n        Examples\n        --------\n        &gt;&gt;&gt; mean, var = SHASH.m1m2(0.0, 1.0)  # Standard normal case\n        &gt;&gt;&gt; np.allclose([mean, var], [0.0, 1.0])\n        True\n        \"\"\"\n        inv_delta = 1.0 / delta\n        two_inv_delta = 2.0 * inv_delta\n        p1 = SHASH.P(inv_delta)\n        p2 = SHASH.P(two_inv_delta)\n        eps_delta = epsilon / delta\n        sinh_eps_delta = np.sinh(eps_delta)\n        cosh_2eps_delta = np.cosh(2 * eps_delta)\n        mean = sinh_eps_delta * p1\n        raw_second = (cosh_2eps_delta * p2 - 1) / 2\n        var = raw_second - mean**2\n        return mean, var\n\n    @classmethod\n    def dist(cls, epsilon: pt.TensorLike, delta: pt.TensorLike, **kwargs: Any) -&gt; Any:\n        \"\"\"Create a SHASH distribution with given parameters.\n\n        Parameters\n        ----------\n        epsilon : TensorLike\n            Skewness parameter controlling distribution asymmetry\n        delta : TensorLike\n            Kurtosis parameter controlling tail weight\n        **kwargs : dict\n            Additional arguments passed to the distribution constructor\n\n        Returns\n        -------\n        SHASH\n            A SHASH distribution instance\n\n        Notes\n        -----\n        The parameters are converted to float tensors before distribution creation.\n        \"\"\"\n        epsilon = as_tensor_variable(floatX(epsilon))\n        delta = as_tensor_variable(floatX(delta))\n        return super().dist([epsilon, delta], **kwargs)\n\n    def logp(self, value: ArrayLike, epsilon: float, delta: float) -&gt; float:\n        \"\"\"Calculate the log probability density of the SHASH distribution.\n\n        Parameters\n        ----------\n        value : array_like\n            Points at which to evaluate the log probability density\n        epsilon : float\n            Skewness parameter controlling distribution asymmetry\n        delta : float\n            Kurtosis parameter controlling tail weight\n\n        Returns\n        -------\n        float\n            Log probability density at the specified points\n\n        Notes\n        -----\n        The implementation follows Jones et al. (2009) equation (2.2).\n        Numerical stability is maintained through log-space calculations.\n        \"\"\"\n        this_S = S(value, epsilon, delta)\n        this_S_sqr = np.square(this_S)\n        this_C_sqr = 1 + this_S_sqr\n        frac2 = (\n            np.log(delta) + np.log(this_C_sqr) / 2 - np.log(1 + np.square(value)) / 2\n        )\n        exp = -this_S_sqr / 2\n        return CONST2 + frac2 + exp\n</code></pre> <code>P(q: float) -&gt; float</code> <code>cached</code> <code>staticmethod</code> \u00b6 <p>The P function as given in Jones et al.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>float</code> <p>Input parameter for the P function</p> required <p>Returns:</p> Type Description <code>float</code> <p>Result of the P function computation</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>@staticmethod\n@lru_cache(maxsize=128)\ndef P(q: float) -&gt; float:\n    \"\"\"The P function as given in Jones et al.\n\n    Parameters\n    ----------\n    q : float\n        Input parameter for the P function\n\n    Returns\n    -------\n    float\n        Result of the P function computation\n    \"\"\"\n    K1 = SHASH.my_K((q + 1) / 2, 0.25)\n    K2 = SHASH.my_K((q - 1) / 2, 0.25)\n    a: Variable[Any, Any] = (K1 + K2) * CONST1  # type: ignore\n    return a  # type: ignore\n</code></pre> <code>dist(epsilon: pt.TensorLike, delta: pt.TensorLike, **kwargs: Any) -&gt; Any</code> <code>classmethod</code> \u00b6 <p>Create a SHASH distribution with given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>epsilon</code> <code>TensorLike</code> <p>Skewness parameter controlling distribution asymmetry</p> required <code>delta</code> <code>TensorLike</code> <p>Kurtosis parameter controlling tail weight</p> required <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to the distribution constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>SHASH</code> <p>A SHASH distribution instance</p> Notes <p>The parameters are converted to float tensors before distribution creation.</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>@classmethod\ndef dist(cls, epsilon: pt.TensorLike, delta: pt.TensorLike, **kwargs: Any) -&gt; Any:\n    \"\"\"Create a SHASH distribution with given parameters.\n\n    Parameters\n    ----------\n    epsilon : TensorLike\n        Skewness parameter controlling distribution asymmetry\n    delta : TensorLike\n        Kurtosis parameter controlling tail weight\n    **kwargs : dict\n        Additional arguments passed to the distribution constructor\n\n    Returns\n    -------\n    SHASH\n        A SHASH distribution instance\n\n    Notes\n    -----\n    The parameters are converted to float tensors before distribution creation.\n    \"\"\"\n    epsilon = as_tensor_variable(floatX(epsilon))\n    delta = as_tensor_variable(floatX(delta))\n    return super().dist([epsilon, delta], **kwargs)\n</code></pre> <code>logp(value: ArrayLike, epsilon: float, delta: float) -&gt; float</code> \u00b6 <p>Calculate the log probability density of the SHASH distribution.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>array_like</code> <p>Points at which to evaluate the log probability density</p> required <code>epsilon</code> <code>float</code> <p>Skewness parameter controlling distribution asymmetry</p> required <code>delta</code> <code>float</code> <p>Kurtosis parameter controlling tail weight</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log probability density at the specified points</p> Notes <p>The implementation follows Jones et al. (2009) equation (2.2). Numerical stability is maintained through log-space calculations.</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>def logp(self, value: ArrayLike, epsilon: float, delta: float) -&gt; float:\n    \"\"\"Calculate the log probability density of the SHASH distribution.\n\n    Parameters\n    ----------\n    value : array_like\n        Points at which to evaluate the log probability density\n    epsilon : float\n        Skewness parameter controlling distribution asymmetry\n    delta : float\n        Kurtosis parameter controlling tail weight\n\n    Returns\n    -------\n    float\n        Log probability density at the specified points\n\n    Notes\n    -----\n    The implementation follows Jones et al. (2009) equation (2.2).\n    Numerical stability is maintained through log-space calculations.\n    \"\"\"\n    this_S = S(value, epsilon, delta)\n    this_S_sqr = np.square(this_S)\n    this_C_sqr = 1 + this_S_sqr\n    frac2 = (\n        np.log(delta) + np.log(this_C_sqr) / 2 - np.log(1 + np.square(value)) / 2\n    )\n    exp = -this_S_sqr / 2\n    return CONST2 + frac2 + exp\n</code></pre> <code>m1(epsilon: float, delta: float) -&gt; float</code> <code>staticmethod</code> \u00b6 <p>The first moment of the SHASH distribution parametrized by epsilon and delta.</p> <p>Parameters:</p> Name Type Description Default <code>epsilon</code> <code>float</code> <p>Skewness parameter controlling asymmetry</p> required <code>delta</code> <code>float</code> <p>Kurtosis parameter controlling tail weight</p> required <p>Returns:</p> Type Description <code>float</code> <p>First moment of the SHASH distribution</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>@staticmethod\ndef m1(epsilon: float, delta: float) -&gt; float:\n    \"\"\"The first moment of the SHASH distribution parametrized by epsilon and delta.\n\n    Parameters\n    ----------\n    epsilon : float\n        Skewness parameter controlling asymmetry\n    delta : float\n        Kurtosis parameter controlling tail weight\n\n    Returns\n    -------\n    float\n        First moment of the SHASH distribution\n    \"\"\"\n    return np.sinh(epsilon / delta) * SHASH.P(1 / delta)\n</code></pre> <code>m1m2(epsilon: float, delta: float) -&gt; Tuple[float, float]</code> <code>staticmethod</code> \u00b6 <p>Compute both first and second moments of the SHASH distribution.</p> <p>This method efficiently calculates both moments together to avoid redundant  computations of the P function.</p> <p>Parameters:</p> Name Type Description Default <code>epsilon</code> <code>float</code> <p>Skewness parameter controlling distribution asymmetry</p> required <code>delta</code> <code>float</code> <p>Kurtosis parameter controlling tail weight</p> required <p>Returns:</p> Name Type Description <code>mean</code> <code>float</code> <p>First moment (mean) of the distribution</p> <code>var</code> <code>float</code> <p>Second central moment (variance) of the distribution</p> Notes <p>This method is more efficient than calling m1() and m2() separately as it reuses intermediate calculations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mean, var = SHASH.m1m2(0.0, 1.0)  # Standard normal case\n&gt;&gt;&gt; np.allclose([mean, var], [0.0, 1.0])\nTrue\n</code></pre> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>@staticmethod\ndef m1m2(epsilon: float, delta: float) -&gt; Tuple[float, float]:\n    \"\"\"Compute both first and second moments of the SHASH distribution.\n\n    This method efficiently calculates both moments together to avoid redundant \n    computations of the P function.\n\n    Parameters\n    ----------\n    epsilon : float\n        Skewness parameter controlling distribution asymmetry\n    delta : float\n        Kurtosis parameter controlling tail weight\n\n    Returns\n    -------\n    mean : float\n        First moment (mean) of the distribution\n    var : float\n        Second central moment (variance) of the distribution\n\n    Notes\n    -----\n    This method is more efficient than calling m1() and m2() separately\n    as it reuses intermediate calculations.\n\n    Examples\n    --------\n    &gt;&gt;&gt; mean, var = SHASH.m1m2(0.0, 1.0)  # Standard normal case\n    &gt;&gt;&gt; np.allclose([mean, var], [0.0, 1.0])\n    True\n    \"\"\"\n    inv_delta = 1.0 / delta\n    two_inv_delta = 2.0 * inv_delta\n    p1 = SHASH.P(inv_delta)\n    p2 = SHASH.P(two_inv_delta)\n    eps_delta = epsilon / delta\n    sinh_eps_delta = np.sinh(eps_delta)\n    cosh_2eps_delta = np.cosh(2 * eps_delta)\n    mean = sinh_eps_delta * p1\n    raw_second = (cosh_2eps_delta * p2 - 1) / 2\n    var = raw_second - mean**2\n    return mean, var\n</code></pre> <code>m2(epsilon: float, delta: float) -&gt; float</code> <code>staticmethod</code> \u00b6 <p>The second moment of the SHASH distribution parametrized by epsilon and delta.</p> <p>Parameters:</p> Name Type Description Default <code>epsilon</code> <code>float</code> <p>Skewness parameter controlling asymmetry</p> required <code>delta</code> <code>float</code> <p>Kurtosis parameter controlling tail weight</p> required <p>Returns:</p> Type Description <code>float</code> <p>Second moment of the SHASH distribution</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>@staticmethod\ndef m2(epsilon: float, delta: float) -&gt; float:\n    \"\"\"The second moment of the SHASH distribution parametrized by epsilon and delta.\n\n    Parameters\n    ----------\n    epsilon : float\n        Skewness parameter controlling asymmetry\n    delta : float\n        Kurtosis parameter controlling tail weight\n\n    Returns\n    -------\n    float\n        Second moment of the SHASH distribution\n    \"\"\"\n    return (np.cosh(2 * epsilon / delta) * SHASH.P(2 / delta) - 1) / 2\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.shash.SHASHb","title":"<code>SHASHb</code>","text":"<p>               Bases: <code>Continuous</code></p> <p>Standardized variant of the SHASH distribution.</p> <p>This distribution extends the base SHASH distribution by standardizing it to have zero mean and unit variance before applying location and scale transformations.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>Location parameter (mean)</p> required <code>sigma</code> <code>float</code> <p>Scale parameter (standard deviation)</p> required <code>epsilon</code> <code>float</code> <p>Skewness parameter controlling asymmetry</p> required <code>delta</code> <code>float</code> <p>Kurtosis parameter controlling tail weight</p> required Notes <p>The distribution is obtained by: 1. Starting with base SHASH distribution 2. Standardizing to zero mean and unit variance 3. Applying Y = mu + sigma * X transformation</p> <p>This standardization can improve numerical stability and parameter interpretability in some applications.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pymc as pm\n&gt;&gt;&gt; with pm.Model():\n...     x = pm.SHASHb('x', mu=0.0, sigma=1.0, epsilon=0.5, delta=1.2)\n</code></pre> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>class SHASHb(Continuous):\n    \"\"\"Standardized variant of the SHASH distribution.\n\n    This distribution extends the base SHASH distribution by standardizing it\n    to have zero mean and unit variance before applying location and scale\n    transformations.\n\n    Parameters\n    ----------\n    mu : float\n        Location parameter (mean)\n    sigma : float\n        Scale parameter (standard deviation)\n    epsilon : float\n        Skewness parameter controlling asymmetry\n    delta : float\n        Kurtosis parameter controlling tail weight\n\n    Notes\n    -----\n    The distribution is obtained by:\n    1. Starting with base SHASH distribution\n    2. Standardizing to zero mean and unit variance\n    3. Applying Y = mu + sigma * X transformation\n\n    This standardization can improve numerical stability and parameter\n    interpretability in some applications.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pymc as pm\n    &gt;&gt;&gt; with pm.Model():\n    ...     x = pm.SHASHb('x', mu=0.0, sigma=1.0, epsilon=0.5, delta=1.2)\n    \"\"\"\n\n    rv_op = shashb\n\n    @classmethod\n    def dist(\n        cls,\n        mu: pt.TensorLike,\n        sigma: pt.TensorLike,\n        epsilon: pt.TensorLike,\n        delta: pt.TensorLike,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Create a SHASHb distribution with given parameters.\n\n        Parameters\n        ----------\n        mu : TensorLike\n            Location parameter (mean)\n        sigma : TensorLike\n            Scale parameter (standard deviation)\n        epsilon : TensorLike\n            Skewness parameter controlling asymmetry\n        delta : TensorLike\n            Kurtosis parameter controlling tail weight\n        **kwargs : dict\n            Additional arguments passed to the distribution constructor\n\n        Returns\n        -------\n        SHASHb\n            A standardized SHASH distribution instance\n        \"\"\"\n        mu = as_tensor_variable(floatX(mu))\n        sigma = as_tensor_variable(floatX(sigma))\n        epsilon = as_tensor_variable(floatX(epsilon))\n        delta = as_tensor_variable(floatX(delta))\n        return super().dist([mu, sigma, epsilon, delta], **kwargs)\n\n    def logp(\n        self, value: ArrayLike, mu: float, sigma: float, epsilon: float, delta: float\n    ) -&gt; float:\n        \"\"\"Calculate the log probability density of the SHASHb distribution.\n\n        Parameters\n        ----------\n        value : array_like\n            Points at which to evaluate the log probability density\n        mu : float\n            Location parameter (mean)\n        sigma : float\n            Scale parameter (standard deviation)\n        epsilon : float\n            Skewness parameter controlling asymmetry\n        delta : float\n            Kurtosis parameter controlling tail weight\n\n        Returns\n        -------\n        float\n            Log probability density at the specified points\n\n        Notes\n        -----\n        The implementation follows Jones et al. (2009) equation (2.2)\n        with standardization and location-scale transformation.\n        \"\"\"\n        mean, var = SHASH.m1m2(epsilon, delta)\n        remapped_value = ((value - mu) / sigma) * np.sqrt(var) + mean  # type: ignore\n        this_S = S(remapped_value, epsilon, delta)\n        this_S_sqr = np.square(this_S)\n        this_C_sqr = 1 + this_S_sqr\n        frac2 = (\n            np.log(delta)\n            + np.log(this_C_sqr) / 2\n            - np.log(1 + np.square(remapped_value)) / 2\n        )\n        exp = -this_S_sqr / 2\n        return CONST2 + frac2 + exp + np.log(var) / 2 - np.log(sigma)\n</code></pre> <code>dist(mu: pt.TensorLike, sigma: pt.TensorLike, epsilon: pt.TensorLike, delta: pt.TensorLike, **kwargs: Any) -&gt; Any</code> <code>classmethod</code> \u00b6 <p>Create a SHASHb distribution with given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>TensorLike</code> <p>Location parameter (mean)</p> required <code>sigma</code> <code>TensorLike</code> <p>Scale parameter (standard deviation)</p> required <code>epsilon</code> <code>TensorLike</code> <p>Skewness parameter controlling asymmetry</p> required <code>delta</code> <code>TensorLike</code> <p>Kurtosis parameter controlling tail weight</p> required <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to the distribution constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>SHASHb</code> <p>A standardized SHASH distribution instance</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>@classmethod\ndef dist(\n    cls,\n    mu: pt.TensorLike,\n    sigma: pt.TensorLike,\n    epsilon: pt.TensorLike,\n    delta: pt.TensorLike,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Create a SHASHb distribution with given parameters.\n\n    Parameters\n    ----------\n    mu : TensorLike\n        Location parameter (mean)\n    sigma : TensorLike\n        Scale parameter (standard deviation)\n    epsilon : TensorLike\n        Skewness parameter controlling asymmetry\n    delta : TensorLike\n        Kurtosis parameter controlling tail weight\n    **kwargs : dict\n        Additional arguments passed to the distribution constructor\n\n    Returns\n    -------\n    SHASHb\n        A standardized SHASH distribution instance\n    \"\"\"\n    mu = as_tensor_variable(floatX(mu))\n    sigma = as_tensor_variable(floatX(sigma))\n    epsilon = as_tensor_variable(floatX(epsilon))\n    delta = as_tensor_variable(floatX(delta))\n    return super().dist([mu, sigma, epsilon, delta], **kwargs)\n</code></pre> <code>logp(value: ArrayLike, mu: float, sigma: float, epsilon: float, delta: float) -&gt; float</code> \u00b6 <p>Calculate the log probability density of the SHASHb distribution.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>array_like</code> <p>Points at which to evaluate the log probability density</p> required <code>mu</code> <code>float</code> <p>Location parameter (mean)</p> required <code>sigma</code> <code>float</code> <p>Scale parameter (standard deviation)</p> required <code>epsilon</code> <code>float</code> <p>Skewness parameter controlling asymmetry</p> required <code>delta</code> <code>float</code> <p>Kurtosis parameter controlling tail weight</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log probability density at the specified points</p> Notes <p>The implementation follows Jones et al. (2009) equation (2.2) with standardization and location-scale transformation.</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>def logp(\n    self, value: ArrayLike, mu: float, sigma: float, epsilon: float, delta: float\n) -&gt; float:\n    \"\"\"Calculate the log probability density of the SHASHb distribution.\n\n    Parameters\n    ----------\n    value : array_like\n        Points at which to evaluate the log probability density\n    mu : float\n        Location parameter (mean)\n    sigma : float\n        Scale parameter (standard deviation)\n    epsilon : float\n        Skewness parameter controlling asymmetry\n    delta : float\n        Kurtosis parameter controlling tail weight\n\n    Returns\n    -------\n    float\n        Log probability density at the specified points\n\n    Notes\n    -----\n    The implementation follows Jones et al. (2009) equation (2.2)\n    with standardization and location-scale transformation.\n    \"\"\"\n    mean, var = SHASH.m1m2(epsilon, delta)\n    remapped_value = ((value - mu) / sigma) * np.sqrt(var) + mean  # type: ignore\n    this_S = S(remapped_value, epsilon, delta)\n    this_S_sqr = np.square(this_S)\n    this_C_sqr = 1 + this_S_sqr\n    frac2 = (\n        np.log(delta)\n        + np.log(this_C_sqr) / 2\n        - np.log(1 + np.square(remapped_value)) / 2\n    )\n    exp = -this_S_sqr / 2\n    return CONST2 + frac2 + exp + np.log(var) / 2 - np.log(sigma)\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.shash.SHASHbRV","title":"<code>SHASHbRV</code>","text":"<p>               Bases: <code>RandomVariable</code></p> <p>Random variable class for the standardized SHASH distribution.</p> <p>This class implements sampling from a SHASH distribution that has been standardized to have zero mean and unit variance before applying location and scale transformations.</p> Notes <p>The transformation involves: 1. Generate SHASH samples 2. Standardize to zero mean and unit variance 3. Apply location-scale transformation</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>class SHASHbRV(RandomVariable):\n    \"\"\"Random variable class for the standardized SHASH distribution.\n\n    This class implements sampling from a SHASH distribution that has been\n    standardized to have zero mean and unit variance before applying\n    location and scale transformations.\n\n    Notes\n    -----\n    The transformation involves:\n    1. Generate SHASH samples\n    2. Standardize to zero mean and unit variance\n    3. Apply location-scale transformation\n    \"\"\"\n\n    name = \"shashb\"\n    signature = \"(),(),(),()-&gt;()\"\n    dtype = \"floatX\"\n    _print_name = (\"SHASHb\", \"\\\\operatorname{SHASHb}\")\n\n    @classmethod\n    def rng_fn(\n        cls,\n        rng: Generator,\n        mu: float,\n        sigma: float,\n        epsilon: float,\n        delta: float,\n        size: Optional[Union[int, Tuple[int, ...]]] = None,\n    ) -&gt; NDArray[np.float64]:\n        \"\"\"Generate random samples from standardized SHASH distribution.\n\n        Parameters\n        ----------\n        rng : Generator\n            NumPy random number generator\n        mu : float\n            Location parameter (mean)\n        sigma : float\n            Scale parameter (standard deviation)\n        epsilon : float\n            Skewness parameter\n        delta : float\n            Kurtosis parameter\n        size : int or tuple of ints, optional\n            Output shape. Default is None.\n\n        Returns\n        -------\n        NDArray[np.float64]\n            Array of random samples from the distribution\n\n        Notes\n        -----\n        The sampling process involves:\n        1. Generate standard normal samples\n        2. Apply SHASH transformation\n        3. Standardize\n        4. Apply location-scale transformation\n        \"\"\"\n        s = rng.normal(size=size)\n\n        def P(q: float) -&gt; float:\n            \"\"\"Helper function to compute P function.\n\n            Parameters\n            ----------\n            q : float\n                Input parameter\n\n            Returns\n            -------\n            float\n                P function value\n            \"\"\"\n            K1 = kv((q + 1) / 2, 0.25)\n            K2 = kv((q - 1) / 2, 0.25)\n            a = (K1 + K2) * CONST1\n            return a\n\n        def m1m2(epsilon: float, delta: float) -&gt; Tuple[float, float]:\n            \"\"\"Helper function to compute moments.\n\n            Parameters\n            ----------\n            epsilon : float\n                Skewness parameter\n            delta : float\n                Kurtosis parameter\n\n            Returns\n            -------\n            Tuple[float, float]\n                Mean and variance\n            \"\"\"\n            inv_delta = 1.0 / delta\n            two_inv_delta = 2.0 * inv_delta\n            p1 = P(inv_delta)\n            p2 = P(two_inv_delta)\n            eps_delta = epsilon / delta\n            sinh_eps_delta = np.sinh(eps_delta)\n            cosh_2eps_delta = np.cosh(2 * eps_delta)\n            mean = sinh_eps_delta * p1\n            raw_second = (cosh_2eps_delta * p2 - 1) / 2\n            var = raw_second - mean**2\n            return mean, var\n\n        mean, var = m1m2(epsilon, delta)\n        out = (\n            (np.sinh((np.arcsinh(s) + epsilon) / delta) - mean) / np.sqrt(var)\n        ) * sigma + mu  # type: ignore\n        return out\n</code></pre> <code>rng_fn(rng: Generator, mu: float, sigma: float, epsilon: float, delta: float, size: Optional[Union[int, Tuple[int, ...]]] = None) -&gt; NDArray[np.float64]</code> <code>classmethod</code> \u00b6 <p>Generate random samples from standardized SHASH distribution.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>NumPy random number generator</p> required <code>mu</code> <code>float</code> <p>Location parameter (mean)</p> required <code>sigma</code> <code>float</code> <p>Scale parameter (standard deviation)</p> required <code>epsilon</code> <code>float</code> <p>Skewness parameter</p> required <code>delta</code> <code>float</code> <p>Kurtosis parameter</p> required <code>size</code> <code>int or tuple of ints</code> <p>Output shape. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Array of random samples from the distribution</p> Notes <p>The sampling process involves: 1. Generate standard normal samples 2. Apply SHASH transformation 3. Standardize 4. Apply location-scale transformation</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>@classmethod\ndef rng_fn(\n    cls,\n    rng: Generator,\n    mu: float,\n    sigma: float,\n    epsilon: float,\n    delta: float,\n    size: Optional[Union[int, Tuple[int, ...]]] = None,\n) -&gt; NDArray[np.float64]:\n    \"\"\"Generate random samples from standardized SHASH distribution.\n\n    Parameters\n    ----------\n    rng : Generator\n        NumPy random number generator\n    mu : float\n        Location parameter (mean)\n    sigma : float\n        Scale parameter (standard deviation)\n    epsilon : float\n        Skewness parameter\n    delta : float\n        Kurtosis parameter\n    size : int or tuple of ints, optional\n        Output shape. Default is None.\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Array of random samples from the distribution\n\n    Notes\n    -----\n    The sampling process involves:\n    1. Generate standard normal samples\n    2. Apply SHASH transformation\n    3. Standardize\n    4. Apply location-scale transformation\n    \"\"\"\n    s = rng.normal(size=size)\n\n    def P(q: float) -&gt; float:\n        \"\"\"Helper function to compute P function.\n\n        Parameters\n        ----------\n        q : float\n            Input parameter\n\n        Returns\n        -------\n        float\n            P function value\n        \"\"\"\n        K1 = kv((q + 1) / 2, 0.25)\n        K2 = kv((q - 1) / 2, 0.25)\n        a = (K1 + K2) * CONST1\n        return a\n\n    def m1m2(epsilon: float, delta: float) -&gt; Tuple[float, float]:\n        \"\"\"Helper function to compute moments.\n\n        Parameters\n        ----------\n        epsilon : float\n            Skewness parameter\n        delta : float\n            Kurtosis parameter\n\n        Returns\n        -------\n        Tuple[float, float]\n            Mean and variance\n        \"\"\"\n        inv_delta = 1.0 / delta\n        two_inv_delta = 2.0 * inv_delta\n        p1 = P(inv_delta)\n        p2 = P(two_inv_delta)\n        eps_delta = epsilon / delta\n        sinh_eps_delta = np.sinh(eps_delta)\n        cosh_2eps_delta = np.cosh(2 * eps_delta)\n        mean = sinh_eps_delta * p1\n        raw_second = (cosh_2eps_delta * p2 - 1) / 2\n        var = raw_second - mean**2\n        return mean, var\n\n    mean, var = m1m2(epsilon, delta)\n    out = (\n        (np.sinh((np.arcsinh(s) + epsilon) / delta) - mean) / np.sqrt(var)\n    ) * sigma + mu  # type: ignore\n    return out\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.shash.SHASHo","title":"<code>SHASHo</code>","text":"<p>               Bases: <code>Continuous</code></p> <p>Location-scale variant of the SHASH distribution.</p> <p>This distribution extends the base SHASH distribution by adding location (mu) and scale (sigma) parameters.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>Location parameter (mean)</p> required <code>sigma</code> <code>float</code> <p>Scale parameter (standard deviation)</p> required <code>epsilon</code> <code>float</code> <p>Skewness parameter controlling asymmetry</p> required <code>delta</code> <code>float</code> <p>Kurtosis parameter controlling tail weight</p> required Notes <p>The distribution is obtained by applying the transformation Y = mu + sigma * X where X follows the base SHASH distribution.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pymc as pm\n&gt;&gt;&gt; with pm.Model():\n...     x = pm.SHASHo('x', mu=0.0, sigma=1.0, epsilon=0.5, delta=1.2)\n</code></pre> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>class SHASHo(Continuous):\n    \"\"\"Location-scale variant of the SHASH distribution.\n\n    This distribution extends the base SHASH distribution by adding\n    location (mu) and scale (sigma) parameters.\n\n    Parameters\n    ----------\n    mu : float\n        Location parameter (mean)\n    sigma : float\n        Scale parameter (standard deviation)\n    epsilon : float\n        Skewness parameter controlling asymmetry\n    delta : float\n        Kurtosis parameter controlling tail weight\n\n    Notes\n    -----\n    The distribution is obtained by applying the transformation\n    Y = mu + sigma * X where X follows the base SHASH distribution.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pymc as pm\n    &gt;&gt;&gt; with pm.Model():\n    ...     x = pm.SHASHo('x', mu=0.0, sigma=1.0, epsilon=0.5, delta=1.2)\n    \"\"\"\n\n    rv_op = shasho\n\n    @classmethod\n    def dist(\n        cls,\n        mu: pt.TensorLike,\n        sigma: pt.TensorLike,\n        epsilon: pt.TensorLike,\n        delta: pt.TensorLike,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Create a SHASHo distribution with given parameters.\n\n        Parameters\n        ----------\n        mu : TensorLike\n            Location parameter (mean)\n        sigma : TensorLike\n            Scale parameter (standard deviation)\n        epsilon : TensorLike\n            Skewness parameter controlling asymmetry\n        delta : TensorLike\n            Kurtosis parameter controlling tail weight\n        **kwargs : dict\n            Additional arguments passed to the distribution constructor\n\n        Returns\n        -------\n        SHASHo\n            A location-scale SHASH distribution instance\n        \"\"\"\n        mu = as_tensor_variable(floatX(mu))\n        sigma = as_tensor_variable(floatX(sigma))\n        epsilon = as_tensor_variable(floatX(epsilon))\n        delta = as_tensor_variable(floatX(delta))\n        return super().dist([mu, sigma, epsilon, delta], **kwargs)\n\n    def logp(\n        self, value: ArrayLike, mu: float, sigma: float, epsilon: float, delta: float\n    ) -&gt; float:\n        \"\"\"Calculate the log probability density of the SHASHo distribution.\n\n        Parameters\n        ----------\n        value : array_like\n            Points at which to evaluate the log probability density\n        mu : float\n            Location parameter (mean)\n        sigma : float\n            Scale parameter (standard deviation)\n        epsilon : float\n            Skewness parameter controlling asymmetry\n        delta : float\n            Kurtosis parameter controlling tail weight\n\n        Returns\n        -------\n        float\n            Log probability density at the specified points\n\n        Notes\n        -----\n        The implementation follows Jones et al. (2009) equation (2.2)\n        with additional location-scale transformation.\n        \"\"\"\n        remapped_value = (value - mu) / sigma  # type: ignore\n        this_S = S(remapped_value, epsilon, delta)\n        this_S_sqr = np.square(this_S)\n        this_C_sqr = 1 + this_S_sqr\n        frac2 = (\n            np.log(delta)\n            + np.log(this_C_sqr) / 2\n            - np.log(1 + np.square(remapped_value)) / 2\n        )\n        exp = -this_S_sqr / 2\n        return CONST2 + frac2 + exp - np.log(sigma)\n</code></pre> <code>dist(mu: pt.TensorLike, sigma: pt.TensorLike, epsilon: pt.TensorLike, delta: pt.TensorLike, **kwargs: Any) -&gt; Any</code> <code>classmethod</code> \u00b6 <p>Create a SHASHo distribution with given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>TensorLike</code> <p>Location parameter (mean)</p> required <code>sigma</code> <code>TensorLike</code> <p>Scale parameter (standard deviation)</p> required <code>epsilon</code> <code>TensorLike</code> <p>Skewness parameter controlling asymmetry</p> required <code>delta</code> <code>TensorLike</code> <p>Kurtosis parameter controlling tail weight</p> required <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to the distribution constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>SHASHo</code> <p>A location-scale SHASH distribution instance</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>@classmethod\ndef dist(\n    cls,\n    mu: pt.TensorLike,\n    sigma: pt.TensorLike,\n    epsilon: pt.TensorLike,\n    delta: pt.TensorLike,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Create a SHASHo distribution with given parameters.\n\n    Parameters\n    ----------\n    mu : TensorLike\n        Location parameter (mean)\n    sigma : TensorLike\n        Scale parameter (standard deviation)\n    epsilon : TensorLike\n        Skewness parameter controlling asymmetry\n    delta : TensorLike\n        Kurtosis parameter controlling tail weight\n    **kwargs : dict\n        Additional arguments passed to the distribution constructor\n\n    Returns\n    -------\n    SHASHo\n        A location-scale SHASH distribution instance\n    \"\"\"\n    mu = as_tensor_variable(floatX(mu))\n    sigma = as_tensor_variable(floatX(sigma))\n    epsilon = as_tensor_variable(floatX(epsilon))\n    delta = as_tensor_variable(floatX(delta))\n    return super().dist([mu, sigma, epsilon, delta], **kwargs)\n</code></pre> <code>logp(value: ArrayLike, mu: float, sigma: float, epsilon: float, delta: float) -&gt; float</code> \u00b6 <p>Calculate the log probability density of the SHASHo distribution.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>array_like</code> <p>Points at which to evaluate the log probability density</p> required <code>mu</code> <code>float</code> <p>Location parameter (mean)</p> required <code>sigma</code> <code>float</code> <p>Scale parameter (standard deviation)</p> required <code>epsilon</code> <code>float</code> <p>Skewness parameter controlling asymmetry</p> required <code>delta</code> <code>float</code> <p>Kurtosis parameter controlling tail weight</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log probability density at the specified points</p> Notes <p>The implementation follows Jones et al. (2009) equation (2.2) with additional location-scale transformation.</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>def logp(\n    self, value: ArrayLike, mu: float, sigma: float, epsilon: float, delta: float\n) -&gt; float:\n    \"\"\"Calculate the log probability density of the SHASHo distribution.\n\n    Parameters\n    ----------\n    value : array_like\n        Points at which to evaluate the log probability density\n    mu : float\n        Location parameter (mean)\n    sigma : float\n        Scale parameter (standard deviation)\n    epsilon : float\n        Skewness parameter controlling asymmetry\n    delta : float\n        Kurtosis parameter controlling tail weight\n\n    Returns\n    -------\n    float\n        Log probability density at the specified points\n\n    Notes\n    -----\n    The implementation follows Jones et al. (2009) equation (2.2)\n    with additional location-scale transformation.\n    \"\"\"\n    remapped_value = (value - mu) / sigma  # type: ignore\n    this_S = S(remapped_value, epsilon, delta)\n    this_S_sqr = np.square(this_S)\n    this_C_sqr = 1 + this_S_sqr\n    frac2 = (\n        np.log(delta)\n        + np.log(this_C_sqr) / 2\n        - np.log(1 + np.square(remapped_value)) / 2\n    )\n    exp = -this_S_sqr / 2\n    return CONST2 + frac2 + exp - np.log(sigma)\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.shash.SHASHo2","title":"<code>SHASHo2</code>","text":"<p>               Bases: <code>Continuous</code></p> <p>Alternative parameterization of the SHASH distribution.</p> <p>This distribution extends the base SHASH distribution by adding location (mu) and an adjusted scale parameter (sigma/delta).</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>Location parameter (mean)</p> required <code>sigma</code> <code>float</code> <p>Scale parameter (before delta adjustment)</p> required <code>epsilon</code> <code>float</code> <p>Skewness parameter controlling asymmetry</p> required <code>delta</code> <code>float</code> <p>Kurtosis parameter controlling tail weight</p> required Notes <p>The distribution is obtained by applying the transformation Y = mu + (sigma/delta) * X where X follows the base SHASH distribution. This parameterization can be useful when the relationship between scale and kurtosis needs to be explicitly modeled.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pymc as pm\n&gt;&gt;&gt; with pm.Model():\n...     x = pm.SHASHo2('x', mu=0.0, sigma=1.0, epsilon=0.5, delta=1.2)\n</code></pre> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>class SHASHo2(Continuous):\n    \"\"\"Alternative parameterization of the SHASH distribution.\n\n    This distribution extends the base SHASH distribution by adding location (mu)\n    and an adjusted scale parameter (sigma/delta).\n\n    Parameters\n    ----------\n    mu : float\n        Location parameter (mean)\n    sigma : float\n        Scale parameter (before delta adjustment)\n    epsilon : float\n        Skewness parameter controlling asymmetry\n    delta : float\n        Kurtosis parameter controlling tail weight\n\n    Notes\n    -----\n    The distribution is obtained by applying the transformation\n    Y = mu + (sigma/delta) * X where X follows the base SHASH distribution.\n    This parameterization can be useful when the relationship between\n    scale and kurtosis needs to be explicitly modeled.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pymc as pm\n    &gt;&gt;&gt; with pm.Model():\n    ...     x = pm.SHASHo2('x', mu=0.0, sigma=1.0, epsilon=0.5, delta=1.2)\n    \"\"\"\n\n    rv_op = shasho2\n\n    @classmethod\n    def dist(\n        cls,\n        mu: pt.TensorLike,\n        sigma: pt.TensorLike,\n        epsilon: pt.TensorLike,\n        delta: pt.TensorLike,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Create a SHASHo2 distribution with given parameters.\n\n        Parameters\n        ----------\n        mu : TensorLike\n            Location parameter (mean)\n        sigma : TensorLike\n            Scale parameter (before delta adjustment)\n        epsilon : TensorLike\n            Skewness parameter controlling asymmetry\n        delta : TensorLike\n            Kurtosis parameter controlling tail weight\n        **kwargs : dict\n            Additional arguments passed to the distribution constructor\n\n        Returns\n        -------\n        SHASHo2\n            An alternative parameterization SHASH distribution instance\n        \"\"\"\n        mu = as_tensor_variable(floatX(mu))\n        sigma = as_tensor_variable(floatX(sigma))\n        epsilon = as_tensor_variable(floatX(epsilon))\n        delta = as_tensor_variable(floatX(delta))\n        return super().dist([mu, sigma, epsilon, delta], **kwargs)\n\n    def logp(\n        self, value: ArrayLike, mu: float, sigma: float, epsilon: float, delta: float\n    ) -&gt; float:\n        \"\"\"Calculate the log probability density of the SHASHo2 distribution.\n\n        Parameters\n        ----------\n        value : array_like\n            Points at which to evaluate the log probability density\n        mu : float\n            Location parameter (mean)\n        sigma : float\n            Scale parameter (before delta adjustment)\n        epsilon : float\n            Skewness parameter controlling asymmetry\n        delta : float\n            Kurtosis parameter controlling tail weight\n\n        Returns\n        -------\n        float\n            Log probability density at the specified points\n\n        Notes\n        -----\n        The implementation follows Jones et al. (2009) equation (2.2)\n        with additional location-scale transformation where scale is\n        adjusted by the kurtosis parameter.\n        \"\"\"\n        sigma_d = sigma / delta\n        remapped_value = (value - mu) / sigma_d  # type: ignore\n        this_S = S(remapped_value, epsilon, delta)\n        this_S_sqr = np.square(this_S)\n        this_C_sqr = 1 + this_S_sqr\n        frac2 = (\n            np.log(delta)\n            + np.log(this_C_sqr) / 2\n            - np.log(1 + np.square(remapped_value)) / 2\n        )\n        exp = -this_S_sqr / 2\n        return CONST2 + frac2 + exp - np.log(sigma_d)\n</code></pre> <code>dist(mu: pt.TensorLike, sigma: pt.TensorLike, epsilon: pt.TensorLike, delta: pt.TensorLike, **kwargs: Any) -&gt; Any</code> <code>classmethod</code> \u00b6 <p>Create a SHASHo2 distribution with given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>TensorLike</code> <p>Location parameter (mean)</p> required <code>sigma</code> <code>TensorLike</code> <p>Scale parameter (before delta adjustment)</p> required <code>epsilon</code> <code>TensorLike</code> <p>Skewness parameter controlling asymmetry</p> required <code>delta</code> <code>TensorLike</code> <p>Kurtosis parameter controlling tail weight</p> required <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to the distribution constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>SHASHo2</code> <p>An alternative parameterization SHASH distribution instance</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>@classmethod\ndef dist(\n    cls,\n    mu: pt.TensorLike,\n    sigma: pt.TensorLike,\n    epsilon: pt.TensorLike,\n    delta: pt.TensorLike,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Create a SHASHo2 distribution with given parameters.\n\n    Parameters\n    ----------\n    mu : TensorLike\n        Location parameter (mean)\n    sigma : TensorLike\n        Scale parameter (before delta adjustment)\n    epsilon : TensorLike\n        Skewness parameter controlling asymmetry\n    delta : TensorLike\n        Kurtosis parameter controlling tail weight\n    **kwargs : dict\n        Additional arguments passed to the distribution constructor\n\n    Returns\n    -------\n    SHASHo2\n        An alternative parameterization SHASH distribution instance\n    \"\"\"\n    mu = as_tensor_variable(floatX(mu))\n    sigma = as_tensor_variable(floatX(sigma))\n    epsilon = as_tensor_variable(floatX(epsilon))\n    delta = as_tensor_variable(floatX(delta))\n    return super().dist([mu, sigma, epsilon, delta], **kwargs)\n</code></pre> <code>logp(value: ArrayLike, mu: float, sigma: float, epsilon: float, delta: float) -&gt; float</code> \u00b6 <p>Calculate the log probability density of the SHASHo2 distribution.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>array_like</code> <p>Points at which to evaluate the log probability density</p> required <code>mu</code> <code>float</code> <p>Location parameter (mean)</p> required <code>sigma</code> <code>float</code> <p>Scale parameter (before delta adjustment)</p> required <code>epsilon</code> <code>float</code> <p>Skewness parameter controlling asymmetry</p> required <code>delta</code> <code>float</code> <p>Kurtosis parameter controlling tail weight</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log probability density at the specified points</p> Notes <p>The implementation follows Jones et al. (2009) equation (2.2) with additional location-scale transformation where scale is adjusted by the kurtosis parameter.</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>def logp(\n    self, value: ArrayLike, mu: float, sigma: float, epsilon: float, delta: float\n) -&gt; float:\n    \"\"\"Calculate the log probability density of the SHASHo2 distribution.\n\n    Parameters\n    ----------\n    value : array_like\n        Points at which to evaluate the log probability density\n    mu : float\n        Location parameter (mean)\n    sigma : float\n        Scale parameter (before delta adjustment)\n    epsilon : float\n        Skewness parameter controlling asymmetry\n    delta : float\n        Kurtosis parameter controlling tail weight\n\n    Returns\n    -------\n    float\n        Log probability density at the specified points\n\n    Notes\n    -----\n    The implementation follows Jones et al. (2009) equation (2.2)\n    with additional location-scale transformation where scale is\n    adjusted by the kurtosis parameter.\n    \"\"\"\n    sigma_d = sigma / delta\n    remapped_value = (value - mu) / sigma_d  # type: ignore\n    this_S = S(remapped_value, epsilon, delta)\n    this_S_sqr = np.square(this_S)\n    this_C_sqr = 1 + this_S_sqr\n    frac2 = (\n        np.log(delta)\n        + np.log(this_C_sqr) / 2\n        - np.log(1 + np.square(remapped_value)) / 2\n    )\n    exp = -this_S_sqr / 2\n    return CONST2 + frac2 + exp - np.log(sigma_d)\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.shash.SHASHo2RV","title":"<code>SHASHo2RV</code>","text":"<p>               Bases: <code>RandomVariable</code></p> <p>Random variable class for the alternative parameterization of SHASH distribution.</p> <p>This class implements sampling from a SHASH distribution where the scale parameter is adjusted by the kurtosis parameter (sigma/delta).</p> Notes <p>The transformation is y = (sigma/delta) * x + mu, where x follows the base SHASH distribution with parameters epsilon and delta.</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>class SHASHo2RV(RandomVariable):\n    \"\"\"Random variable class for the alternative parameterization of SHASH distribution.\n\n    This class implements sampling from a SHASH distribution where the scale parameter\n    is adjusted by the kurtosis parameter (sigma/delta).\n\n    Notes\n    -----\n    The transformation is y = (sigma/delta) * x + mu, where x follows the base SHASH\n    distribution with parameters epsilon and delta.\n    \"\"\"\n\n    name = \"shasho2\"\n    signature = \"(),(),(),()-&gt;()\"\n    dtype = \"floatX\"\n    _print_name = (\"SHASHo2\", \"\\\\operatorname{SHASHo2}\")\n\n    @classmethod\n    def rng_fn(\n        cls,\n        rng: Generator,\n        mu: pt.TensorLike,\n        sigma: pt.TensorLike,\n        epsilon: pt.TensorLike,\n        delta: pt.TensorLike,\n        size: Optional[Union[int, Tuple[int, ...]]] = None,\n    ) -&gt; NDArray[np.float64]:\n        \"\"\"Generate random samples from the alternative parameterization SHASH distribution.\n\n        Parameters\n        ----------\n        rng : Generator\n            NumPy random number generator\n        mu : TensorLike\n            Location parameter (mean)\n        sigma : TensorLike\n            Scale parameter (before delta adjustment)\n        epsilon : TensorLike\n            Skewness parameter\n        delta : TensorLike\n            Kurtosis parameter\n        size : int or tuple of ints, optional\n            Output shape. Default is None.\n\n        Returns\n        -------\n        NDArray[np.float64]\n            Array of random samples from the distribution\n\n        Notes\n        -----\n        The sampling process involves:\n        1. Generate standard normal samples\n        2. Apply SHASH transformation\n        3. Scale by sigma/delta and shift by mu\n        \"\"\"\n        s = rng.normal(size=size)\n        sigma_d = sigma / delta  # type: ignore\n        return np.sinh((np.arcsinh(s) + epsilon) / delta) * sigma_d + mu  # type: ignore\n</code></pre> <code>rng_fn(rng: Generator, mu: pt.TensorLike, sigma: pt.TensorLike, epsilon: pt.TensorLike, delta: pt.TensorLike, size: Optional[Union[int, Tuple[int, ...]]] = None) -&gt; NDArray[np.float64]</code> <code>classmethod</code> \u00b6 <p>Generate random samples from the alternative parameterization SHASH distribution.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>NumPy random number generator</p> required <code>mu</code> <code>TensorLike</code> <p>Location parameter (mean)</p> required <code>sigma</code> <code>TensorLike</code> <p>Scale parameter (before delta adjustment)</p> required <code>epsilon</code> <code>TensorLike</code> <p>Skewness parameter</p> required <code>delta</code> <code>TensorLike</code> <p>Kurtosis parameter</p> required <code>size</code> <code>int or tuple of ints</code> <p>Output shape. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Array of random samples from the distribution</p> Notes <p>The sampling process involves: 1. Generate standard normal samples 2. Apply SHASH transformation 3. Scale by sigma/delta and shift by mu</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>@classmethod\ndef rng_fn(\n    cls,\n    rng: Generator,\n    mu: pt.TensorLike,\n    sigma: pt.TensorLike,\n    epsilon: pt.TensorLike,\n    delta: pt.TensorLike,\n    size: Optional[Union[int, Tuple[int, ...]]] = None,\n) -&gt; NDArray[np.float64]:\n    \"\"\"Generate random samples from the alternative parameterization SHASH distribution.\n\n    Parameters\n    ----------\n    rng : Generator\n        NumPy random number generator\n    mu : TensorLike\n        Location parameter (mean)\n    sigma : TensorLike\n        Scale parameter (before delta adjustment)\n    epsilon : TensorLike\n        Skewness parameter\n    delta : TensorLike\n        Kurtosis parameter\n    size : int or tuple of ints, optional\n        Output shape. Default is None.\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Array of random samples from the distribution\n\n    Notes\n    -----\n    The sampling process involves:\n    1. Generate standard normal samples\n    2. Apply SHASH transformation\n    3. Scale by sigma/delta and shift by mu\n    \"\"\"\n    s = rng.normal(size=size)\n    sigma_d = sigma / delta  # type: ignore\n    return np.sinh((np.arcsinh(s) + epsilon) / delta) * sigma_d + mu  # type: ignore\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.shash.SHASHoRV","title":"<code>SHASHoRV</code>","text":"<p>               Bases: <code>RandomVariable</code></p> <p>Random variable class for the location-scale SHASH distribution.</p> <p>This class implements sampling from a SHASH distribution that has been transformed to include location (mu) and scale (sigma) parameters.</p> Notes <p>The transformation is y = sigma * x + mu, where x follows the base SHASH distribution with parameters epsilon and delta.</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>class SHASHoRV(RandomVariable):\n    \"\"\"Random variable class for the location-scale SHASH distribution.\n\n    This class implements sampling from a SHASH distribution that has been\n    transformed to include location (mu) and scale (sigma) parameters.\n\n    Notes\n    -----\n    The transformation is y = sigma * x + mu, where x follows the base SHASH\n    distribution with parameters epsilon and delta.\n    \"\"\"\n\n    name = \"shasho\"\n    signature = \"(),(),(),()-&gt;()\"\n    dtype = \"floatX\"\n    _print_name = (\"SHASHo\", \"\\\\operatorname{SHASHo}\")\n\n    @classmethod\n    def rng_fn(\n        cls,\n        rng: Generator,\n        mu: pt.TensorLike,\n        sigma: pt.TensorLike,\n        epsilon: pt.TensorLike,\n        delta: pt.TensorLike,\n        size: Optional[Union[int, Tuple[int, ...]]] = None,\n    ) -&gt; NDArray[np.float64]:\n        \"\"\"Generate random samples from the location-scale SHASH distribution.\n\n        Parameters\n        ----------\n        rng : Generator\n            NumPy random number generator\n        mu : TensorLike\n            Location parameter (mean)\n        sigma : TensorLike\n            Scale parameter (standard deviation)\n        epsilon : TensorLike\n            Skewness parameter\n        delta : TensorLike\n            Kurtosis parameter\n        size : int or tuple of ints, optional\n            Output shape. Default is None.\n\n        Returns\n        -------\n        NDArray[np.float64]\n            Array of random samples from the distribution\n\n        Notes\n        -----\n        The sampling process involves:\n        1. Generate standard normal samples\n        2. Apply SHASH transformation\n        3. Scale and shift the result\n        \"\"\"\n        s = rng.normal(size=size)\n        return np.sinh((np.arcsinh(s) + epsilon) / delta) * sigma + mu  # type: ignore\n</code></pre> <code>rng_fn(rng: Generator, mu: pt.TensorLike, sigma: pt.TensorLike, epsilon: pt.TensorLike, delta: pt.TensorLike, size: Optional[Union[int, Tuple[int, ...]]] = None) -&gt; NDArray[np.float64]</code> <code>classmethod</code> \u00b6 <p>Generate random samples from the location-scale SHASH distribution.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>NumPy random number generator</p> required <code>mu</code> <code>TensorLike</code> <p>Location parameter (mean)</p> required <code>sigma</code> <code>TensorLike</code> <p>Scale parameter (standard deviation)</p> required <code>epsilon</code> <code>TensorLike</code> <p>Skewness parameter</p> required <code>delta</code> <code>TensorLike</code> <p>Kurtosis parameter</p> required <code>size</code> <code>int or tuple of ints</code> <p>Output shape. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Array of random samples from the distribution</p> Notes <p>The sampling process involves: 1. Generate standard normal samples 2. Apply SHASH transformation 3. Scale and shift the result</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>@classmethod\ndef rng_fn(\n    cls,\n    rng: Generator,\n    mu: pt.TensorLike,\n    sigma: pt.TensorLike,\n    epsilon: pt.TensorLike,\n    delta: pt.TensorLike,\n    size: Optional[Union[int, Tuple[int, ...]]] = None,\n) -&gt; NDArray[np.float64]:\n    \"\"\"Generate random samples from the location-scale SHASH distribution.\n\n    Parameters\n    ----------\n    rng : Generator\n        NumPy random number generator\n    mu : TensorLike\n        Location parameter (mean)\n    sigma : TensorLike\n        Scale parameter (standard deviation)\n    epsilon : TensorLike\n        Skewness parameter\n    delta : TensorLike\n        Kurtosis parameter\n    size : int or tuple of ints, optional\n        Output shape. Default is None.\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Array of random samples from the distribution\n\n    Notes\n    -----\n    The sampling process involves:\n    1. Generate standard normal samples\n    2. Apply SHASH transformation\n    3. Scale and shift the result\n    \"\"\"\n    s = rng.normal(size=size)\n    return np.sinh((np.arcsinh(s) + epsilon) / delta) * sigma + mu  # type: ignore\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.shash.SHASHrv","title":"<code>SHASHrv</code>","text":"<p>               Bases: <code>RandomVariable</code></p> <p>Random variable class for the base SHASH distribution.</p> <p>This class implements sampling from the basic SHASH distribution without location and scale parameters.</p> Notes <p>The base SHASH distribution is obtained by applying the sinh-arcsinh transformation to a standard normal distribution.</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>class SHASHrv(RandomVariable):\n    \"\"\"Random variable class for the base SHASH distribution.\n\n    This class implements sampling from the basic SHASH distribution\n    without location and scale parameters.\n\n    Notes\n    -----\n    The base SHASH distribution is obtained by applying the sinh-arcsinh\n    transformation to a standard normal distribution.\n    \"\"\"\n\n    name = \"shash\"\n    signature = \"(),()-&gt;()\"\n    dtype = \"floatX\"\n    _print_name = (\"SHASH\", \"\\\\operatorname{SHASH}\")\n\n    @classmethod\n    def rng_fn(\n        cls,\n        rng: Generator,\n        epsilon: float,\n        delta: float,\n        size: Optional[Union[int, Tuple[int, ...]]] = None,\n    ) -&gt; NDArray[np.float64]:\n        \"\"\"Generate random samples from the base SHASH distribution.\n\n        Parameters\n        ----------\n        rng : Generator\n            NumPy random number generator\n        epsilon : float\n            Skewness parameter controlling asymmetry\n        delta : float\n            Kurtosis parameter controlling tail weight\n        size : int or tuple of ints, optional\n            Output shape. Default is None.\n\n        Returns\n        -------\n        NDArray[np.float64]\n            Array of random samples from the distribution\n\n        Notes\n        -----\n        The sampling process involves:\n        1. Generate standard normal samples\n        2. Apply sinh-arcsinh transformation with parameters\n        \"\"\"\n        return np.sinh(\n            (np.arcsinh(rng.normal(loc=0, scale=1, size=size)) + epsilon) / delta\n        )\n</code></pre> <code>rng_fn(rng: Generator, epsilon: float, delta: float, size: Optional[Union[int, Tuple[int, ...]]] = None) -&gt; NDArray[np.float64]</code> <code>classmethod</code> \u00b6 <p>Generate random samples from the base SHASH distribution.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>NumPy random number generator</p> required <code>epsilon</code> <code>float</code> <p>Skewness parameter controlling asymmetry</p> required <code>delta</code> <code>float</code> <p>Kurtosis parameter controlling tail weight</p> required <code>size</code> <code>int or tuple of ints</code> <p>Output shape. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Array of random samples from the distribution</p> Notes <p>The sampling process involves: 1. Generate standard normal samples 2. Apply sinh-arcsinh transformation with parameters</p> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>@classmethod\ndef rng_fn(\n    cls,\n    rng: Generator,\n    epsilon: float,\n    delta: float,\n    size: Optional[Union[int, Tuple[int, ...]]] = None,\n) -&gt; NDArray[np.float64]:\n    \"\"\"Generate random samples from the base SHASH distribution.\n\n    Parameters\n    ----------\n    rng : Generator\n        NumPy random number generator\n    epsilon : float\n        Skewness parameter controlling asymmetry\n    delta : float\n        Kurtosis parameter controlling tail weight\n    size : int or tuple of ints, optional\n        Output shape. Default is None.\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Array of random samples from the distribution\n\n    Notes\n    -----\n    The sampling process involves:\n    1. Generate standard normal samples\n    2. Apply sinh-arcsinh transformation with parameters\n    \"\"\"\n    return np.sinh(\n        (np.arcsinh(rng.normal(loc=0, scale=1, size=size)) + epsilon) / delta\n    )\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.shash.C","title":"<code>C(x: ArrayLike, epsilon: float, delta: float) -&gt; NDArray[np.float64]</code>","text":"<p>Apply the cosh-arcsinh transformation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Input values to transform</p> required <code>epsilon</code> <code>float</code> <p>Skewness parameter controlling asymmetry</p> required <code>delta</code> <code>float</code> <p>Kurtosis parameter controlling tail weight</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Transformed values</p> Notes <p>This function computes C(x) = sqrt(1 + S(x)^2), where S is the sinh-arcsinh transformation. It is used in computing probability densities of SHASH distributions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; C(0.0, epsilon=0.0, delta=1.0)\n1.0\n</code></pre> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>def C(x: ArrayLike, epsilon: float, delta: float) -&gt; NDArray[np.float64]:\n    \"\"\"Apply the cosh-arcsinh transformation.\n\n    Parameters\n    ----------\n    x : array_like\n        Input values to transform\n    epsilon : float\n        Skewness parameter controlling asymmetry\n    delta : float\n        Kurtosis parameter controlling tail weight\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Transformed values\n\n    Notes\n    -----\n    This function computes C(x) = sqrt(1 + S(x)^2), where S is the\n    sinh-arcsinh transformation. It is used in computing probability\n    densities of SHASH distributions.\n\n    Examples\n    --------\n    &gt;&gt;&gt; C(0.0, epsilon=0.0, delta=1.0)\n    1.0\n    \"\"\"\n    return np.cosh(np.arcsinh(x) * delta - epsilon)\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.shash.S","title":"<code>S(x: ArrayLike, epsilon: float, delta: float) -&gt; NDArray[np.float64]</code>","text":"<p>Apply the Sinh-arcsinh transformation.</p> <p>This transformation allows for flexible modeling of skewness and kurtosis.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Input values to transform</p> required <code>epsilon</code> <code>float</code> <p>Skewness parameter. Positive values give positive skewness</p> required <code>delta</code> <code>float</code> <p>Kurtosis parameter. Values &lt; 1 give heavier tails than normal</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Transformed values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; S(0.0, epsilon=0.0, delta=1.0)\n0.0\n&gt;&gt;&gt; S(1.0, epsilon=0.5, delta=2.0)  # Positive skew, lighter tails\n1.32460...\n</code></pre> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>def S(x: ArrayLike, epsilon: float, delta: float) -&gt; NDArray[np.float64]:\n    \"\"\"Apply the Sinh-arcsinh transformation.\n\n    This transformation allows for flexible modeling of skewness and kurtosis.\n\n    Parameters\n    ----------\n    x : array_like\n        Input values to transform\n    epsilon : float\n        Skewness parameter. Positive values give positive skewness\n    delta : float\n        Kurtosis parameter. Values &lt; 1 give heavier tails than normal\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Transformed values\n\n    Examples\n    --------\n    &gt;&gt;&gt; S(0.0, epsilon=0.0, delta=1.0)\n    0.0\n    &gt;&gt;&gt; S(1.0, epsilon=0.5, delta=2.0)  # Positive skew, lighter tails\n    1.32460...\n    \"\"\"\n    return np.sinh(np.arcsinh(x) * delta - epsilon)\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.hbr.shash.S_inv","title":"<code>S_inv(x: ArrayLike, epsilon: float, delta: float) -&gt; NDArray[np.float64]</code>","text":"<p>Apply the inverse sinh-arcsinh transformation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Input values to transform</p> required <code>epsilon</code> <code>float</code> <p>Skewness parameter controlling asymmetry</p> required <code>delta</code> <code>float</code> <p>Kurtosis parameter controlling tail weight</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>Inverse transformed values</p> Notes <p>This is the inverse of the S() transformation function. Used primarily in sampling from SHASH distributions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; x = 1.0\n&gt;&gt;&gt; np.allclose(S_inv(S(x, 0.5, 2.0), 0.5, 2.0), x)\nTrue\n</code></pre> Source code in <code>pcntoolkit/regression_model/hbr/shash.py</code> <pre><code>def S_inv(x: ArrayLike, epsilon: float, delta: float) -&gt; NDArray[np.float64]:\n    \"\"\"Apply the inverse sinh-arcsinh transformation.\n\n    Parameters\n    ----------\n    x : array_like\n        Input values to transform\n    epsilon : float\n        Skewness parameter controlling asymmetry\n    delta : float\n        Kurtosis parameter controlling tail weight\n\n    Returns\n    -------\n    NDArray[np.float64]\n        Inverse transformed values\n\n    Notes\n    -----\n    This is the inverse of the S() transformation function.\n    Used primarily in sampling from SHASH distributions.\n\n    Examples\n    --------\n    &gt;&gt;&gt; x = 1.0\n    &gt;&gt;&gt; np.allclose(S_inv(S(x, 0.5, 2.0), 0.5, 2.0), x)\n    True\n    \"\"\"\n    return np.sinh((np.arcsinh(x) + epsilon) / delta)\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.reg_conf","title":"<code>reg_conf</code>","text":"<p>Configuration base module for regression models in PCNToolkit.</p> <p>This module provides the abstract base class RegConf that defines the interface for all regression model configurations in the PCNToolkit package. It establishes a standardized way to handle model parameters, validation, and serialization across different regression implementations.</p> <p>The module implements a robust configuration management system that ensures: - Consistent parameter validation across all regression models - Standardized serialization methods for saving/loading configurations - Clear error reporting for invalid configurations - Type safety through static typing - Extensible design for adding new regression models</p> <p>Classes:</p> Name Description <code>RegConf</code> <p>Abstract base class for regression model configurations. Provides the interface that all concrete configuration classes must implement.</p> Notes <p>When implementing a new regression model configuration: 1. Create a new class that inherits from RegConf 2. Implement all abstract methods and properties 3. Define model-specific parameters in init 4. Add validation logic in detect_configuration_problems() 5. Implement serialization in to_dict() and from_dict()</p> Example <p>class MyModelConf(RegConf): ...     def init(self, learning_rate: float = 0.01): ...         self.learning_rate = learning_rate ...         self.post_init() ... ...     @property ...     def has_random_effect(self) -&gt; bool: ...         return False ... ...     def detect_configuration_problems(self) -&gt; List[str]: ...         problems = [] ...         if self.learning_rate &lt;= 0: ...             problems.append(\"learning_rate must be positive\") ...         return problems</p> See Also <p>pcntoolkit.regression_model.blr.blr_conf : Bayesian Linear Regression configuration pcntoolkit.regression_model.gpr.gpr_conf : Gaussian Process Regression configuration pcntoolkit.regression_model.hbr.hbr_conf : Hierarchical Bayesian Regression configuration</p>"},{"location":"api/#pcntoolkit.regression_model.reg_conf.RegConf","title":"<code>RegConf</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for regression model configurations.</p> <p>This class defines the interface for storing and validating configuration parameters of regression models. It handles only configuration parameters and not learned coefficients, which are stored in the model instances themselves.</p> <p>The class provides methods for serialization to/from dictionaries and validation of configuration parameters.</p> <p>Parameters:</p> Name Type Description Default <code>Specific</code> required <p>Attributes:</p> Name Type Description <code>has_random_effect</code> <code>bool</code> <p>Indicates whether the regression model includes random effects.</p> <p>Methods:</p> Name Description <code>__post_init__</code> <p>Validates the configuration after initialization.</p> <code>detect_configuration_problems</code> <p>Checks for any issues in the configuration parameters.</p> <code>from_args</code> <p>Creates a configuration instance from command-line arguments.</p> <code>from_dict</code> <p>Creates a configuration instance from a dictionary.</p> <code>to_dict</code> <p>Serializes the configuration to a dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any configuration problems are detected during initialization.</p> <p>Examples:</p> <p>Subclasses should implement this abstract base class like so:</p> <pre><code>&gt;&gt;&gt; class MyModelConf(RegConf):\n...     def __init__(self, param1: float, param2: str):\n...         self.param1 = param1\n...         self.param2 = param2\n...\n...     @property\n...     def has_random_effect(self) -&gt; bool:\n...         return False\n...\n...     def detect_configuration_problems(self) -&gt; List[str]:\n...         problems = []\n...         if self.param1 &lt; 0:\n...             problems.append(\"param1 must be non-negative\")\n...         return problems\n</code></pre> Notes <ul> <li>All configuration parameters should be immutable after initialization</li> <li>Validation is automatically performed via post_init</li> <li>Subclasses must implement all abstract methods</li> <li>The class follows the configuration validation pattern</li> </ul> Source code in <code>pcntoolkit/regression_model/reg_conf.py</code> <pre><code>class RegConf(ABC):\n    \"\"\"\n    Abstract base class for regression model configurations.\n\n    This class defines the interface for storing and validating configuration parameters\n    of regression models. It handles only configuration parameters and not learned\n    coefficients, which are stored in the model instances themselves.\n\n    The class provides methods for serialization to/from dictionaries and validation\n    of configuration parameters.\n\n    Parameters\n    ----------\n    Specific parameters are defined in concrete subclasses.\n\n    Attributes\n    ----------\n    has_random_effect : bool\n        Indicates whether the regression model includes random effects.\n\n    Methods\n    -------\n    __post_init__()\n        Validates the configuration after initialization.\n    detect_configuration_problems()\n        Checks for any issues in the configuration parameters.\n    from_args(args)\n        Creates a configuration instance from command-line arguments.\n    from_dict(dct)\n        Creates a configuration instance from a dictionary.\n    to_dict(path=None)\n        Serializes the configuration to a dictionary.\n\n    Raises\n    ------\n    ValueError\n        If any configuration problems are detected during initialization.\n\n    Examples\n    --------\n    Subclasses should implement this abstract base class like so:\n\n    &gt;&gt;&gt; class MyModelConf(RegConf):\n    ...     def __init__(self, param1: float, param2: str):\n    ...         self.param1 = param1\n    ...         self.param2 = param2\n    ...\n    ...     @property\n    ...     def has_random_effect(self) -&gt; bool:\n    ...         return False\n    ...\n    ...     def detect_configuration_problems(self) -&gt; List[str]:\n    ...         problems = []\n    ...         if self.param1 &lt; 0:\n    ...             problems.append(\"param1 must be non-negative\")\n    ...         return problems\n\n    Notes\n    -----\n    - All configuration parameters should be immutable after initialization\n    - Validation is automatically performed via __post_init__\n    - Subclasses must implement all abstract methods\n    - The class follows the configuration validation pattern\n    \"\"\"\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"\n        Validates the configuration after initialization.\n\n        This method is automatically called after object initialization to verify\n        that all configuration parameters are valid. It uses detect_configuration_problems()\n        to identify any issues with the configuration.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        ValueError\n            If any configuration problems are detected, with a detailed list of the problems.\n\n        Notes\n        -----\n        - Prints a confirmation message if configuration is valid\n        - The error message includes numbered list of all detected problems\n        \"\"\"\n        configuration_problems = self.detect_configuration_problems()\n        if len(configuration_problems) &gt; 0:\n            problem_list = \"\\n\".join(\n                [f\"{i+1}:\\t{v}\" for i, v in enumerate(configuration_problems)]\n            )\n            raise ValueError(\n                f\"The following problems have been detected in the regression model configuration:\\n{problem_list}\"\n            )\n        else:\n            print(\"Configuration of regression model is valid.\")\n\n    @property\n    @abstractmethod\n    def has_random_effect(self) -&gt; bool:\n        \"\"\"\n        Indicates whether the regression model includes random effects.\n\n        Returns\n        -------\n        bool\n            True if the model includes random effects, False otherwise.\n\n        Notes\n        -----\n        - This is an abstract property that must be implemented by subclasses\n        - Default implementation returns False\n        \"\"\"\n\n    @abstractmethod\n    def detect_configuration_problems(self) -&gt; List[str]:\n        \"\"\"\n        Checks for any issues in the configuration parameters.\n\n        This method should perform validation checks on all configuration parameters\n        and return a list of string descriptions of any problems found.\n\n        Returns\n        -------\n        List[str]\n            A list of strings describing any configuration problems.\n            An empty list indicates no problems were found.\n\n        Notes\n        -----\n        - Subclasses must implement this method\n        - Each problem should be described in a human-readable format\n        - Return an empty list if no problems are detected\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def from_args(cls, args: dict) -&gt; RegConf:\n        \"\"\"\n        Creates a configuration instance from command-line arguments.\n\n        Parameters\n        ----------\n        args : dict\n            Dictionary of command-line arguments, typically parsed from argparse.\n\n        Returns\n        -------\n        RegConf\n            A new instance of the configuration class.\n\n        Notes\n        -----\n        - Subclasses must implement this method\n        - Should handle type conversion from string arguments\n        - Should validate all required arguments are present\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def from_dict(cls, dct: dict) -&gt; RegConf:\n        \"\"\"\n        Creates a configuration instance from a dictionary.\n\n        Parameters\n        ----------\n        dct : dict\n            Dictionary containing configuration parameters.\n\n        Returns\n        -------\n        RegConf\n            A new instance of the configuration class.\n\n        Notes\n        -----\n        - Subclasses must implement this method\n        - Should validate all required keys are present\n        - Useful for loading configurations from JSON/YAML files\n        \"\"\"\n\n    @abstractmethod\n    def to_dict(self, path: str | None = None) -&gt; Dict[str, Any]:\n        \"\"\"\n        Serializes the configuration to a dictionary.\n\n        Parameters\n        ----------\n        path : str | None, optional\n            Optional file path for configurations that include file references.\n            Used to resolve relative paths to absolute paths.\n\n        Returns\n        -------\n        Dict[str, Any]\n            Dictionary representation of the configuration.\n\n        Notes\n        -----\n        - Subclasses must implement this method\n        - Should handle conversion of all parameters to JSON-serializable types\n        - If path is provided, should resolve any relative paths to absolute paths\n        \"\"\"\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.reg_conf.RegConf.has_random_effect","title":"<code>has_random_effect: bool</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Indicates whether the regression model includes random effects.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the model includes random effects, False otherwise.</p> Notes <ul> <li>This is an abstract property that must be implemented by subclasses</li> <li>Default implementation returns False</li> </ul>"},{"location":"api/#pcntoolkit.regression_model.reg_conf.RegConf.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"<p>Validates the configuration after initialization.</p> <p>This method is automatically called after object initialization to verify that all configuration parameters are valid. It uses detect_configuration_problems() to identify any issues with the configuration.</p> <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any configuration problems are detected, with a detailed list of the problems.</p> Notes <ul> <li>Prints a confirmation message if configuration is valid</li> <li>The error message includes numbered list of all detected problems</li> </ul> Source code in <code>pcntoolkit/regression_model/reg_conf.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"\n    Validates the configuration after initialization.\n\n    This method is automatically called after object initialization to verify\n    that all configuration parameters are valid. It uses detect_configuration_problems()\n    to identify any issues with the configuration.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If any configuration problems are detected, with a detailed list of the problems.\n\n    Notes\n    -----\n    - Prints a confirmation message if configuration is valid\n    - The error message includes numbered list of all detected problems\n    \"\"\"\n    configuration_problems = self.detect_configuration_problems()\n    if len(configuration_problems) &gt; 0:\n        problem_list = \"\\n\".join(\n            [f\"{i+1}:\\t{v}\" for i, v in enumerate(configuration_problems)]\n        )\n        raise ValueError(\n            f\"The following problems have been detected in the regression model configuration:\\n{problem_list}\"\n        )\n    else:\n        print(\"Configuration of regression model is valid.\")\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.reg_conf.RegConf.detect_configuration_problems","title":"<code>detect_configuration_problems() -&gt; List[str]</code>  <code>abstractmethod</code>","text":"<p>Checks for any issues in the configuration parameters.</p> <p>This method should perform validation checks on all configuration parameters and return a list of string descriptions of any problems found.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of strings describing any configuration problems. An empty list indicates no problems were found.</p> Notes <ul> <li>Subclasses must implement this method</li> <li>Each problem should be described in a human-readable format</li> <li>Return an empty list if no problems are detected</li> </ul> Source code in <code>pcntoolkit/regression_model/reg_conf.py</code> <pre><code>@abstractmethod\ndef detect_configuration_problems(self) -&gt; List[str]:\n    \"\"\"\n    Checks for any issues in the configuration parameters.\n\n    This method should perform validation checks on all configuration parameters\n    and return a list of string descriptions of any problems found.\n\n    Returns\n    -------\n    List[str]\n        A list of strings describing any configuration problems.\n        An empty list indicates no problems were found.\n\n    Notes\n    -----\n    - Subclasses must implement this method\n    - Each problem should be described in a human-readable format\n    - Return an empty list if no problems are detected\n    \"\"\"\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.reg_conf.RegConf.from_args","title":"<code>from_args(args: dict) -&gt; RegConf</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Creates a configuration instance from command-line arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>dict</code> <p>Dictionary of command-line arguments, typically parsed from argparse.</p> required <p>Returns:</p> Type Description <code>RegConf</code> <p>A new instance of the configuration class.</p> Notes <ul> <li>Subclasses must implement this method</li> <li>Should handle type conversion from string arguments</li> <li>Should validate all required arguments are present</li> </ul> Source code in <code>pcntoolkit/regression_model/reg_conf.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_args(cls, args: dict) -&gt; RegConf:\n    \"\"\"\n    Creates a configuration instance from command-line arguments.\n\n    Parameters\n    ----------\n    args : dict\n        Dictionary of command-line arguments, typically parsed from argparse.\n\n    Returns\n    -------\n    RegConf\n        A new instance of the configuration class.\n\n    Notes\n    -----\n    - Subclasses must implement this method\n    - Should handle type conversion from string arguments\n    - Should validate all required arguments are present\n    \"\"\"\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.reg_conf.RegConf.from_dict","title":"<code>from_dict(dct: dict) -&gt; RegConf</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Creates a configuration instance from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dct</code> <code>dict</code> <p>Dictionary containing configuration parameters.</p> required <p>Returns:</p> Type Description <code>RegConf</code> <p>A new instance of the configuration class.</p> Notes <ul> <li>Subclasses must implement this method</li> <li>Should validate all required keys are present</li> <li>Useful for loading configurations from JSON/YAML files</li> </ul> Source code in <code>pcntoolkit/regression_model/reg_conf.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_dict(cls, dct: dict) -&gt; RegConf:\n    \"\"\"\n    Creates a configuration instance from a dictionary.\n\n    Parameters\n    ----------\n    dct : dict\n        Dictionary containing configuration parameters.\n\n    Returns\n    -------\n    RegConf\n        A new instance of the configuration class.\n\n    Notes\n    -----\n    - Subclasses must implement this method\n    - Should validate all required keys are present\n    - Useful for loading configurations from JSON/YAML files\n    \"\"\"\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.reg_conf.RegConf.to_dict","title":"<code>to_dict(path: str | None = None) -&gt; Dict[str, Any]</code>  <code>abstractmethod</code>","text":"<p>Serializes the configuration to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | None</code> <p>Optional file path for configurations that include file references. Used to resolve relative paths to absolute paths.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary representation of the configuration.</p> Notes <ul> <li>Subclasses must implement this method</li> <li>Should handle conversion of all parameters to JSON-serializable types</li> <li>If path is provided, should resolve any relative paths to absolute paths</li> </ul> Source code in <code>pcntoolkit/regression_model/reg_conf.py</code> <pre><code>@abstractmethod\ndef to_dict(self, path: str | None = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Serializes the configuration to a dictionary.\n\n    Parameters\n    ----------\n    path : str | None, optional\n        Optional file path for configurations that include file references.\n        Used to resolve relative paths to absolute paths.\n\n    Returns\n    -------\n    Dict[str, Any]\n        Dictionary representation of the configuration.\n\n    Notes\n    -----\n    - Subclasses must implement this method\n    - Should handle conversion of all parameters to JSON-serializable types\n    - If path is provided, should resolve any relative paths to absolute paths\n    \"\"\"\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.regression_model","title":"<code>regression_model</code>","text":"<p>Abstract base class for regression models in the PCNToolkit.</p> <p>This module provides the base class for all regression models, defining the common interface and shared functionality that all regression implementations must follow.</p> Notes <p>All regression model implementations should inherit from this base class and implement the abstract methods.</p>"},{"location":"api/#pcntoolkit.regression_model.regression_model.RegressionModel","title":"<code>RegressionModel</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for regression models.</p> <p>This class defines the interface for all regression models in the toolkit, providing common attributes and methods that must be implemented by concrete subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique identifier for the regression model instance</p> required <code>reg_conf</code> <code>RegConf</code> <p>Configuration object containing regression model parameters</p> required <code>is_fitted</code> <code>bool</code> <p>Flag indicating if the model has been fitted to data, by default False</p> <code>False</code> <code>is_from_dict</code> <code>bool</code> <p>Flag indicating if the model was instantiated from a dictionary, by default False</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>is_fitted</code> <code>bool</code> <p>Indicates whether the model has been fitted to data</p> Source code in <code>pcntoolkit/regression_model/regression_model.py</code> <pre><code>class RegressionModel(ABC):\n    \"\"\"\n    Abstract base class for regression models.\n\n    This class defines the interface for all regression models in the toolkit,\n    providing common attributes and methods that must be implemented by concrete\n    subclasses.\n\n    Parameters\n    ----------\n    name : str\n        Unique identifier for the regression model instance\n    reg_conf : RegConf\n        Configuration object containing regression model parameters\n    is_fitted : bool, optional\n        Flag indicating if the model has been fitted to data, by default False\n    is_from_dict : bool, optional\n        Flag indicating if the model was instantiated from a dictionary, by default False\n\n    Attributes\n    ----------\n    is_fitted : bool\n        Indicates whether the model has been fitted to data\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        reg_conf: RegConf,\n        is_fitted: bool = False,\n        is_from_dict: bool = False,\n    ):\n        self._name: str = name\n        self._reg_conf: RegConf = reg_conf\n        self.is_fitted: bool = is_fitted\n        self._is_from_dict: bool = is_from_dict\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        Get the model's name.\n\n        Returns\n        -------\n        str\n            The unique identifier of the model\n        \"\"\"\n        return self._name\n\n    @property\n    def reg_conf(self) -&gt; RegConf:\n        \"\"\"\n        Get the model's configuration.\n\n        Returns\n        -------\n        RegConf\n            The configuration object containing model parameters\n        \"\"\"\n        return self._reg_conf\n\n    @property\n    def is_from_dict(self) -&gt; bool:\n        \"\"\"\n        Check if model was instantiated from dictionary.\n\n        Returns\n        -------\n        bool\n            True if model was created from dictionary, False otherwise\n        \"\"\"\n        return self._is_from_dict\n\n    def to_dict(self, path: str | None = None) -&gt; dict:\n        \"\"\"\n        Convert model instance to dictionary representation.\n\n        Parameters\n        ----------\n        path : str | None, optional\n            Path to save any associated files, by default None\n\n        Returns\n        -------\n        dict\n            Dictionary containing model parameters and configuration\n        \"\"\"\n        my_dict: dict[str, str | dict | bool] = {}\n        my_dict[\"name\"] = self.name\n        my_dict[\"type\"] = self.__class__.__name__\n        my_dict[\"reg_conf\"] = self.reg_conf.to_dict(path)\n        my_dict[\"is_fitted\"] = self.is_fitted\n        my_dict[\"is_from_dict\"] = self.is_from_dict\n        return my_dict\n\n    @classmethod\n    @abstractmethod\n    def from_dict(cls, my_dict: dict, path: str) -&gt; RegressionModel:\n        \"\"\"\n        Create model instance from dictionary representation.\n\n        Parameters\n        ----------\n        dct : dict\n            Dictionary containing model parameters and configuration\n        path : str\n            Path to load any associated files\n\n        Returns\n        -------\n        RegressionModel\n            New instance of the regression model\n\n        Raises\n        ------\n        NotImplementedError\n            Must be implemented by concrete subclasses\n        \"\"\" \n\n    @classmethod\n    @abstractmethod\n    def from_args(cls, name: str, args: dict) -&gt; RegressionModel:\n        \"\"\"\n        Create model instance from arguments dictionary.\n\n        Parameters\n        ----------\n        name : str\n            Unique identifier for the model instance\n        args : dict\n            Dictionary of model parameters and configuration\n\n        Returns\n        -------\n        RegressionModel\n            New instance of the regression model\n\n        Raises\n        ------\n        NotImplementedError\n            Must be implemented by concrete subclasses\n        \"\"\"\n\n    @property\n    def has_random_effect(self) -&gt; bool:\n        \"\"\"\n        Check if model includes random effects.\n\n        Returns\n        -------\n        bool\n            True if model includes random effects, False otherwise\n        \"\"\"\n        return self.reg_conf.has_random_effect\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.regression_model.RegressionModel.has_random_effect","title":"<code>has_random_effect: bool</code>  <code>property</code>","text":"<p>Check if model includes random effects.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if model includes random effects, False otherwise</p>"},{"location":"api/#pcntoolkit.regression_model.regression_model.RegressionModel.is_from_dict","title":"<code>is_from_dict: bool</code>  <code>property</code>","text":"<p>Check if model was instantiated from dictionary.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if model was created from dictionary, False otherwise</p>"},{"location":"api/#pcntoolkit.regression_model.regression_model.RegressionModel.name","title":"<code>name: str</code>  <code>property</code>","text":"<p>Get the model's name.</p> <p>Returns:</p> Type Description <code>str</code> <p>The unique identifier of the model</p>"},{"location":"api/#pcntoolkit.regression_model.regression_model.RegressionModel.reg_conf","title":"<code>reg_conf: RegConf</code>  <code>property</code>","text":"<p>Get the model's configuration.</p> <p>Returns:</p> Type Description <code>RegConf</code> <p>The configuration object containing model parameters</p>"},{"location":"api/#pcntoolkit.regression_model.regression_model.RegressionModel.from_args","title":"<code>from_args(name: str, args: dict) -&gt; RegressionModel</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Create model instance from arguments dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique identifier for the model instance</p> required <code>args</code> <code>dict</code> <p>Dictionary of model parameters and configuration</p> required <p>Returns:</p> Type Description <code>RegressionModel</code> <p>New instance of the regression model</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by concrete subclasses</p> Source code in <code>pcntoolkit/regression_model/regression_model.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_args(cls, name: str, args: dict) -&gt; RegressionModel:\n    \"\"\"\n    Create model instance from arguments dictionary.\n\n    Parameters\n    ----------\n    name : str\n        Unique identifier for the model instance\n    args : dict\n        Dictionary of model parameters and configuration\n\n    Returns\n    -------\n    RegressionModel\n        New instance of the regression model\n\n    Raises\n    ------\n    NotImplementedError\n        Must be implemented by concrete subclasses\n    \"\"\"\n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.regression_model.RegressionModel.from_dict","title":"<code>from_dict(my_dict: dict, path: str) -&gt; RegressionModel</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Create model instance from dictionary representation.</p> <p>Parameters:</p> Name Type Description Default <code>dct</code> <code>dict</code> <p>Dictionary containing model parameters and configuration</p> required <code>path</code> <code>str</code> <p>Path to load any associated files</p> required <p>Returns:</p> Type Description <code>RegressionModel</code> <p>New instance of the regression model</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by concrete subclasses</p> Source code in <code>pcntoolkit/regression_model/regression_model.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_dict(cls, my_dict: dict, path: str) -&gt; RegressionModel:\n    \"\"\"\n    Create model instance from dictionary representation.\n\n    Parameters\n    ----------\n    dct : dict\n        Dictionary containing model parameters and configuration\n    path : str\n        Path to load any associated files\n\n    Returns\n    -------\n    RegressionModel\n        New instance of the regression model\n\n    Raises\n    ------\n    NotImplementedError\n        Must be implemented by concrete subclasses\n    \"\"\" \n</code></pre>"},{"location":"api/#pcntoolkit.regression_model.regression_model.RegressionModel.to_dict","title":"<code>to_dict(path: str | None = None) -&gt; dict</code>","text":"<p>Convert model instance to dictionary representation.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | None</code> <p>Path to save any associated files, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing model parameters and configuration</p> Source code in <code>pcntoolkit/regression_model/regression_model.py</code> <pre><code>def to_dict(self, path: str | None = None) -&gt; dict:\n    \"\"\"\n    Convert model instance to dictionary representation.\n\n    Parameters\n    ----------\n    path : str | None, optional\n        Path to save any associated files, by default None\n\n    Returns\n    -------\n    dict\n        Dictionary containing model parameters and configuration\n    \"\"\"\n    my_dict: dict[str, str | dict | bool] = {}\n    my_dict[\"name\"] = self.name\n    my_dict[\"type\"] = self.__class__.__name__\n    my_dict[\"reg_conf\"] = self.reg_conf.to_dict(path)\n    my_dict[\"is_fitted\"] = self.is_fitted\n    my_dict[\"is_from_dict\"] = self.is_from_dict\n    return my_dict\n</code></pre>"},{"location":"api/#pcntoolkit.util","title":"<code>util</code>","text":""},{"location":"api/#pcntoolkit.util.utils","title":"<code>utils</code>","text":"<p>A collection of general utility functions for the package.</p>"},{"location":"api/#pcntoolkit.util.utils.get_type_of_object","title":"<code>get_type_of_object(path: str) -&gt; str</code>","text":"<p>Return the type of object at the given path</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>object location</p> required <p>Returns:</p> Type Description <code>    The typ of the object (file, directory, other, nonexistant)</code> Source code in <code>pcntoolkit/util/utils.py</code> <pre><code>def get_type_of_object(path: str) -&gt; str:\n    \"\"\"Return the type of object at the given path\n\n    Parameters\n    ----------\n    path : str\n        object location\n\n    Returns\n    -------\n        The typ of the object (file, directory, other, nonexistant)\n    \"\"\"\n    if os.path.exists(path):\n        if os.path.isdir(path):\n            return \"directory\"\n        elif os.path.isfile(path):\n            return \"file\"\n        else:\n            return \"other\"\n    else:\n        return \"nonexistant\"\n</code></pre>"},{"location":"api/#pcntoolkit.util.utils.yes_or_no","title":"<code>yes_or_no(question: str) -&gt; bool</code>","text":"<p>Utility function to ask a yes/no question from the user.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>String for user query.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Boolean of True for 'yes' and False for 'no'.</p> Source code in <code>pcntoolkit/util/utils.py</code> <pre><code>def yes_or_no(question: str) -&gt; bool:\n    \"\"\"Utility function to ask a yes/no question from the user.\n\n    Parameters\n    ----------\n    question : str\n        String for user query.\n\n    Returns\n    -------\n    bool\n        Boolean of True for 'yes' and False for 'no'.\n    \"\"\"\n\n    while \"the answer is invalid\":\n        reply = str(input(question + \" (y/n): \")).lower().strip()\n        if reply[:1] == \"y\":\n            return True\n        if reply[:1] == \"n\":\n            return False\n    return False\n</code></pre>"}]}